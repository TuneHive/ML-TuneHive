{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.api.layers import Dense, Embedding, GRU, LeakyReLU, Concatenate, Masking, Layer, StringLookup, Normalization, BatchNormalization\n",
    "from keras.api import Input\n",
    "from keras.api.models import Model\n",
    "from keras.api.losses import SparseCategoricalCrossentropy\n",
    "from keras.api.metrics import SparseCategoricalAccuracy, Mean, TopKCategoricalAccuracy\n",
    "# from transformers.models.bert import TFBertTokenizer, TFBertEmbeddings  # embedding and tokenizer for description/nlp related stufff\n",
    "from keras.api.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>SongID</th>\n",
       "      <th>TimeStamp_Central</th>\n",
       "      <th>Performer_x</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song_x</th>\n",
       "      <th>TimeStamp_UTC</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Performer_y</th>\n",
       "      <th>Song_y</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>session_3_hour</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Twenty Five MilesEdwin Starr</td>\n",
       "      <td>5/25/2021 5:18:00 PM</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>25 Miles</td>\n",
       "      <td>Twenty Five Miles</td>\n",
       "      <td>2021-05-25 23:18:00</td>\n",
       "      <td>9761</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>Twenty Five Miles</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.964</td>\n",
       "      <td>124.567</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Devil's EyesGreyhounds</td>\n",
       "      <td>5/25/2021 5:15:00 PM</td>\n",
       "      <td>Greyhounds</td>\n",
       "      <td>Change of Pace</td>\n",
       "      <td>Devil's Eyes</td>\n",
       "      <td>2021-05-25 23:15:00</td>\n",
       "      <td>206</td>\n",
       "      <td>Greyhounds</td>\n",
       "      <td>Devil's Eyes</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.858</td>\n",
       "      <td>113.236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pussy and PizzaMurs</td>\n",
       "      <td>5/25/2021 5:12:00 PM</td>\n",
       "      <td>Murs</td>\n",
       "      <td>Have a Nice Life</td>\n",
       "      <td>Pussy and Pizza</td>\n",
       "      <td>2021-05-25 23:12:00</td>\n",
       "      <td>6404</td>\n",
       "      <td>Murs</td>\n",
       "      <td>Pussy and Pizza</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.381</td>\n",
       "      <td>93.991</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Our Special PlaceThe Heavy</td>\n",
       "      <td>5/25/2021 4:46:00 PM</td>\n",
       "      <td>The Heavy</td>\n",
       "      <td>Great Vengeance and Furious Fire</td>\n",
       "      <td>Our Special Place</td>\n",
       "      <td>2021-05-25 22:46:00</td>\n",
       "      <td>6205</td>\n",
       "      <td>The Heavy</td>\n",
       "      <td>Our Special Place</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.939</td>\n",
       "      <td>193.996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Make Peace and be FreePerfect Confusion</td>\n",
       "      <td>5/25/2021 4:39:00 PM</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Make Peace and be Free</td>\n",
       "      <td>2021-05-25 22:39:00</td>\n",
       "      <td>6051</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Make Peace and be Free</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.431</td>\n",
       "      <td>78.037</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50013</th>\n",
       "      <td>62902</td>\n",
       "      <td>From Me To You - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 10:04:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Past Masters (Vols. 1 &amp; 2 / Remastered)</td>\n",
       "      <td>From Me To You - Remastered 2009</td>\n",
       "      <td>2017-01-01 16:04:00</td>\n",
       "      <td>5693</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>From Me To You - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.6130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.966</td>\n",
       "      <td>136.125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50014</th>\n",
       "      <td>62903</td>\n",
       "      <td>And I Love Her - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 10:01:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>A Hard Day's Night (Remastered)</td>\n",
       "      <td>And I Love Her - Remastered 2009</td>\n",
       "      <td>2017-01-01 16:01:00</td>\n",
       "      <td>360</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>And I Love Her - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.636</td>\n",
       "      <td>113.312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50015</th>\n",
       "      <td>62904</td>\n",
       "      <td>Ticket To Ride - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:58:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Help! (Remastered)</td>\n",
       "      <td>Ticket To Ride - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:58:00</td>\n",
       "      <td>9715</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Ticket To Ride - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.749</td>\n",
       "      <td>123.419</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50016</th>\n",
       "      <td>62905</td>\n",
       "      <td>Come Together - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:54:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Abbey Road (Remastered)</td>\n",
       "      <td>Come Together - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:54:00</td>\n",
       "      <td>7425</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Come Together - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.187</td>\n",
       "      <td>165.007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50017</th>\n",
       "      <td>62906</td>\n",
       "      <td>Penny Lane - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:51:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Magical Mystery Tour (Remastered)</td>\n",
       "      <td>Penny Lane - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:51:00</td>\n",
       "      <td>4401</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Penny Lane - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.490</td>\n",
       "      <td>113.038</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50018 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index_x                                       SongID  \\\n",
       "0            0                 Twenty Five MilesEdwin Starr   \n",
       "1            1                       Devil's EyesGreyhounds   \n",
       "2            2                          Pussy and PizzaMurs   \n",
       "3            8                   Our Special PlaceThe Heavy   \n",
       "4           10      Make Peace and be FreePerfect Confusion   \n",
       "...        ...                                          ...   \n",
       "50013    62902  From Me To You - Remastered 2009The Beatles   \n",
       "50014    62903  And I Love Her - Remastered 2009The Beatles   \n",
       "50015    62904  Ticket To Ride - Remastered 2009The Beatles   \n",
       "50016    62905   Come Together - Remastered 2009The Beatles   \n",
       "50017    62906      Penny Lane - Remastered 2009The Beatles   \n",
       "\n",
       "          TimeStamp_Central        Performer_x  \\\n",
       "0      5/25/2021 5:18:00 PM        Edwin Starr   \n",
       "1      5/25/2021 5:15:00 PM         Greyhounds   \n",
       "2      5/25/2021 5:12:00 PM               Murs   \n",
       "3      5/25/2021 4:46:00 PM          The Heavy   \n",
       "4      5/25/2021 4:39:00 PM  Perfect Confusion   \n",
       "...                     ...                ...   \n",
       "50013  1/1/2017 10:04:00 AM        The Beatles   \n",
       "50014  1/1/2017 10:01:00 AM        The Beatles   \n",
       "50015   1/1/2017 9:58:00 AM        The Beatles   \n",
       "50016   1/1/2017 9:54:00 AM        The Beatles   \n",
       "50017   1/1/2017 9:51:00 AM        The Beatles   \n",
       "\n",
       "                                         Album  \\\n",
       "0                                     25 Miles   \n",
       "1                               Change of Pace   \n",
       "2                             Have a Nice Life   \n",
       "3             Great Vengeance and Furious Fire   \n",
       "4                            Perfect Confusion   \n",
       "...                                        ...   \n",
       "50013  Past Masters (Vols. 1 & 2 / Remastered)   \n",
       "50014          A Hard Day's Night (Remastered)   \n",
       "50015                       Help! (Remastered)   \n",
       "50016                  Abbey Road (Remastered)   \n",
       "50017        Magical Mystery Tour (Remastered)   \n",
       "\n",
       "                                 Song_x        TimeStamp_UTC  index_y  \\\n",
       "0                     Twenty Five Miles  2021-05-25 23:18:00     9761   \n",
       "1                          Devil's Eyes  2021-05-25 23:15:00      206   \n",
       "2                       Pussy and Pizza  2021-05-25 23:12:00     6404   \n",
       "3                     Our Special Place  2021-05-25 22:46:00     6205   \n",
       "4                Make Peace and be Free  2021-05-25 22:39:00     6051   \n",
       "...                                 ...                  ...      ...   \n",
       "50013  From Me To You - Remastered 2009  2017-01-01 16:04:00     5693   \n",
       "50014  And I Love Her - Remastered 2009  2017-01-01 16:01:00      360   \n",
       "50015  Ticket To Ride - Remastered 2009  2017-01-01 15:58:00     9715   \n",
       "50016   Come Together - Remastered 2009  2017-01-01 15:54:00     7425   \n",
       "50017      Penny Lane - Remastered 2009  2017-01-01 15:51:00     4401   \n",
       "\n",
       "             Performer_y                            Song_y  ... mode  \\\n",
       "0            Edwin Starr                 Twenty Five Miles  ...  1.0   \n",
       "1             Greyhounds                      Devil's Eyes  ...  0.0   \n",
       "2                   Murs                   Pussy and Pizza  ...  1.0   \n",
       "3              The Heavy                 Our Special Place  ...  1.0   \n",
       "4      Perfect Confusion            Make Peace and be Free  ...  1.0   \n",
       "...                  ...                               ...  ...  ...   \n",
       "50013        The Beatles  From Me To You - Remastered 2009  ...  1.0   \n",
       "50014        The Beatles  And I Love Her - Remastered 2009  ...  0.0   \n",
       "50015        The Beatles  Ticket To Ride - Remastered 2009  ...  1.0   \n",
       "50016        The Beatles   Come Together - Remastered 2009  ...  0.0   \n",
       "50017        The Beatles      Penny Lane - Remastered 2009  ...  1.0   \n",
       "\n",
       "      speechiness acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0          0.0607       0.0595          0.000015    0.2240    0.964  124.567   \n",
       "1          0.0456       0.3540          0.000414    0.0974    0.858  113.236   \n",
       "2          0.0659       0.0708          0.000004    0.0780    0.381   93.991   \n",
       "3          0.0386       0.2720          0.003610    0.0991    0.939  193.996   \n",
       "4          0.0315       0.0138          0.000017    0.0649    0.431   78.037   \n",
       "...           ...          ...               ...       ...      ...      ...   \n",
       "50013      0.0309       0.6130          0.000000    0.2690    0.966  136.125   \n",
       "50014      0.0337       0.6400          0.000000    0.0681    0.636  113.312   \n",
       "50015      0.0678       0.0457          0.000000    0.2330    0.749  123.419   \n",
       "50016      0.0393       0.0302          0.248000    0.0926    0.187  165.007   \n",
       "50017      0.0316       0.2120          0.026000    0.1360    0.490  113.038   \n",
       "\n",
       "       time_signature       session_3_hour  session_id  \n",
       "0                 4.0  2021-05-25 21:00:00        4332  \n",
       "1                 4.0  2021-05-25 21:00:00        4332  \n",
       "2                 4.0  2021-05-25 21:00:00        4332  \n",
       "3                 4.0  2021-05-25 21:00:00        4332  \n",
       "4                 4.0  2021-05-25 21:00:00        4332  \n",
       "...               ...                  ...         ...  \n",
       "50013             4.0  2017-01-01 15:00:00           0  \n",
       "50014             4.0  2017-01-01 15:00:00           0  \n",
       "50015             4.0  2017-01-01 15:00:00           0  \n",
       "50016             4.0  2017-01-01 15:00:00           0  \n",
       "50017             4.0  2017-01-01 15:00:00           0  \n",
       "\n",
       "[50018 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "df = pd.read_csv(\"data/session-data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50018 entries, 0 to 50017\n",
      "Data columns (total 30 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index_x                    50018 non-null  int64  \n",
      " 1   SongID                     50018 non-null  object \n",
      " 2   TimeStamp_Central          50018 non-null  object \n",
      " 3   Performer_x                50018 non-null  object \n",
      " 4   Album                      47890 non-null  object \n",
      " 5   Song_x                     50018 non-null  object \n",
      " 6   TimeStamp_UTC              50018 non-null  object \n",
      " 7   index_y                    50018 non-null  int64  \n",
      " 8   Performer_y                50018 non-null  object \n",
      " 9   Song_y                     50018 non-null  object \n",
      " 10  spotify_genre              50018 non-null  object \n",
      " 11  spotify_track_id           50018 non-null  object \n",
      " 12  spotify_track_preview_url  36001 non-null  object \n",
      " 13  spotify_track_duration_ms  50018 non-null  float64\n",
      " 14  spotify_track_popularity   50018 non-null  float64\n",
      " 15  spotify_track_explicit     50018 non-null  bool   \n",
      " 16  danceability               50018 non-null  float64\n",
      " 17  energy                     50018 non-null  float64\n",
      " 18  key                        50018 non-null  float64\n",
      " 19  loudness                   50018 non-null  float64\n",
      " 20  mode                       50018 non-null  float64\n",
      " 21  speechiness                50018 non-null  float64\n",
      " 22  acousticness               50018 non-null  float64\n",
      " 23  instrumentalness           50018 non-null  float64\n",
      " 24  liveness                   50018 non-null  float64\n",
      " 25  valence                    50018 non-null  float64\n",
      " 26  tempo                      50018 non-null  float64\n",
      " 27  time_signature             50018 non-null  float64\n",
      " 28  session_3_hour             50018 non-null  object \n",
      " 29  session_id                 50018 non-null  int64  \n",
      "dtypes: bool(1), float64(14), int64(3), object(12)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        0.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "50013    1.0\n",
       "50014    0.0\n",
       "50015    1.0\n",
       "50016    0.0\n",
       "50017    1.0\n",
       "Name: mode, Length: 50018, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_col_name = 'mode'\n",
    "df.loc[:, test_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove N.A.N data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df[~df['danceability'].isna()]\n",
    "# df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature columns (as provided)\n",
    "feature_columns = [\n",
    "    'spotify_genre',\n",
    "    'spotify_track_popularity',\n",
    "    'danceability',\n",
    "    'loudness',\n",
    "    'acousticness',\n",
    "    'instrumentalness',\n",
    "    'tempo',\n",
    "]\n",
    "\n",
    "# Define the DataPreprocessor class\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, df, feature_columns, batch_size=16, fixed_genre_size=10, train_size=0.8):\n",
    "        \"\"\"\n",
    "        Initializes the data preprocessor with necessary parameters and preprocessing layers.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame containing session data.\n",
    "            feature_columns (list): List of feature column names.\n",
    "            batch_size (int): The batch size for dataset creation.\n",
    "            fixed_genre_size (int): The fixed size for genre vectorization.\n",
    "            train_size (float): Proportion of the data to use for training (between 0 and 1).\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_columns = feature_columns\n",
    "        self.batch_size = batch_size\n",
    "        self.fixed_genre_size = fixed_genre_size\n",
    "        self.train_size = train_size\n",
    "\n",
    "        # Split the dataset into training and testing datasets\n",
    "        self.train_df, self.test_df = train_test_split(self.df, train_size=self.train_size, random_state=42)\n",
    "        \n",
    "        # Numeric feature preprocessing\n",
    "        self.numeric_data = self.df[feature_columns[1:]].apply(pd.to_numeric, errors='coerce')\n",
    "        self.mean_values = self.numeric_data.mean()\n",
    "        self.std_values = self.numeric_data.std()\n",
    "\n",
    "        # Initialize LabelEncoder for SongID and spotify_genre\n",
    "        self.song_id_encoder = LabelEncoder()\n",
    "        self.genre_encoder = LabelEncoder()\n",
    "\n",
    "        # Extract unique SongIDs and genres\n",
    "        unique_song_ids = self.df['SongID'].unique()\n",
    "        all_genres = []\n",
    "        for genre_str in self.df['spotify_genre']:\n",
    "            try:\n",
    "                genre_list = ast.literal_eval(genre_str)  # Safely parse the string into a list\n",
    "                if isinstance(genre_list, list):\n",
    "                    all_genres.extend(genre_list)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing genre: {e}\")\n",
    "\n",
    "        unique_genres = list(set(all_genres))\n",
    "\n",
    "        # Fit the LabelEncoders on the data\n",
    "        self.song_id_encoder.fit(unique_song_ids)\n",
    "        self.genre_encoder.fit(unique_genres)\n",
    "\n",
    "        self.items_size = len(self.song_id_encoder.classes_)  # Number of unique SongIDs\n",
    "        self.genres_size = len(self.genre_encoder.classes_)  \n",
    "        \n",
    "        self.dataset = None\n",
    "\n",
    "    def preprocess_song_id(self, song_id):\n",
    "        \"\"\"\n",
    "        Encode the SongID using LabelEncoder.\n",
    "        \"\"\"\n",
    "        return self.song_id_encoder.transform([song_id])[0]\n",
    "\n",
    "    def clean_genre(self, value, default_value=0, dtype=tf.int32):\n",
    "        \"\"\"\n",
    "        Clean and process the 'spotify_genre' feature.\n",
    "        \"\"\"\n",
    "        if value is None or (isinstance(value, str) and not value.strip()):\n",
    "            return np.full((self.fixed_genre_size,), default_value, dtype=dtype.as_numpy_dtype)\n",
    "\n",
    "        try:\n",
    "            genre_list = eval(value) if isinstance(value, str) else value\n",
    "            if isinstance(genre_list, list):\n",
    "                genre_encoded = self.genre_encoder.transform(genre_list)\n",
    "            else:\n",
    "                genre_encoded = self.genre_encoder.transform([value])\n",
    "        except Exception:\n",
    "            genre_encoded = self.genre_encoder.transform([value])\n",
    "\n",
    "        # Pad or truncate to fixed size\n",
    "        return np.pad(genre_encoded, (0, max(0, self.fixed_genre_size - len(genre_encoded))),\n",
    "                      mode='constant')[:self.fixed_genre_size].astype(dtype.as_numpy_dtype)\n",
    "\n",
    "    def clean_numeric_feature(self, value, default_value=0.0, feature_name=\"feature\", mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Clean, process, and normalize numerical features using Z-score normalization.\n",
    "        \"\"\"\n",
    "        if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "            return default_value\n",
    "\n",
    "        try:\n",
    "            value = float(value)\n",
    "            # Apply Z-score normalization if mean and std are provided\n",
    "            if mean is not None and std is not None and std != 0:\n",
    "                z_score_value = (value - mean) / std\n",
    "                return z_score_value\n",
    "            return value  # Return raw value if no normalization\n",
    "        except ValueError:\n",
    "            return default_value\n",
    "\n",
    "    def create_session_dataset(self, session_df):\n",
    "        \"\"\"\n",
    "        Create session dataset as a list of dictionaries for each session.\n",
    "        \"\"\"\n",
    "        session_df = session_df.sort_values(by=['session_id', 'TimeStamp_UTC'])\n",
    "        grouped = session_df.groupby('session_id')\n",
    "        sessions_data = []\n",
    "        for session_id, group in grouped:\n",
    "            session_data = group.to_dict(orient='records')\n",
    "            sessions_data.append(session_data)\n",
    "        return sessions_data\n",
    "\n",
    "    def preprocess_data(self, sessions, k=1):\n",
    "        \"\"\"\n",
    "        Preprocess session data into TensorFlow dataset with split genre and features,\n",
    "        filtering out sequences where the next item sequence length is not greater than 10.\n",
    "        \"\"\"\n",
    "        item_sequences = []\n",
    "        next_item_sequences = []\n",
    "        genre_sequences = []\n",
    "        feature_sequences = []\n",
    "        processed_item_count = 0\n",
    "\n",
    "        for idx, session in enumerate(sessions):\n",
    "            session_item_sequences = []\n",
    "            session_next_item_sequences = []\n",
    "            session_genre_sequences = []\n",
    "            session_feature_sequences = []\n",
    "\n",
    "            for i in range(len(session) - 1):\n",
    "                # Process items\n",
    "                session_item_encoded = self.preprocess_song_id(session[i]['SongID'])\n",
    "                next_session_item_encoded = self.preprocess_song_id(session[i + 1]['SongID'])\n",
    "                session_item_sequences.append(session_item_encoded)\n",
    "                session_next_item_sequences.append(next_session_item_encoded)\n",
    "\n",
    "                # Process genre\n",
    "                genre_cleaned = self.clean_genre(session[i].get('spotify_genre', None))\n",
    "                session_genre_sequences.append(genre_cleaned)\n",
    "\n",
    "                # Process numerical features\n",
    "                numeric_features = []\n",
    "                for col in self.feature_columns:\n",
    "                    if col != 'spotify_genre':\n",
    "                        mean = self.mean_values.get(col, None)\n",
    "                        std = self.std_values.get(col, None)\n",
    "                        cleaned_feature = self.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n",
    "                        numeric_features.append(cleaned_feature)\n",
    "\n",
    "                session_feature_sequences.append(numeric_features)\n",
    "\n",
    "            # Filter out sessions where the next item sequence length is not greater than 10\n",
    "            if len(session_next_item_sequences) > k:\n",
    "                # Extend sequences only if the next item sequence length is greater than 10\n",
    "                item_sequences.extend(session_item_sequences)\n",
    "                next_item_sequences.extend(session_next_item_sequences)\n",
    "                genre_sequences.extend(session_genre_sequences)\n",
    "                feature_sequences.extend(session_feature_sequences)\n",
    "                processed_item_count += len(session_item_sequences)\n",
    "\n",
    "                print(f\"Session {idx + 1} processed with {len(session_item_sequences)} items.\")\n",
    "            else:\n",
    "                print(f\"Session {idx + 1} skipped because next item sequence length is {len(session_next_item_sequences)}.\")\n",
    "\n",
    "        print(f\"Total processed items: {processed_item_count}\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        item_sequences = tf.stack(item_sequences, axis=-1)\n",
    "        next_item_sequences = tf.stack(next_item_sequences, axis=-1)\n",
    "        genre_sequences_tensor = tf.constant(genre_sequences, dtype=tf.int32)\n",
    "        feature_sequences_tensor = tf.constant(feature_sequences, dtype=tf.float32)\n",
    "\n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'item': item_sequences,\n",
    "            'genre': genre_sequences_tensor,\n",
    "            'features': feature_sequences_tensor,\n",
    "            'next_item': next_item_sequences\n",
    "        })\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_session_dataset_tensor(self, k):\n",
    "        \"\"\"\n",
    "        Main function to create session dataset as tensors and return the dataset.\n",
    "        \"\"\"\n",
    "        if self.dataset is not None:\n",
    "            print(\"Dataset already created\")\n",
    "            return\n",
    "        \n",
    "        print(\"Creating session dataset\")\n",
    "        sessions_data = self.create_session_dataset(self.train_df)  # Use train data for training\n",
    "        print(\"Creating tensor dataset\")\n",
    "        dataset = self.preprocess_data(sessions_data, k=k)\n",
    "\n",
    "        # Shuffle and batch the training data\n",
    "        dataset = dataset.shuffle(buffer_size=1024).batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        return dataset\n",
    "\n",
    "    def get_test_data(self, k):\n",
    "        \"\"\"\n",
    "        Return preprocessed test dataset without shuffling.\n",
    "        \"\"\"\n",
    "        sessions_data = self.create_session_dataset(self.test_df)\n",
    "        dataset = self.preprocess_data(sessions_data, k)\n",
    "\n",
    "        # Batch the test data without shuffling\n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def batch_timer(self, dataset):\n",
    "        \"\"\"\n",
    "        Timer function to track the time taken for batch processing.\n",
    "        \"\"\"\n",
    "        for batch in dataset:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Simulate processing (e.g., model training or data transformation)\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            print(f\"Batch processing time: {batch_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating session dataset\n",
      "Creating tensor dataset\n",
      "Session 1 processed with 4 items.\n",
      "Session 2 processed with 6 items.\n",
      "Session 3 processed with 18 items.\n",
      "Session 4 skipped because next item sequence length is 1.\n",
      "Session 5 processed with 3 items.\n",
      "Session 6 processed with 10 items.\n",
      "Session 7 processed with 22 items.\n",
      "Session 8 processed with 9 items.\n",
      "Session 9 skipped because next item sequence length is 2.\n",
      "Session 10 processed with 9 items.\n",
      "Session 11 processed with 8 items.\n",
      "Session 12 processed with 9 items.\n",
      "Session 13 processed with 12 items.\n",
      "Session 14 processed with 11 items.\n",
      "Session 15 processed with 15 items.\n",
      "Session 16 skipped because next item sequence length is 2.\n",
      "Session 17 processed with 6 items.\n",
      "Session 18 processed with 14 items.\n",
      "Session 19 skipped because next item sequence length is 2.\n",
      "Session 20 processed with 8 items.\n",
      "Session 21 processed with 9 items.\n",
      "Session 22 processed with 15 items.\n",
      "Session 23 processed with 6 items.\n",
      "Session 24 processed with 6 items.\n",
      "Session 25 processed with 5 items.\n",
      "Session 26 skipped because next item sequence length is 1.\n",
      "Session 27 processed with 8 items.\n",
      "Session 28 processed with 3 items.\n",
      "Session 29 processed with 6 items.\n",
      "Session 30 processed with 6 items.\n",
      "Session 31 processed with 18 items.\n",
      "Session 32 processed with 13 items.\n",
      "Session 33 processed with 3 items.\n",
      "Session 34 skipped because next item sequence length is 0.\n",
      "Session 35 skipped because next item sequence length is 1.\n",
      "Session 36 processed with 5 items.\n",
      "Session 37 processed with 5 items.\n",
      "Session 38 processed with 6 items.\n",
      "Session 39 processed with 3 items.\n",
      "Session 40 processed with 6 items.\n",
      "Session 41 processed with 16 items.\n",
      "Session 42 processed with 9 items.\n",
      "Session 43 processed with 5 items.\n",
      "Session 44 processed with 3 items.\n",
      "Session 45 processed with 22 items.\n",
      "Session 46 processed with 11 items.\n",
      "Session 47 skipped because next item sequence length is 2.\n",
      "Session 48 processed with 6 items.\n",
      "Session 49 processed with 9 items.\n",
      "Session 50 skipped because next item sequence length is 2.\n",
      "Session 51 processed with 3 items.\n",
      "Session 52 processed with 4 items.\n",
      "Session 53 skipped because next item sequence length is 2.\n",
      "Session 54 processed with 5 items.\n",
      "Session 55 processed with 5 items.\n",
      "Session 56 processed with 3 items.\n",
      "Session 57 processed with 10 items.\n",
      "Session 58 processed with 9 items.\n",
      "Session 59 processed with 5 items.\n",
      "Session 60 processed with 4 items.\n",
      "Session 61 processed with 10 items.\n",
      "Session 62 processed with 4 items.\n",
      "Session 63 processed with 13 items.\n",
      "Session 64 processed with 6 items.\n",
      "Session 65 processed with 12 items.\n",
      "Session 66 processed with 5 items.\n",
      "Session 67 processed with 6 items.\n",
      "Session 68 processed with 14 items.\n",
      "Session 69 skipped because next item sequence length is 2.\n",
      "Session 70 skipped because next item sequence length is 2.\n",
      "Session 71 processed with 5 items.\n",
      "Session 72 processed with 5 items.\n",
      "Session 73 skipped because next item sequence length is 2.\n",
      "Session 74 processed with 3 items.\n",
      "Session 75 processed with 12 items.\n",
      "Session 76 processed with 12 items.\n",
      "Session 77 processed with 10 items.\n",
      "Session 78 skipped because next item sequence length is 1.\n",
      "Session 79 skipped because next item sequence length is 1.\n",
      "Session 80 skipped because next item sequence length is 2.\n",
      "Session 81 processed with 11 items.\n",
      "Session 82 processed with 7 items.\n",
      "Session 83 skipped because next item sequence length is 1.\n",
      "Session 84 skipped because next item sequence length is 2.\n",
      "Session 85 processed with 14 items.\n",
      "Session 86 processed with 22 items.\n",
      "Session 87 processed with 6 items.\n",
      "Session 88 skipped because next item sequence length is 2.\n",
      "Session 89 processed with 7 items.\n",
      "Session 90 processed with 5 items.\n",
      "Session 91 skipped because next item sequence length is 1.\n",
      "Session 92 skipped because next item sequence length is 2.\n",
      "Session 93 processed with 12 items.\n",
      "Session 94 skipped because next item sequence length is 1.\n",
      "Session 95 processed with 12 items.\n",
      "Session 96 skipped because next item sequence length is 2.\n",
      "Session 97 processed with 10 items.\n",
      "Session 98 processed with 13 items.\n",
      "Session 99 processed with 6 items.\n",
      "Session 100 processed with 6 items.\n",
      "Total processed items: 664\n",
      "Session 1 skipped because next item sequence length is 1.\n",
      "Session 2 skipped because next item sequence length is 2.\n",
      "Session 3 processed with 3 items.\n",
      "Session 4 skipped because next item sequence length is 0.\n",
      "Session 5 processed with 5 items.\n",
      "Session 6 skipped because next item sequence length is 2.\n",
      "Session 7 skipped because next item sequence length is 2.\n",
      "Session 8 processed with 3 items.\n",
      "Session 9 skipped because next item sequence length is 0.\n",
      "Session 10 processed with 3 items.\n",
      "Session 11 skipped because next item sequence length is 1.\n",
      "Session 12 skipped because next item sequence length is 1.\n",
      "Session 13 processed with 6 items.\n",
      "Session 14 skipped because next item sequence length is 1.\n",
      "Session 15 skipped because next item sequence length is 0.\n",
      "Session 16 skipped because next item sequence length is 2.\n",
      "Session 17 skipped because next item sequence length is 1.\n",
      "Session 18 skipped because next item sequence length is 2.\n",
      "Session 19 processed with 3 items.\n",
      "Session 20 skipped because next item sequence length is 0.\n",
      "Session 21 skipped because next item sequence length is 2.\n",
      "Session 22 skipped because next item sequence length is 0.\n",
      "Session 23 skipped because next item sequence length is 2.\n",
      "Session 24 processed with 8 items.\n",
      "Session 25 skipped because next item sequence length is 2.\n",
      "Session 26 skipped because next item sequence length is 0.\n",
      "Session 27 skipped because next item sequence length is 0.\n",
      "Session 28 skipped because next item sequence length is 0.\n",
      "Session 29 skipped because next item sequence length is 2.\n",
      "Session 30 skipped because next item sequence length is 1.\n",
      "Session 31 skipped because next item sequence length is 2.\n",
      "Session 32 skipped because next item sequence length is 0.\n",
      "Session 33 skipped because next item sequence length is 1.\n",
      "Session 34 processed with 4 items.\n",
      "Session 35 skipped because next item sequence length is 0.\n",
      "Session 36 processed with 3 items.\n",
      "Session 37 processed with 3 items.\n",
      "Session 38 skipped because next item sequence length is 2.\n",
      "Session 39 skipped because next item sequence length is 0.\n",
      "Session 40 skipped because next item sequence length is 0.\n",
      "Session 41 skipped because next item sequence length is 0.\n",
      "Session 42 skipped because next item sequence length is 0.\n",
      "Session 43 skipped because next item sequence length is 0.\n",
      "Session 44 skipped because next item sequence length is 1.\n",
      "Session 45 skipped because next item sequence length is 1.\n",
      "Session 46 skipped because next item sequence length is 1.\n",
      "Session 47 skipped because next item sequence length is 0.\n",
      "Session 48 skipped because next item sequence length is 2.\n",
      "Session 49 processed with 4 items.\n",
      "Session 50 skipped because next item sequence length is 0.\n",
      "Session 51 skipped because next item sequence length is 0.\n",
      "Session 52 processed with 3 items.\n",
      "Session 53 processed with 5 items.\n",
      "Session 54 processed with 5 items.\n",
      "Session 55 skipped because next item sequence length is 1.\n",
      "Session 56 skipped because next item sequence length is 1.\n",
      "Session 57 skipped because next item sequence length is 0.\n",
      "Session 58 skipped because next item sequence length is 1.\n",
      "Session 59 skipped because next item sequence length is 1.\n",
      "Session 60 skipped because next item sequence length is 0.\n",
      "Session 61 processed with 3 items.\n",
      "Session 62 skipped because next item sequence length is 0.\n",
      "Session 63 processed with 5 items.\n",
      "Session 64 skipped because next item sequence length is 1.\n",
      "Session 65 skipped because next item sequence length is 0.\n",
      "Session 66 skipped because next item sequence length is 0.\n",
      "Session 67 skipped because next item sequence length is 0.\n",
      "Session 68 skipped because next item sequence length is 0.\n",
      "Session 69 skipped because next item sequence length is 2.\n",
      "Session 70 skipped because next item sequence length is 0.\n",
      "Session 71 skipped because next item sequence length is 1.\n",
      "Session 72 skipped because next item sequence length is 1.\n",
      "Session 73 skipped because next item sequence length is 0.\n",
      "Session 74 processed with 3 items.\n",
      "Session 75 skipped because next item sequence length is 2.\n",
      "Session 76 processed with 4 items.\n",
      "Session 77 skipped because next item sequence length is 1.\n",
      "Session 78 skipped because next item sequence length is 2.\n",
      "Session 79 skipped because next item sequence length is 0.\n",
      "Total processed items: 73\n"
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor(df[:1000], feature_columns)\n",
    "\n",
    "# Create the session dataset tensor\n",
    "train_dataset = preprocessor.create_session_dataset_tensor(k=2)\n",
    "test_dataset = preprocessor.get_test_data(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items (SongID): [268 396 275 493 295  74 603  24 610 355 132 398 214 519 327 225]\n",
      "Genre: [[  0   0   0   0   0   0   0   0   0   0]\n",
      " [  6  68 242 256 271   0   0   0   0   0]\n",
      " [ 83 109 243 249   0   0   0   0   0   0]\n",
      " [148 150 151 153 197 205   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 38 103 122 128 200 201 271 292 306   0]\n",
      " [122 128 147 151 201 205 216 271 299   0]\n",
      " [  4  68 117 118 186 271 286 290 310   0]\n",
      " [122 128 147 151 201 205 216 271 299   0]\n",
      " [ 12  13 205 227 247 262 271 277   0   0]\n",
      " [ 19 235   0   0   0   0   0   0   0   0]\n",
      " [  6  68 118 186 242 271 286 290   0   0]\n",
      " [ 83 243 245   0   0   0   0   0   0   0]\n",
      " [ 77 235   0   0   0   0   0   0   0   0]\n",
      " [  6  21  68 140 166 254 256 271 303   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]]\n",
      "Features: [[-1.3143413   0.9073849   0.23488271  1.0598418   1.8925259  -0.6905913 ]\n",
      " [ 0.8148456   0.74032754 -0.2417433   2.1009765  -0.3540921  -0.07123876]\n",
      " [ 0.92993677  0.9266607   0.2267399   0.87914896 -0.3540921  -0.24551427]\n",
      " [ 0.29693526 -0.9045449   0.5787809  -0.74923724 -0.3540921   1.1694999 ]\n",
      " [-1.371887    0.20060375  0.7424514  -0.7277262  -0.3540921  -1.4119604 ]\n",
      " [-0.0483383  -0.44192454  1.583604   -0.8371314  -0.3540756  -1.2827619 ]\n",
      " [-1.4869782  -1.5727743   0.24465409 -0.48809314 -0.2908442  -0.97291625]\n",
      " [ 1.045028   -0.33912    -1.0915816   2.1612072  -0.3540921   0.57824427]\n",
      " [-0.68133986 -0.96879774  0.20774     2.6215436  -0.3498558  -1.2531738 ]\n",
      " [ 0.5846632  -0.9045449   0.9256647  -0.84597975  0.996514   -1.6793048 ]\n",
      " [-0.8539766   0.5411438   0.05519796  3.305595   -0.10439467  0.9990542 ]\n",
      " [ 1.1025736  -1.6241766   0.3268965   1.7567996  -0.354066    1.9096793 ]\n",
      " [ 1.3903016   0.4062128   0.82116526  0.09614687 -0.3540921   1.3092728 ]\n",
      " [ 1.2752104  -1.2322344  -0.8687399   0.03591594 -0.2908442  -2.4630024 ]\n",
      " [-0.56624866  0.05924753  0.08912635 -0.00710615 -0.35099557 -0.10123367]\n",
      " [-1.1992502  -0.24274077  0.20584    -0.6696464  -0.35387927  0.12628704]]\n",
      "Next Items (Next SongID): [149 323 337 161 331 300 486 252 416 549 609 531  58 547 510  29]\n",
      "Items (SongID): [526 110  35 434  13  15 521 400 409 445 580 501 191 458 379 450]\n",
      "Genre: [[ 11  76 198 245   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 23 121 122 128 151 201 205 213   0   0]\n",
      " [  6  68 118 142 186 246 271 276 286 290]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [  6 186 271 290 334   0   0   0   0   0]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [  6 123 133 140 271   0   0   0   0   0]\n",
      " [  6  38  68 140 256 271   0   0   0   0]\n",
      " [ 45 190 256 271   0   0   0   0   0   0]\n",
      " [  6  38  68 140 256 271   0   0   0   0]\n",
      " [ 12  13  73 137 138 140 155 156 191 227]\n",
      " [ 12 123 227 247 262 271   0   0   0   0]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [266   0   0   0   0   0   0   0   0   0]\n",
      " [266   0   0   0   0   0   0   0   0   0]]\n",
      "Features: [[ 0.0092073   1.9161543   0.628452    0.69415396 -0.3540921  -0.830093  ]\n",
      " [-1.8322517   1.0294652  -0.07182993 -0.6657744  -0.347372   -1.0301946 ]\n",
      " [-0.68133986 -0.28129247  0.48323852 -0.5952182  -0.3540921  -1.1172308 ]\n",
      " [ 0.6997544  -0.724637    0.35729635 -0.84347284  3.7306678   1.640708  ]\n",
      " [-0.68133986  0.7596034  -0.42414233 -0.76214385  1.8134661   0.31594977]\n",
      " [ 1.6204839   0.5604196  -2.816501    0.25963083 -0.35356832 -0.9717639 ]\n",
      " [ 0.23938967  0.72747695 -0.06884423  0.11765791 -0.26317325 -1.1419723 ]\n",
      " [-0.50870305 -0.57043016 -0.32642856 -0.820826   -0.3449343  -0.6793051 ]\n",
      " [ 0.92993677 -0.32626945  0.6561376  -0.8083496   3.4539583  -0.42328072]\n",
      " [ 0.5271176  -0.8531426  -0.38749966 -0.47733763 -0.33926836 -0.87700033]\n",
      " [ 0.6422088  -2.3759346   0.27831104  1.2190235  -0.29881606  1.8360308 ]\n",
      " [-0.62379426 -1.1101539   0.7077087  -0.8459247   1.8200543   2.1206267 ]\n",
      " [ 0.98748237 -1.4828204   0.49138132 -0.845941    0.33768177 -0.61331624]\n",
      " [-2.0048885  -0.76318866  0.07121216  2.38062    -0.35408017 -1.3149599 ]\n",
      " [ 0.06675289 -1.1744068   0.39611042  1.1157705  -0.2986184  -1.0298219 ]\n",
      " [-0.10588389  1.1900973   0.45853865 -0.79225934 -0.06684123  1.6466392 ]]\n",
      "Next Items (Next SongID): [110  35  53  13  15 521 400 591 445 580 222 191 458 369 450  21]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "    print(\"Items (SongID):\", batch['item'].numpy())\n",
    "    print(\"Genre:\", batch['genre'].numpy())\n",
    "    print(\"Features:\", batch['features'].numpy())\n",
    "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())\n",
    "    \n",
    "for batch in test_dataset.take(1):\n",
    "    print(\"Items (SongID):\", batch['item'].numpy())\n",
    "    print(\"Genre:\", batch['genre'].numpy())\n",
    "    print(\"Features:\", batch['features'].numpy())\n",
    "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemEmbedding(Layer):\n",
    "    def __init__(self, num_items, item_embed_dim):\n",
    "        super(ItemEmbedding, self).__init__()\n",
    "        \n",
    "        self.item_embedding = Embedding(input_dim=num_items, output_dim=item_embed_dim, mask_zero=True, name='item_embedding')\n",
    "\n",
    "    def call(self, items):\n",
    "        # Embed items\n",
    "        items_embedded = self.item_embedding(items)\n",
    "        return items_embedded\n",
    "\n",
    "class GRU4REC(Model):\n",
    "    def __init__(self, rnn_params, genre_embed_dim, item_embed_dim, ffn1_units, feature_dense_units,  preprocessed_data:DataPreprocessor):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        print(f\"items size: {preprocessed_data.items_size}\")\n",
    "        print(f\"genres size: {preprocessed_data.genres_size}\")\n",
    "        self.embedding = ItemEmbedding(preprocessed_data.items_size, item_embed_dim)\n",
    "        \n",
    "        # Genre embedding (only for genre, which is categorical and a string)\n",
    "        self.genre_embedding = Embedding(input_dim=preprocessed_data.genres_size, output_dim=genre_embed_dim, mask_zero=True, name='genre_embedding')\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn_layers = []\n",
    "        self.rnn_layers.append(GRU(**rnn_params[0], return_sequences=True, name='gru_0'))\n",
    "        for i in range(1, len(rnn_params) - 1):\n",
    "            self.rnn_layers.append(GRU(**rnn_params[i], return_sequences=True, name=f'gru_{i}'))\n",
    "        self.rnn_layers.append(GRU(**rnn_params[-1], return_sequences=False, name=f\"gru_{len(rnn_params)-1})\"))\n",
    "\n",
    "        self.concat = Concatenate(axis=-1, name='concat_1')\n",
    "        self.batch_norm = BatchNormalization(name='batchnorm')\n",
    "\n",
    "        # Feed-forward layers\n",
    "        self.feature_dense = Dense(feature_dense_units, activation='relu', name='feature_dense')  # Dense layer for features (if required)\n",
    "        self.ffn1 = Dense(ffn1_units, name='feed_forward_1')\n",
    "        self.activation1 = LeakyReLU(alpha=0.2, name='freaky_relu')\n",
    "        self.out = Dense(preprocessed_data.items_size, activation='softmax', name='output_layer')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass for the GRU4REC model.\n",
    "        :param inputs: Tuple (item_sequences, item_features, item_genres)\n",
    "        :param training: Boolean indicating if the model is in training mode\n",
    "        \"\"\"\n",
    "        \n",
    "        item_sequences, item_features, item_genres = inputs\n",
    "\n",
    "        # Embed items\n",
    "        item_embedded = self.embedding(item_sequences)\n",
    "        item_embedded = tf.expand_dims(item_embedded, axis=1)\n",
    "        # Genre embedding\n",
    "        genre_embedded = self.genre_embedding(item_genres)\n",
    "        genre_embedded = tf.reduce_mean(genre_embedded, axis=1)\n",
    "        genre_embedded = tf.expand_dims(genre_embedded, axis=1)\n",
    "        \n",
    "        # Feature transformation (features are passed directly as floats, so no embedding is needed)\n",
    "        feature_transformed = self.feature_dense(item_features)\n",
    "        feature_transformed = tf.expand_dims(feature_transformed, axis=1)\n",
    "\n",
    "        # Pass through RNN layers\n",
    "        x = item_embedded\n",
    "        x = self.rnn_layers[0](x)\n",
    "        for i in range(1, len(self.rnn_layers)):\n",
    "            x = self.concat([item_embedded, x])  # Concatenate item embeddings with RNN outputs\n",
    "            x = self.rnn_layers[i](x)\n",
    "\n",
    "        # Concatenate RNN output with feature embeddings and genre embeddings\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.concat([x, feature_transformed, genre_embedded])\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        # Feed-forward layers\n",
    "        x = self.ffn1(x)\n",
    "        x = self.activation1(x)\n",
    "        logits = self.out(x)  # Shape: (batch_size, num_items)\n",
    "\n",
    "        # Generate the sequence of items (choose the item with the highest probability using argmax)\n",
    "        predicted_items = tf.argmax(logits, axis=-1)  # (batch_size, sequence_length)\n",
    "\n",
    "        return predicted_items, logits  # Return both predicted item indices and logits (probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.metrics import Recall\n",
    "\n",
    "class RecallAtK(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k=10, name=\"recall_at_k\", **kwargs):\n",
    "        super(RecallAtK, self).__init__(name=name, **kwargs)\n",
    "        self.k = k\n",
    "        self.recall_at_k = Recall(top_k=self.k)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Update the state of the metric.\n",
    "        \"\"\"\n",
    "        # Since y_true is a list of true items and y_pred are the predicted scores,\n",
    "        # we need to calculate recall for top-k predicted items\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Calculate the top-k predicted items\n",
    "        top_k_preds = tf.argsort(y_pred, axis=-1, direction='DESCENDING')[:, :self.k]\n",
    "\n",
    "        # Calculate recall by comparing true labels with the top-k predictions\n",
    "        recall = tf.reduce_mean(tf.cast(tf.equal(y_true, top_k_preds), tf.float32), axis=-1)\n",
    "        return recall\n",
    "\n",
    "    def result(self):\n",
    "        return self.recall_at_k.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.recall_at_k.reset_state()\n",
    "\n",
    "def train_gru4rec(model, train_dataset, optimizer, loss_fn, epochs, k, val_dataset=None):\n",
    "    # Create the RecallAtK metric\n",
    "    recall_at_k = RecallAtK(k=k)\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Reset metrics for the epoch\n",
    "        # recall_at_k.reset_state()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Train the model using the dataset\n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            item_sequences = batch['item']\n",
    "            item_genres = batch['genre']\n",
    "            item_features = batch['features']\n",
    "            targets = batch['next_item']\n",
    "            # print(f\"item_sequences:\", item_sequences)\n",
    "            # print(f\"item_genres: \", item_genres)\n",
    "            # print(f\"item_features:\", item_features)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass: outputs is a tuple (predicted_items, logits)\n",
    "                predicted_items, logits = model((item_sequences, item_features, item_genres), training=True)\n",
    "                logits = tf.squeeze(logits, axis=1)\n",
    "                # print(logits)  # Log logits for debugging\n",
    "                \n",
    "                # Compute the loss using logits (not predicted_items)\n",
    "                loss = loss_fn(targets, logits)  # Use logits for loss calculation\n",
    "                print(f\"Epoch {epoch+1}, Step {step+1} Loss = {loss}\")\n",
    "                epoch_loss += loss.numpy()\n",
    "            \n",
    "            # Compute gradients and apply them\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Update Recall@k metric using logits\n",
    "            # recall_at_k.update_state(targets, logits)  # Use logits for metric calculation\n",
    "\n",
    "            # # Generate the predicted sequence (for Recall at k)\n",
    "            # input_sequence = item_sequences\n",
    "            # num_predictions = k  # Length of the sequence to predict\n",
    "\n",
    "            # predicted_sequence = []\n",
    "            # input_seq = input_sequence  # Start with the initial sequence\n",
    "\n",
    "            # for _ in range(num_predictions):\n",
    "            #     # Get the model's prediction (logits) for the next item\n",
    "            #     predicted_items, logits = model(input_seq, training=False)\n",
    "\n",
    "            #     # Add the predicted item (argmax) to the sequence\n",
    "            #     predicted_sequence.append(predicted_items)\n",
    "\n",
    "            #     # Update the input sequence by appending the predicted item\n",
    "            #     input_seq = tf.concat([input_seq, predicted_items[:, -1:]], axis=-1)  # Append the last predicted item\n",
    "\n",
    "            # predicted_sequence = tf.stack(predicted_sequence, axis=1)  # Shape: [batch_size, num_predictions]\n",
    "\n",
    "            # # Calculate recall by comparing predicted sequence with targets\n",
    "            # for batch_idx in range(predicted_sequence.shape[0]):\n",
    "            #     top_k_preds = predicted_sequence[batch_idx]  # Predicted top-k items for this batch item\n",
    "            #     true_item = targets[batch_idx]  # True next item for this batch item\n",
    "                \n",
    "            #     # Check if true item is in the top-k predictions\n",
    "            #     if true_item in top_k_preds.numpy():\n",
    "            #         recall_at_k.update_state(tf.convert_to_tensor([true_item]), predicted_sequence)  # Update recall metric\n",
    "        \n",
    "        # Calculate average loss and Recall@k for the training epoch\n",
    "        avg_loss = epoch_loss / (step + 1)\n",
    "        # train_recall_at_k = recall_at_k.result().numpy()\n",
    "        print(f\"Training loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            continue\n",
    "        \n",
    "        # Validation step\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(val_dataset):\n",
    "            item_sequences = batch['item']\n",
    "            item_genres = batch['genre']\n",
    "            item_features = batch['features']\n",
    "            targets = batch['next_item']\n",
    "            # print(f\"item_sequences:\", item_sequences)\n",
    "            # print(f\"item_genres: \", item_genres)\n",
    "            # print(f\"item_features:\", item_features)\n",
    "            # Forward pass on validation set\n",
    "            predicted_items, logits = model((item_sequences, item_features, item_genres), training=False)\n",
    "            logits = tf.squeeze(logits, axis=1)\n",
    "            loss = loss_fn(targets, logits)\n",
    "            val_loss += loss.numpy()\n",
    "            \n",
    "            # # Update validation metrics\n",
    "            # recall_at_k.update_state(targets, logits)\n",
    "        \n",
    "        avg_val_loss = val_loss / (step + 1)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "def plot_training_history(loss_history, metric_history, metric_name, top_k):\n",
    "    \"\"\"Plot the training loss and accuracy.\"\"\"\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    # Create subplots for loss and accuracy\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Plot the training loss\n",
    "    ax1.plot(epochs, loss_history, label='Loss', color='blue', linestyle='-', marker='o')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot the top-k accuracy\n",
    "    ax2.plot(epochs, metric_history, label=metric_name, color='green', linestyle='-', marker='o')\n",
    "    ax2.set_title(f'Training {metric_name}')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel(metric_name)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items size: 635\n",
      "genres size: 335\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# num_items = len()\n",
    "# feature_vocab_size = len(feature_columns)\n",
    "\n",
    "model = GRU4REC(\n",
    "    rnn_params=[\n",
    "        {\"units\": 128, \"dropout\": 0.3},\n",
    "        {\"units\": 64, \"dropout\": 0.3}\n",
    "    ],\n",
    "    item_embed_dim=16,\n",
    "    genre_embed_dim=16,\n",
    "    ffn1_units=128,\n",
    "    feature_dense_units=32,\n",
    "    preprocessed_data=preprocessor\n",
    ")\n",
    "\n",
    "# model = GRU4REC(\n",
    "#     rnn_params=[\n",
    "#         {\"units\": 128},\n",
    "#         {\"units\": 128},\n",
    "#         {\"units\": 64}\n",
    "#     ],\n",
    "#     item_embed_dim=32,\n",
    "#     genre_embed_dim=16,\n",
    "#     ffn1_units=128,\n",
    "#     feature_dense_units=64,\n",
    "#     preprocessed_data=preprocessor\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1381: UserWarning: Layer 'gru4rec_50' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: '''gru_2)' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$''\n",
      "  warnings.warn(\n",
      "e:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'gru4rec_50', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1 Loss = 6.566092014312744\n",
      "Epoch 1, Step 2 Loss = 6.406118392944336\n",
      "Epoch 1, Step 3 Loss = 6.463351249694824\n",
      "Epoch 1, Step 4 Loss = 6.360672473907471\n",
      "Epoch 1, Step 5 Loss = 6.403079986572266\n",
      "Epoch 1, Step 6 Loss = 6.541332244873047\n",
      "Epoch 1, Step 7 Loss = 6.4264678955078125\n",
      "Epoch 1, Step 8 Loss = 6.499197959899902\n",
      "Epoch 1, Step 9 Loss = 6.429801940917969\n",
      "Epoch 1, Step 10 Loss = 6.487912178039551\n",
      "Epoch 1, Step 11 Loss = 6.318941116333008\n",
      "Epoch 1, Step 12 Loss = 6.488556385040283\n",
      "Epoch 1, Step 13 Loss = 6.5233073234558105\n",
      "Epoch 1, Step 14 Loss = 6.421163082122803\n",
      "Epoch 1, Step 15 Loss = 6.533219337463379\n",
      "Epoch 1, Step 16 Loss = 6.3440260887146\n",
      "Epoch 1, Step 17 Loss = 6.391934394836426\n",
      "Epoch 1, Step 18 Loss = 6.423031806945801\n",
      "Epoch 1, Step 19 Loss = 6.40487003326416\n",
      "Epoch 1, Step 20 Loss = 6.614275932312012\n",
      "Epoch 1, Step 21 Loss = 6.607202529907227\n",
      "Epoch 1, Step 22 Loss = 6.416532039642334\n",
      "Epoch 1, Step 23 Loss = 6.544864654541016\n",
      "Epoch 1, Step 24 Loss = 6.539334297180176\n",
      "Epoch 1, Step 25 Loss = 6.399381637573242\n",
      "Epoch 1, Step 26 Loss = 6.470499038696289\n",
      "Epoch 1, Step 27 Loss = 6.504290580749512\n",
      "Epoch 1, Step 28 Loss = 6.640321731567383\n",
      "Epoch 1, Step 29 Loss = 6.694775581359863\n",
      "Epoch 1, Step 30 Loss = 6.654747009277344\n",
      "Epoch 1, Step 31 Loss = 6.361658096313477\n",
      "Epoch 1, Step 32 Loss = 6.801822662353516\n",
      "Epoch 1, Step 33 Loss = 6.455120086669922\n",
      "Epoch 1, Step 34 Loss = 6.685147285461426\n",
      "Epoch 1, Step 35 Loss = 6.353894233703613\n",
      "Epoch 1, Step 36 Loss = 6.498597145080566\n",
      "Epoch 1, Step 37 Loss = 6.345613479614258\n",
      "Epoch 1, Step 38 Loss = 6.63771915435791\n",
      "Epoch 1, Step 39 Loss = 6.615917682647705\n",
      "Epoch 1, Step 40 Loss = 6.7519707679748535\n",
      "Epoch 1, Step 41 Loss = 6.705927848815918\n",
      "Training loss: 6.5057\n",
      "Epoch 2/500\n",
      "Epoch 2, Step 1 Loss = 6.142507553100586\n",
      "Epoch 2, Step 2 Loss = 5.878403663635254\n",
      "Epoch 2, Step 3 Loss = 5.921126365661621\n",
      "Epoch 2, Step 4 Loss = 5.689504623413086\n",
      "Epoch 2, Step 5 Loss = 5.898599624633789\n",
      "Epoch 2, Step 6 Loss = 5.808345794677734\n",
      "Epoch 2, Step 7 Loss = 6.052687644958496\n",
      "Epoch 2, Step 8 Loss = 5.868320465087891\n",
      "Epoch 2, Step 9 Loss = 5.835062026977539\n",
      "Epoch 2, Step 10 Loss = 5.946185111999512\n",
      "Epoch 2, Step 11 Loss = 5.758459091186523\n",
      "Epoch 2, Step 12 Loss = 5.8557891845703125\n",
      "Epoch 2, Step 13 Loss = 5.549792766571045\n",
      "Epoch 2, Step 14 Loss = 5.751264572143555\n",
      "Epoch 2, Step 15 Loss = 5.8795247077941895\n",
      "Epoch 2, Step 16 Loss = 5.671966552734375\n",
      "Epoch 2, Step 17 Loss = 5.760157585144043\n",
      "Epoch 2, Step 18 Loss = 5.779741287231445\n",
      "Epoch 2, Step 19 Loss = 5.776950836181641\n",
      "Epoch 2, Step 20 Loss = 5.880441665649414\n",
      "Epoch 2, Step 21 Loss = 5.715507984161377\n",
      "Epoch 2, Step 22 Loss = 5.704386234283447\n",
      "Epoch 2, Step 23 Loss = 5.850940704345703\n",
      "Epoch 2, Step 24 Loss = 5.72822380065918\n",
      "Epoch 2, Step 25 Loss = 5.740085601806641\n",
      "Epoch 2, Step 26 Loss = 5.493007183074951\n",
      "Epoch 2, Step 27 Loss = 5.9580397605896\n",
      "Epoch 2, Step 28 Loss = 5.8889875411987305\n",
      "Epoch 2, Step 29 Loss = 5.726415634155273\n",
      "Epoch 2, Step 30 Loss = 5.735034465789795\n",
      "Epoch 2, Step 31 Loss = 5.556499004364014\n",
      "Epoch 2, Step 32 Loss = 5.81031608581543\n",
      "Epoch 2, Step 33 Loss = 5.610184669494629\n",
      "Epoch 2, Step 34 Loss = 5.459977149963379\n",
      "Epoch 2, Step 35 Loss = 5.825396537780762\n",
      "Epoch 2, Step 36 Loss = 5.629062175750732\n",
      "Epoch 2, Step 37 Loss = 5.609095573425293\n",
      "Epoch 2, Step 38 Loss = 5.541038513183594\n",
      "Epoch 2, Step 39 Loss = 5.533502101898193\n",
      "Epoch 2, Step 40 Loss = 5.6413726806640625\n",
      "Epoch 2, Step 41 Loss = 5.332767486572266\n",
      "Training loss: 5.7511\n",
      "Epoch 3/500\n",
      "Epoch 3, Step 1 Loss = 5.205626487731934\n",
      "Epoch 3, Step 2 Loss = 5.163343906402588\n",
      "Epoch 3, Step 3 Loss = 5.189706802368164\n",
      "Epoch 3, Step 4 Loss = 5.104473114013672\n",
      "Epoch 3, Step 5 Loss = 5.14410400390625\n",
      "Epoch 3, Step 6 Loss = 4.8878984451293945\n",
      "Epoch 3, Step 7 Loss = 4.945611953735352\n",
      "Epoch 3, Step 8 Loss = 5.053867340087891\n",
      "Epoch 3, Step 9 Loss = 4.897409439086914\n",
      "Epoch 3, Step 10 Loss = 5.010511875152588\n",
      "Epoch 3, Step 11 Loss = 5.2393107414245605\n",
      "Epoch 3, Step 12 Loss = 5.001450538635254\n",
      "Epoch 3, Step 13 Loss = 4.693367004394531\n",
      "Epoch 3, Step 14 Loss = 4.736629486083984\n",
      "Epoch 3, Step 15 Loss = 5.080654144287109\n",
      "Epoch 3, Step 16 Loss = 4.765089988708496\n",
      "Epoch 3, Step 17 Loss = 4.798856735229492\n",
      "Epoch 3, Step 18 Loss = 4.819911003112793\n",
      "Epoch 3, Step 19 Loss = 5.023964881896973\n",
      "Epoch 3, Step 20 Loss = 4.687103748321533\n",
      "Epoch 3, Step 21 Loss = 4.772839546203613\n",
      "Epoch 3, Step 22 Loss = 4.739407539367676\n",
      "Epoch 3, Step 23 Loss = 4.591978073120117\n",
      "Epoch 3, Step 24 Loss = 4.669358253479004\n",
      "Epoch 3, Step 25 Loss = 4.467838287353516\n",
      "Epoch 3, Step 26 Loss = 4.893738269805908\n",
      "Epoch 3, Step 27 Loss = 4.62120246887207\n",
      "Epoch 3, Step 28 Loss = 4.927160263061523\n",
      "Epoch 3, Step 29 Loss = 4.712797164916992\n",
      "Epoch 3, Step 30 Loss = 4.178176403045654\n",
      "Epoch 3, Step 31 Loss = 4.724108695983887\n",
      "Epoch 3, Step 32 Loss = 4.710886001586914\n",
      "Epoch 3, Step 33 Loss = 4.3457841873168945\n",
      "Epoch 3, Step 34 Loss = 4.9253034591674805\n",
      "Epoch 3, Step 35 Loss = 4.64866828918457\n",
      "Epoch 3, Step 36 Loss = 4.911111354827881\n",
      "Epoch 3, Step 37 Loss = 4.651921272277832\n",
      "Epoch 3, Step 38 Loss = 4.405494689941406\n",
      "Epoch 3, Step 39 Loss = 4.68030309677124\n",
      "Epoch 3, Step 40 Loss = 4.3033013343811035\n",
      "Epoch 3, Step 41 Loss = 4.599553108215332\n",
      "Training loss: 4.8032\n",
      "Epoch 4/500\n",
      "Epoch 4, Step 1 Loss = 3.8734424114227295\n",
      "Epoch 4, Step 2 Loss = 3.7923903465270996\n",
      "Epoch 4, Step 3 Loss = 3.8190999031066895\n",
      "Epoch 4, Step 4 Loss = 3.41560697555542\n",
      "Epoch 4, Step 5 Loss = 3.944632053375244\n",
      "Epoch 4, Step 6 Loss = 3.340156078338623\n",
      "Epoch 4, Step 7 Loss = 3.4291937351226807\n",
      "Epoch 4, Step 8 Loss = 3.874861478805542\n",
      "Epoch 4, Step 9 Loss = 3.5641541481018066\n",
      "Epoch 4, Step 10 Loss = 3.3911867141723633\n",
      "Epoch 4, Step 11 Loss = 3.5186729431152344\n",
      "Epoch 4, Step 12 Loss = 3.3880226612091064\n",
      "Epoch 4, Step 13 Loss = 2.98732852935791\n",
      "Epoch 4, Step 14 Loss = 3.7824931144714355\n",
      "Epoch 4, Step 15 Loss = 3.3869757652282715\n",
      "Epoch 4, Step 16 Loss = 3.610226631164551\n",
      "Epoch 4, Step 17 Loss = 3.407897472381592\n",
      "Epoch 4, Step 18 Loss = 3.4926352500915527\n",
      "Epoch 4, Step 19 Loss = 3.701019048690796\n",
      "Epoch 4, Step 20 Loss = 3.733638286590576\n",
      "Epoch 4, Step 21 Loss = 3.16568660736084\n",
      "Epoch 4, Step 22 Loss = 3.533773899078369\n",
      "Epoch 4, Step 23 Loss = 3.527756690979004\n",
      "Epoch 4, Step 24 Loss = 3.852393627166748\n",
      "Epoch 4, Step 25 Loss = 3.3611247539520264\n",
      "Epoch 4, Step 26 Loss = 3.7628557682037354\n",
      "Epoch 4, Step 27 Loss = 3.710522174835205\n",
      "Epoch 4, Step 28 Loss = 3.8990607261657715\n",
      "Epoch 4, Step 29 Loss = 3.57735538482666\n",
      "Epoch 4, Step 30 Loss = 3.6728153228759766\n",
      "Epoch 4, Step 31 Loss = 3.497431993484497\n",
      "Epoch 4, Step 32 Loss = 3.3947091102600098\n",
      "Epoch 4, Step 33 Loss = 3.157679796218872\n",
      "Epoch 4, Step 34 Loss = 3.509467601776123\n",
      "Epoch 4, Step 35 Loss = 3.4322502613067627\n",
      "Epoch 4, Step 36 Loss = 3.9032583236694336\n",
      "Epoch 4, Step 37 Loss = 3.4014716148376465\n",
      "Epoch 4, Step 38 Loss = 3.7483062744140625\n",
      "Epoch 4, Step 39 Loss = 3.5816240310668945\n",
      "Epoch 4, Step 40 Loss = 3.694788694381714\n",
      "Epoch 4, Step 41 Loss = 3.7295708656311035\n",
      "Training loss: 3.5748\n",
      "Epoch 5/500\n",
      "Epoch 5, Step 1 Loss = 2.771803140640259\n",
      "Epoch 5, Step 2 Loss = 2.6022396087646484\n",
      "Epoch 5, Step 3 Loss = 2.3223204612731934\n",
      "Epoch 5, Step 4 Loss = 2.6165287494659424\n",
      "Epoch 5, Step 5 Loss = 2.1408064365386963\n",
      "Epoch 5, Step 6 Loss = 2.297616481781006\n",
      "Epoch 5, Step 7 Loss = 2.603219985961914\n",
      "Epoch 5, Step 8 Loss = 2.165731191635132\n",
      "Epoch 5, Step 9 Loss = 2.042695999145508\n",
      "Epoch 5, Step 10 Loss = 2.333583354949951\n",
      "Epoch 5, Step 11 Loss = 2.561619281768799\n",
      "Epoch 5, Step 12 Loss = 2.2245383262634277\n",
      "Epoch 5, Step 13 Loss = 2.418679714202881\n",
      "Epoch 5, Step 14 Loss = 2.6902644634246826\n",
      "Epoch 5, Step 15 Loss = 2.6426916122436523\n",
      "Epoch 5, Step 16 Loss = 2.2908530235290527\n",
      "Epoch 5, Step 17 Loss = 2.588195562362671\n",
      "Epoch 5, Step 18 Loss = 2.2427682876586914\n",
      "Epoch 5, Step 19 Loss = 2.794938087463379\n",
      "Epoch 5, Step 20 Loss = 2.1642990112304688\n",
      "Epoch 5, Step 21 Loss = 2.1617307662963867\n",
      "Epoch 5, Step 22 Loss = 2.3550288677215576\n",
      "Epoch 5, Step 23 Loss = 2.4432168006896973\n",
      "Epoch 5, Step 24 Loss = 2.5176076889038086\n",
      "Epoch 5, Step 25 Loss = 2.447946071624756\n",
      "Epoch 5, Step 26 Loss = 2.218705654144287\n",
      "Epoch 5, Step 27 Loss = 2.260989189147949\n",
      "Epoch 5, Step 28 Loss = 2.6032586097717285\n",
      "Epoch 5, Step 29 Loss = 2.7613844871520996\n",
      "Epoch 5, Step 30 Loss = 2.6004769802093506\n",
      "Epoch 5, Step 31 Loss = 2.3413705825805664\n",
      "Epoch 5, Step 32 Loss = 2.9117465019226074\n",
      "Epoch 5, Step 33 Loss = 2.181525945663452\n",
      "Epoch 5, Step 34 Loss = 2.4333577156066895\n",
      "Epoch 5, Step 35 Loss = 2.449439525604248\n",
      "Epoch 5, Step 36 Loss = 2.4394187927246094\n",
      "Epoch 5, Step 37 Loss = 2.599424362182617\n",
      "Epoch 5, Step 38 Loss = 2.5126588344573975\n",
      "Epoch 5, Step 39 Loss = 1.8097729682922363\n",
      "Epoch 5, Step 40 Loss = 2.4812562465667725\n",
      "Epoch 5, Step 41 Loss = 2.875469207763672\n",
      "Training loss: 2.4371\n",
      "Epoch 6/500\n",
      "Epoch 6, Step 1 Loss = 1.4907835721969604\n",
      "Epoch 6, Step 2 Loss = 1.5171093940734863\n",
      "Epoch 6, Step 3 Loss = 2.00992488861084\n",
      "Epoch 6, Step 4 Loss = 1.3063722848892212\n",
      "Epoch 6, Step 5 Loss = 1.152766466140747\n",
      "Epoch 6, Step 6 Loss = 1.031125545501709\n",
      "Epoch 6, Step 7 Loss = 1.5432591438293457\n",
      "Epoch 6, Step 8 Loss = 1.5021709203720093\n",
      "Epoch 6, Step 9 Loss = 1.278316855430603\n",
      "Epoch 6, Step 10 Loss = 1.4182837009429932\n",
      "Epoch 6, Step 11 Loss = 1.3518121242523193\n",
      "Epoch 6, Step 12 Loss = 1.9167566299438477\n",
      "Epoch 6, Step 13 Loss = 1.464261770248413\n",
      "Epoch 6, Step 14 Loss = 1.3452285528182983\n",
      "Epoch 6, Step 15 Loss = 1.6623344421386719\n",
      "Epoch 6, Step 16 Loss = 1.657019853591919\n",
      "Epoch 6, Step 17 Loss = 1.2203996181488037\n",
      "Epoch 6, Step 18 Loss = 1.0275453329086304\n",
      "Epoch 6, Step 19 Loss = 1.4608745574951172\n",
      "Epoch 6, Step 20 Loss = 1.3161789178848267\n",
      "Epoch 6, Step 21 Loss = 1.3885066509246826\n",
      "Epoch 6, Step 22 Loss = 1.4601454734802246\n",
      "Epoch 6, Step 23 Loss = 1.3396633863449097\n",
      "Epoch 6, Step 24 Loss = 1.3442754745483398\n",
      "Epoch 6, Step 25 Loss = 1.9564523696899414\n",
      "Epoch 6, Step 26 Loss = 1.7328474521636963\n",
      "Epoch 6, Step 27 Loss = 1.5537123680114746\n",
      "Epoch 6, Step 28 Loss = 1.2643182277679443\n",
      "Epoch 6, Step 29 Loss = 1.7116189002990723\n",
      "Epoch 6, Step 30 Loss = 1.4629111289978027\n",
      "Epoch 6, Step 31 Loss = 1.4970111846923828\n",
      "Epoch 6, Step 32 Loss = 1.5846102237701416\n",
      "Epoch 6, Step 33 Loss = 1.5678068399429321\n",
      "Epoch 6, Step 34 Loss = 1.8752944469451904\n",
      "Epoch 6, Step 35 Loss = 1.6073434352874756\n",
      "Epoch 6, Step 36 Loss = 1.546907663345337\n",
      "Epoch 6, Step 37 Loss = 1.7159769535064697\n",
      "Epoch 6, Step 38 Loss = 1.8985472917556763\n",
      "Epoch 6, Step 39 Loss = 2.015214443206787\n",
      "Epoch 6, Step 40 Loss = 2.1484665870666504\n",
      "Epoch 6, Step 41 Loss = 1.9803613424301147\n",
      "Training loss: 1.5445\n",
      "Epoch 7/500\n",
      "Epoch 7, Step 1 Loss = 0.9861075282096863\n",
      "Epoch 7, Step 2 Loss = 1.0424494743347168\n",
      "Epoch 7, Step 3 Loss = 0.9372067451477051\n",
      "Epoch 7, Step 4 Loss = 0.7711807489395142\n",
      "Epoch 7, Step 5 Loss = 0.7142264246940613\n",
      "Epoch 7, Step 6 Loss = 1.083132028579712\n",
      "Epoch 7, Step 7 Loss = 1.2288767099380493\n",
      "Epoch 7, Step 8 Loss = 1.2190073728561401\n",
      "Epoch 7, Step 9 Loss = 0.6466769576072693\n",
      "Epoch 7, Step 10 Loss = 0.9773902893066406\n",
      "Epoch 7, Step 11 Loss = 1.0534511804580688\n",
      "Epoch 7, Step 12 Loss = 0.9430203437805176\n",
      "Epoch 7, Step 13 Loss = 1.0532679557800293\n",
      "Epoch 7, Step 14 Loss = 1.3667821884155273\n",
      "Epoch 7, Step 15 Loss = 1.1493902206420898\n",
      "Epoch 7, Step 16 Loss = 1.2801408767700195\n",
      "Epoch 7, Step 17 Loss = 0.971499502658844\n",
      "Epoch 7, Step 18 Loss = 1.1858621835708618\n",
      "Epoch 7, Step 19 Loss = 1.2428545951843262\n",
      "Epoch 7, Step 20 Loss = 1.1002635955810547\n",
      "Epoch 7, Step 21 Loss = 0.6875035762786865\n",
      "Epoch 7, Step 22 Loss = 1.1436939239501953\n",
      "Epoch 7, Step 23 Loss = 0.9955111742019653\n",
      "Epoch 7, Step 24 Loss = 1.0413360595703125\n",
      "Epoch 7, Step 25 Loss = 0.9878226518630981\n",
      "Epoch 7, Step 26 Loss = 1.510459542274475\n",
      "Epoch 7, Step 27 Loss = 1.1660207509994507\n",
      "Epoch 7, Step 28 Loss = 1.6234995126724243\n",
      "Epoch 7, Step 29 Loss = 1.152429223060608\n",
      "Epoch 7, Step 30 Loss = 0.6796504259109497\n",
      "Epoch 7, Step 31 Loss = 1.6063138246536255\n",
      "Epoch 7, Step 32 Loss = 1.1886115074157715\n",
      "Epoch 7, Step 33 Loss = 1.4027425050735474\n",
      "Epoch 7, Step 34 Loss = 1.253861427307129\n",
      "Epoch 7, Step 35 Loss = 1.223451852798462\n",
      "Epoch 7, Step 36 Loss = 1.641024112701416\n",
      "Epoch 7, Step 37 Loss = 1.1112806797027588\n",
      "Epoch 7, Step 38 Loss = 1.4373972415924072\n",
      "Epoch 7, Step 39 Loss = 1.089728593826294\n",
      "Epoch 7, Step 40 Loss = 1.594907522201538\n",
      "Epoch 7, Step 41 Loss = 1.9595569372177124\n",
      "Training loss: 1.1573\n",
      "Epoch 8/500\n",
      "Epoch 8, Step 1 Loss = 0.5408941507339478\n",
      "Epoch 8, Step 2 Loss = 0.9281616806983948\n",
      "Epoch 8, Step 3 Loss = 0.8812671899795532\n",
      "Epoch 8, Step 4 Loss = 1.016090989112854\n",
      "Epoch 8, Step 5 Loss = 0.8810959458351135\n",
      "Epoch 8, Step 6 Loss = 0.7410160303115845\n",
      "Epoch 8, Step 7 Loss = 0.44568493962287903\n",
      "Epoch 8, Step 8 Loss = 0.9028805494308472\n",
      "Epoch 8, Step 9 Loss = 0.6364202499389648\n",
      "Epoch 8, Step 10 Loss = 0.7439982891082764\n",
      "Epoch 8, Step 11 Loss = 1.1159956455230713\n",
      "Epoch 8, Step 12 Loss = 1.0056614875793457\n",
      "Epoch 8, Step 13 Loss = 0.6724278330802917\n",
      "Epoch 8, Step 14 Loss = 0.8403643369674683\n",
      "Epoch 8, Step 15 Loss = 0.613887369632721\n",
      "Epoch 8, Step 16 Loss = 0.8778281211853027\n",
      "Epoch 8, Step 17 Loss = 0.6227715015411377\n",
      "Epoch 8, Step 18 Loss = 0.9496783018112183\n",
      "Epoch 8, Step 19 Loss = 0.8615676164627075\n",
      "Epoch 8, Step 20 Loss = 1.0217561721801758\n",
      "Epoch 8, Step 21 Loss = 0.9566117525100708\n",
      "Epoch 8, Step 22 Loss = 1.0761810541152954\n",
      "Epoch 8, Step 23 Loss = 1.2162432670593262\n",
      "Epoch 8, Step 24 Loss = 0.3439696431159973\n",
      "Epoch 8, Step 25 Loss = 1.0847289562225342\n",
      "Epoch 8, Step 26 Loss = 1.6202000379562378\n",
      "Epoch 8, Step 27 Loss = 1.9202537536621094\n",
      "Epoch 8, Step 28 Loss = 0.8770335912704468\n",
      "Epoch 8, Step 29 Loss = 1.3258790969848633\n",
      "Epoch 8, Step 30 Loss = 0.8258675932884216\n",
      "Epoch 8, Step 31 Loss = 0.9387906193733215\n",
      "Epoch 8, Step 32 Loss = 0.9607364535331726\n",
      "Epoch 8, Step 33 Loss = 0.69069504737854\n",
      "Epoch 8, Step 34 Loss = 1.2419226169586182\n",
      "Epoch 8, Step 35 Loss = 1.3265388011932373\n",
      "Epoch 8, Step 36 Loss = 1.205979347229004\n",
      "Epoch 8, Step 37 Loss = 1.3953033685684204\n",
      "Epoch 8, Step 38 Loss = 1.5017876625061035\n",
      "Epoch 8, Step 39 Loss = 1.4512999057769775\n",
      "Epoch 8, Step 40 Loss = 0.9257302284240723\n",
      "Epoch 8, Step 41 Loss = 0.9156692028045654\n",
      "Training loss: 0.9781\n",
      "Epoch 9/500\n",
      "Epoch 9, Step 1 Loss = 0.6624488830566406\n",
      "Epoch 9, Step 2 Loss = 0.6515557169914246\n",
      "Epoch 9, Step 3 Loss = 0.40984994173049927\n",
      "Epoch 9, Step 4 Loss = 0.47582125663757324\n",
      "Epoch 9, Step 5 Loss = 0.7010485529899597\n",
      "Epoch 9, Step 6 Loss = 0.7784523963928223\n",
      "Epoch 9, Step 7 Loss = 0.41372960805892944\n",
      "Epoch 9, Step 8 Loss = 0.5226168632507324\n",
      "Epoch 9, Step 9 Loss = 0.9191307425498962\n",
      "Epoch 9, Step 10 Loss = 0.43707138299942017\n",
      "Epoch 9, Step 11 Loss = 1.0990536212921143\n",
      "Epoch 9, Step 12 Loss = 0.4929850697517395\n",
      "Epoch 9, Step 13 Loss = 1.0365500450134277\n",
      "Epoch 9, Step 14 Loss = 1.3570888042449951\n",
      "Epoch 9, Step 15 Loss = 0.8775562644004822\n",
      "Epoch 9, Step 16 Loss = 1.0692367553710938\n",
      "Epoch 9, Step 17 Loss = 0.5443814396858215\n",
      "Epoch 9, Step 18 Loss = 0.8875748515129089\n",
      "Epoch 9, Step 19 Loss = 0.8637796640396118\n",
      "Epoch 9, Step 20 Loss = 1.471859335899353\n",
      "Epoch 9, Step 21 Loss = 0.8747109174728394\n",
      "Epoch 9, Step 22 Loss = 0.9293177127838135\n",
      "Epoch 9, Step 23 Loss = 0.830096960067749\n",
      "Epoch 9, Step 24 Loss = 0.8393673896789551\n",
      "Epoch 9, Step 25 Loss = 0.8352957963943481\n",
      "Epoch 9, Step 26 Loss = 1.0331478118896484\n",
      "Epoch 9, Step 27 Loss = 1.2398221492767334\n",
      "Epoch 9, Step 28 Loss = 0.7936316728591919\n",
      "Epoch 9, Step 29 Loss = 0.5974077582359314\n",
      "Epoch 9, Step 30 Loss = 0.6259780526161194\n",
      "Epoch 9, Step 31 Loss = 1.2570955753326416\n",
      "Epoch 9, Step 32 Loss = 0.7142878770828247\n",
      "Epoch 9, Step 33 Loss = 0.7323440313339233\n",
      "Epoch 9, Step 34 Loss = 0.652938961982727\n",
      "Epoch 9, Step 35 Loss = 0.9756214022636414\n",
      "Epoch 9, Step 36 Loss = 1.3129551410675049\n",
      "Epoch 9, Step 37 Loss = 1.050229787826538\n",
      "Epoch 9, Step 38 Loss = 1.8300158977508545\n",
      "Epoch 9, Step 39 Loss = 0.7194933891296387\n",
      "Epoch 9, Step 40 Loss = 1.1363109350204468\n",
      "Epoch 9, Step 41 Loss = 1.6333118677139282\n",
      "Training loss: 0.8850\n",
      "Epoch 10/500\n",
      "Epoch 10, Step 1 Loss = 0.5804462432861328\n",
      "Epoch 10, Step 2 Loss = 0.7255194187164307\n",
      "Epoch 10, Step 3 Loss = 0.6495653390884399\n",
      "Epoch 10, Step 4 Loss = 0.6455672979354858\n",
      "Epoch 10, Step 5 Loss = 0.6000036597251892\n",
      "Epoch 10, Step 6 Loss = 0.7942105531692505\n",
      "Epoch 10, Step 7 Loss = 0.9670143127441406\n",
      "Epoch 10, Step 8 Loss = 0.5714952945709229\n",
      "Epoch 10, Step 9 Loss = 0.48203861713409424\n",
      "Epoch 10, Step 10 Loss = 0.3311004340648651\n",
      "Epoch 10, Step 11 Loss = 0.3425206243991852\n",
      "Epoch 10, Step 12 Loss = 0.9121936559677124\n",
      "Epoch 10, Step 13 Loss = 1.452796220779419\n",
      "Epoch 10, Step 14 Loss = 0.45836034417152405\n",
      "Epoch 10, Step 15 Loss = 0.663261890411377\n",
      "Epoch 10, Step 16 Loss = 0.7714290618896484\n",
      "Epoch 10, Step 17 Loss = 1.2008304595947266\n",
      "Epoch 10, Step 18 Loss = 0.6135777831077576\n",
      "Epoch 10, Step 19 Loss = 0.9435681700706482\n",
      "Epoch 10, Step 20 Loss = 1.0394670963287354\n",
      "Epoch 10, Step 21 Loss = 0.9477442502975464\n",
      "Epoch 10, Step 22 Loss = 0.7483336925506592\n",
      "Epoch 10, Step 23 Loss = 0.8332913517951965\n",
      "Epoch 10, Step 24 Loss = 0.93315589427948\n",
      "Epoch 10, Step 25 Loss = 0.9553663730621338\n",
      "Epoch 10, Step 26 Loss = 0.7281014919281006\n",
      "Epoch 10, Step 27 Loss = 0.6341626644134521\n",
      "Epoch 10, Step 28 Loss = 0.5658538341522217\n",
      "Epoch 10, Step 29 Loss = 0.8891146183013916\n",
      "Epoch 10, Step 30 Loss = 1.3305699825286865\n",
      "Epoch 10, Step 31 Loss = 1.4135336875915527\n",
      "Epoch 10, Step 32 Loss = 0.2564374804496765\n",
      "Epoch 10, Step 33 Loss = 1.0499613285064697\n",
      "Epoch 10, Step 34 Loss = 1.1157684326171875\n",
      "Epoch 10, Step 35 Loss = 1.627903699874878\n",
      "Epoch 10, Step 36 Loss = 0.9958308935165405\n",
      "Epoch 10, Step 37 Loss = 1.383800983428955\n",
      "Epoch 10, Step 38 Loss = 0.9509322643280029\n",
      "Epoch 10, Step 39 Loss = 0.8558621406555176\n",
      "Epoch 10, Step 40 Loss = 1.1591824293136597\n",
      "Epoch 10, Step 41 Loss = 1.1838667392730713\n",
      "Training loss: 0.8611\n",
      "Epoch 11/500\n",
      "Epoch 11, Step 1 Loss = 0.6158178448677063\n",
      "Epoch 11, Step 2 Loss = 0.9654597043991089\n",
      "Epoch 11, Step 3 Loss = 0.5134507417678833\n",
      "Epoch 11, Step 4 Loss = 0.7131794691085815\n",
      "Epoch 11, Step 5 Loss = 0.6909399032592773\n",
      "Epoch 11, Step 6 Loss = 0.4991512596607208\n",
      "Epoch 11, Step 7 Loss = 0.5437857508659363\n",
      "Epoch 11, Step 8 Loss = 0.8651161193847656\n",
      "Epoch 11, Step 9 Loss = 0.6620465517044067\n",
      "Epoch 11, Step 10 Loss = 0.651380181312561\n",
      "Epoch 11, Step 11 Loss = 0.9754565954208374\n",
      "Epoch 11, Step 12 Loss = 0.40886902809143066\n",
      "Epoch 11, Step 13 Loss = 0.49310949444770813\n",
      "Epoch 11, Step 14 Loss = 0.72554612159729\n",
      "Epoch 11, Step 15 Loss = 0.937178373336792\n",
      "Epoch 11, Step 16 Loss = 0.7181919813156128\n",
      "Epoch 11, Step 17 Loss = 0.6736894249916077\n",
      "Epoch 11, Step 18 Loss = 1.0372108221054077\n",
      "Epoch 11, Step 19 Loss = 0.7116906642913818\n",
      "Epoch 11, Step 20 Loss = 0.661805272102356\n",
      "Epoch 11, Step 21 Loss = 0.5314529538154602\n",
      "Epoch 11, Step 22 Loss = 0.7854204773902893\n",
      "Epoch 11, Step 23 Loss = 0.9786139726638794\n",
      "Epoch 11, Step 24 Loss = 0.7092658281326294\n",
      "Epoch 11, Step 25 Loss = 0.8417407870292664\n",
      "Epoch 11, Step 26 Loss = 0.7348484992980957\n",
      "Epoch 11, Step 27 Loss = 1.1106394529342651\n",
      "Epoch 11, Step 28 Loss = 1.0118684768676758\n",
      "Epoch 11, Step 29 Loss = 1.1767547130584717\n",
      "Epoch 11, Step 30 Loss = 1.4917206764221191\n",
      "Epoch 11, Step 31 Loss = 1.0090155601501465\n",
      "Epoch 11, Step 32 Loss = 0.7137638330459595\n",
      "Epoch 11, Step 33 Loss = 0.4577767252922058\n",
      "Epoch 11, Step 34 Loss = 0.7116572856903076\n",
      "Epoch 11, Step 35 Loss = 0.8335111141204834\n",
      "Epoch 11, Step 36 Loss = 1.259875774383545\n",
      "Epoch 11, Step 37 Loss = 0.8621834516525269\n",
      "Epoch 11, Step 38 Loss = 0.5540252923965454\n",
      "Epoch 11, Step 39 Loss = 0.7434990406036377\n",
      "Epoch 11, Step 40 Loss = 1.1369984149932861\n",
      "Epoch 11, Step 41 Loss = 0.7612010836601257\n",
      "Training loss: 0.7922\n",
      "Epoch 12/500\n",
      "Epoch 12, Step 1 Loss = 0.6080549955368042\n",
      "Epoch 12, Step 2 Loss = 0.7494192719459534\n",
      "Epoch 12, Step 3 Loss = 0.4025823473930359\n",
      "Epoch 12, Step 4 Loss = 0.47731804847717285\n",
      "Epoch 12, Step 5 Loss = 0.7339966893196106\n",
      "Epoch 12, Step 6 Loss = 0.567337691783905\n",
      "Epoch 12, Step 7 Loss = 1.0545693635940552\n",
      "Epoch 12, Step 8 Loss = 1.0126880407333374\n",
      "Epoch 12, Step 9 Loss = 0.6699138283729553\n",
      "Epoch 12, Step 10 Loss = 0.39573293924331665\n",
      "Epoch 12, Step 11 Loss = 0.4507257342338562\n",
      "Epoch 12, Step 12 Loss = 0.5533329844474792\n",
      "Epoch 12, Step 13 Loss = 0.44496774673461914\n",
      "Epoch 12, Step 14 Loss = 0.6079785823822021\n",
      "Epoch 12, Step 15 Loss = 0.6202689409255981\n",
      "Epoch 12, Step 16 Loss = 0.4000905752182007\n",
      "Epoch 12, Step 17 Loss = 0.5832921266555786\n",
      "Epoch 12, Step 18 Loss = 0.49606838822364807\n",
      "Epoch 12, Step 19 Loss = 0.9093188047409058\n",
      "Epoch 12, Step 20 Loss = 1.2038607597351074\n",
      "Epoch 12, Step 21 Loss = 0.976778507232666\n",
      "Epoch 12, Step 22 Loss = 0.7314741611480713\n",
      "Epoch 12, Step 23 Loss = 1.0051764249801636\n",
      "Epoch 12, Step 24 Loss = 1.007176399230957\n",
      "Epoch 12, Step 25 Loss = 0.7837363481521606\n",
      "Epoch 12, Step 26 Loss = 1.1461971998214722\n",
      "Epoch 12, Step 27 Loss = 0.8144145011901855\n",
      "Epoch 12, Step 28 Loss = 0.8135784864425659\n",
      "Epoch 12, Step 29 Loss = 0.35351622104644775\n",
      "Epoch 12, Step 30 Loss = 0.6079329252243042\n",
      "Epoch 12, Step 31 Loss = 1.0601555109024048\n",
      "Epoch 12, Step 32 Loss = 0.6634609699249268\n",
      "Epoch 12, Step 33 Loss = 0.3096207082271576\n",
      "Epoch 12, Step 34 Loss = 1.2656108140945435\n",
      "Epoch 12, Step 35 Loss = 0.6813483834266663\n",
      "Epoch 12, Step 36 Loss = 0.8774582147598267\n",
      "Epoch 12, Step 37 Loss = 0.8979418873786926\n",
      "Epoch 12, Step 38 Loss = 1.000859022140503\n",
      "Epoch 12, Step 39 Loss = 1.230926752090454\n",
      "Epoch 12, Step 40 Loss = 1.257201075553894\n",
      "Epoch 12, Step 41 Loss = 0.8435932397842407\n",
      "Training loss: 0.7627\n",
      "Epoch 13/500\n",
      "Epoch 13, Step 1 Loss = 1.0545480251312256\n",
      "Epoch 13, Step 2 Loss = 0.6913008093833923\n",
      "Epoch 13, Step 3 Loss = 0.4539361596107483\n",
      "Epoch 13, Step 4 Loss = 0.541501522064209\n",
      "Epoch 13, Step 5 Loss = 0.66697758436203\n",
      "Epoch 13, Step 6 Loss = 0.35812416672706604\n",
      "Epoch 13, Step 7 Loss = 0.5153504014015198\n",
      "Epoch 13, Step 8 Loss = 0.3969936966896057\n",
      "Epoch 13, Step 9 Loss = 0.7229760885238647\n",
      "Epoch 13, Step 10 Loss = 0.8046815991401672\n",
      "Epoch 13, Step 11 Loss = 0.6388893127441406\n",
      "Epoch 13, Step 12 Loss = 1.1361384391784668\n",
      "Epoch 13, Step 13 Loss = 0.9836850762367249\n",
      "Epoch 13, Step 14 Loss = 0.39036327600479126\n",
      "Epoch 13, Step 15 Loss = 0.5678321123123169\n",
      "Epoch 13, Step 16 Loss = 0.5685536861419678\n",
      "Epoch 13, Step 17 Loss = 0.6858499050140381\n",
      "Epoch 13, Step 18 Loss = 1.1945362091064453\n",
      "Epoch 13, Step 19 Loss = 0.3702806234359741\n",
      "Epoch 13, Step 20 Loss = 0.9232544302940369\n",
      "Epoch 13, Step 21 Loss = 0.937065839767456\n",
      "Epoch 13, Step 22 Loss = 0.9307698011398315\n",
      "Epoch 13, Step 23 Loss = 0.7582290768623352\n",
      "Epoch 13, Step 24 Loss = 1.4206901788711548\n",
      "Epoch 13, Step 25 Loss = 0.9312092065811157\n",
      "Epoch 13, Step 26 Loss = 0.7859328389167786\n",
      "Epoch 13, Step 27 Loss = 0.6535686254501343\n",
      "Epoch 13, Step 28 Loss = 0.7289238572120667\n",
      "Epoch 13, Step 29 Loss = 0.9829226732254028\n",
      "Epoch 13, Step 30 Loss = 0.4876931309700012\n",
      "Epoch 13, Step 31 Loss = 1.0062650442123413\n",
      "Epoch 13, Step 32 Loss = 0.7246891260147095\n",
      "Epoch 13, Step 33 Loss = 1.0020259618759155\n",
      "Epoch 13, Step 34 Loss = 0.7339161038398743\n",
      "Epoch 13, Step 35 Loss = 0.7392195463180542\n",
      "Epoch 13, Step 36 Loss = 0.6147102117538452\n",
      "Epoch 13, Step 37 Loss = 1.3868030309677124\n",
      "Epoch 13, Step 38 Loss = 1.2507282495498657\n",
      "Epoch 13, Step 39 Loss = 0.8064641952514648\n",
      "Epoch 13, Step 40 Loss = 0.7581093907356262\n",
      "Epoch 13, Step 41 Loss = 0.7939683198928833\n",
      "Training loss: 0.7829\n",
      "Epoch 14/500\n",
      "Epoch 14, Step 1 Loss = 0.4081360697746277\n",
      "Epoch 14, Step 2 Loss = 0.46851208806037903\n",
      "Epoch 14, Step 3 Loss = 0.8591898083686829\n",
      "Epoch 14, Step 4 Loss = 0.49873146414756775\n",
      "Epoch 14, Step 5 Loss = 0.3305152952671051\n",
      "Epoch 14, Step 6 Loss = 0.5264858603477478\n",
      "Epoch 14, Step 7 Loss = 0.4585370421409607\n",
      "Epoch 14, Step 8 Loss = 0.4932817220687866\n",
      "Epoch 14, Step 9 Loss = 0.2950800657272339\n",
      "Epoch 14, Step 10 Loss = 0.5007351040840149\n",
      "Epoch 14, Step 11 Loss = 1.270917534828186\n",
      "Epoch 14, Step 12 Loss = 0.720673143863678\n",
      "Epoch 14, Step 13 Loss = 0.4165313243865967\n",
      "Epoch 14, Step 14 Loss = 0.3598439693450928\n",
      "Epoch 14, Step 15 Loss = 0.7715232372283936\n",
      "Epoch 14, Step 16 Loss = 1.0015255212783813\n",
      "Epoch 14, Step 17 Loss = 1.0282907485961914\n",
      "Epoch 14, Step 18 Loss = 0.6489881277084351\n",
      "Epoch 14, Step 19 Loss = 0.6602124571800232\n",
      "Epoch 14, Step 20 Loss = 0.30302155017852783\n",
      "Epoch 14, Step 21 Loss = 1.1984871625900269\n",
      "Epoch 14, Step 22 Loss = 0.9435019493103027\n",
      "Epoch 14, Step 23 Loss = 0.9718922972679138\n",
      "Epoch 14, Step 24 Loss = 1.0118474960327148\n",
      "Epoch 14, Step 25 Loss = 1.052286148071289\n",
      "Epoch 14, Step 26 Loss = 0.6087005138397217\n",
      "Epoch 14, Step 27 Loss = 1.3612382411956787\n",
      "Epoch 14, Step 28 Loss = 0.9841926097869873\n",
      "Epoch 14, Step 29 Loss = 0.883101224899292\n",
      "Epoch 14, Step 30 Loss = 0.5089438557624817\n",
      "Epoch 14, Step 31 Loss = 0.8140013217926025\n",
      "Epoch 14, Step 32 Loss = 1.0299969911575317\n",
      "Epoch 14, Step 33 Loss = 1.0210340023040771\n",
      "Epoch 14, Step 34 Loss = 0.8804536461830139\n",
      "Epoch 14, Step 35 Loss = 0.6367608308792114\n",
      "Epoch 14, Step 36 Loss = 1.0664854049682617\n",
      "Epoch 14, Step 37 Loss = 0.6451910734176636\n",
      "Epoch 14, Step 38 Loss = 1.234629511833191\n",
      "Epoch 14, Step 39 Loss = 0.9893783330917358\n",
      "Epoch 14, Step 40 Loss = 0.5951385498046875\n",
      "Epoch 14, Step 41 Loss = 1.055688500404358\n",
      "Training loss: 0.7686\n",
      "Epoch 15/500\n",
      "Epoch 15, Step 1 Loss = 0.44226574897766113\n",
      "Epoch 15, Step 2 Loss = 1.0716989040374756\n",
      "Epoch 15, Step 3 Loss = 0.5567470788955688\n",
      "Epoch 15, Step 4 Loss = 0.3555634617805481\n",
      "Epoch 15, Step 5 Loss = 0.49107956886291504\n",
      "Epoch 15, Step 6 Loss = 1.1475553512573242\n",
      "Epoch 15, Step 7 Loss = 0.939516544342041\n",
      "Epoch 15, Step 8 Loss = 0.5923439264297485\n",
      "Epoch 15, Step 9 Loss = 0.47467130422592163\n",
      "Epoch 15, Step 10 Loss = 0.7098777890205383\n",
      "Epoch 15, Step 11 Loss = 0.5725985169410706\n",
      "Epoch 15, Step 12 Loss = 0.38990622758865356\n",
      "Epoch 15, Step 13 Loss = 0.7874293327331543\n",
      "Epoch 15, Step 14 Loss = 0.5567237138748169\n",
      "Epoch 15, Step 15 Loss = 0.9081312417984009\n",
      "Epoch 15, Step 16 Loss = 0.7042008638381958\n",
      "Epoch 15, Step 17 Loss = 0.831811785697937\n",
      "Epoch 15, Step 18 Loss = 0.6668641567230225\n",
      "Epoch 15, Step 19 Loss = 0.7267956733703613\n",
      "Epoch 15, Step 20 Loss = 0.6829877495765686\n",
      "Epoch 15, Step 21 Loss = 0.9800914525985718\n",
      "Epoch 15, Step 22 Loss = 1.0370917320251465\n",
      "Epoch 15, Step 23 Loss = 0.9724798202514648\n",
      "Epoch 15, Step 24 Loss = 0.556429922580719\n",
      "Epoch 15, Step 25 Loss = 0.994172215461731\n",
      "Epoch 15, Step 26 Loss = 0.821869969367981\n",
      "Epoch 15, Step 27 Loss = 0.6573059558868408\n",
      "Epoch 15, Step 28 Loss = 0.5778397917747498\n",
      "Epoch 15, Step 29 Loss = 0.4838818907737732\n",
      "Epoch 15, Step 30 Loss = 1.697289228439331\n",
      "Epoch 15, Step 31 Loss = 0.5753880739212036\n",
      "Epoch 15, Step 32 Loss = 0.5235412120819092\n",
      "Epoch 15, Step 33 Loss = 0.8666318655014038\n",
      "Epoch 15, Step 34 Loss = 0.5987732410430908\n",
      "Epoch 15, Step 35 Loss = 1.015498161315918\n",
      "Epoch 15, Step 36 Loss = 1.0264108180999756\n",
      "Epoch 15, Step 37 Loss = 1.0430678129196167\n",
      "Epoch 15, Step 38 Loss = 0.7301936149597168\n",
      "Epoch 15, Step 39 Loss = 1.4137024879455566\n",
      "Epoch 15, Step 40 Loss = 1.5806561708450317\n",
      "Epoch 15, Step 41 Loss = 0.4998010993003845\n",
      "Training loss: 0.7869\n",
      "Epoch 16/500\n",
      "Epoch 16, Step 1 Loss = 0.5267549157142639\n",
      "Epoch 16, Step 2 Loss = 0.18573299050331116\n",
      "Epoch 16, Step 3 Loss = 0.12989331781864166\n",
      "Epoch 16, Step 4 Loss = 0.5463614463806152\n",
      "Epoch 16, Step 5 Loss = 0.3727026581764221\n",
      "Epoch 16, Step 6 Loss = 0.44366952776908875\n",
      "Epoch 16, Step 7 Loss = 0.5259280204772949\n",
      "Epoch 16, Step 8 Loss = 0.4044318199157715\n",
      "Epoch 16, Step 9 Loss = 0.8496717214584351\n",
      "Epoch 16, Step 10 Loss = 0.49853524565696716\n",
      "Epoch 16, Step 11 Loss = 0.8490248918533325\n",
      "Epoch 16, Step 12 Loss = 0.4529663920402527\n",
      "Epoch 16, Step 13 Loss = 1.064291000366211\n",
      "Epoch 16, Step 14 Loss = 0.6522601842880249\n",
      "Epoch 16, Step 15 Loss = 0.2045792043209076\n",
      "Epoch 16, Step 16 Loss = 0.8292824029922485\n",
      "Epoch 16, Step 17 Loss = 0.4242629408836365\n",
      "Epoch 16, Step 18 Loss = 1.0306639671325684\n",
      "Epoch 16, Step 19 Loss = 0.5833911895751953\n",
      "Epoch 16, Step 20 Loss = 0.9763727188110352\n",
      "Epoch 16, Step 21 Loss = 0.7264044284820557\n",
      "Epoch 16, Step 22 Loss = 0.9938434362411499\n",
      "Epoch 16, Step 23 Loss = 0.83073890209198\n",
      "Epoch 16, Step 24 Loss = 1.0386078357696533\n",
      "Epoch 16, Step 25 Loss = 0.9786149263381958\n",
      "Epoch 16, Step 26 Loss = 1.1543095111846924\n",
      "Epoch 16, Step 27 Loss = 0.7444589138031006\n",
      "Epoch 16, Step 28 Loss = 0.9880741834640503\n",
      "Epoch 16, Step 29 Loss = 0.6004665493965149\n",
      "Epoch 16, Step 30 Loss = 0.3667432367801666\n",
      "Epoch 16, Step 31 Loss = 0.8356794118881226\n",
      "Epoch 16, Step 32 Loss = 0.9865281581878662\n",
      "Epoch 16, Step 33 Loss = 0.8399676084518433\n",
      "Epoch 16, Step 34 Loss = 0.897575318813324\n",
      "Epoch 16, Step 35 Loss = 0.9851438999176025\n",
      "Epoch 16, Step 36 Loss = 0.6365233063697815\n",
      "Epoch 16, Step 37 Loss = 0.6712885499000549\n",
      "Epoch 16, Step 38 Loss = 0.9113468527793884\n",
      "Epoch 16, Step 39 Loss = 0.6196831464767456\n",
      "Epoch 16, Step 40 Loss = 1.01162850856781\n",
      "Epoch 16, Step 41 Loss = 1.2511141300201416\n",
      "Training loss: 0.7224\n",
      "Epoch 17/500\n",
      "Epoch 17, Step 1 Loss = 0.26285967230796814\n",
      "Epoch 17, Step 2 Loss = 0.4426785707473755\n",
      "Epoch 17, Step 3 Loss = 0.8848548531532288\n",
      "Epoch 17, Step 4 Loss = 0.6638274192810059\n",
      "Epoch 17, Step 5 Loss = 0.2815087139606476\n",
      "Epoch 17, Step 6 Loss = 0.3317541778087616\n",
      "Epoch 17, Step 7 Loss = 0.7790915966033936\n",
      "Epoch 17, Step 8 Loss = 0.45924878120422363\n",
      "Epoch 17, Step 9 Loss = 0.4517374634742737\n",
      "Epoch 17, Step 10 Loss = 0.3906559944152832\n",
      "Epoch 17, Step 11 Loss = 0.6727700233459473\n",
      "Epoch 17, Step 12 Loss = 0.8608802556991577\n",
      "Epoch 17, Step 13 Loss = 0.5336951017379761\n",
      "Epoch 17, Step 14 Loss = 0.465518981218338\n",
      "Epoch 17, Step 15 Loss = 0.7483054399490356\n",
      "Epoch 17, Step 16 Loss = 0.6493294835090637\n",
      "Epoch 17, Step 17 Loss = 0.7425192594528198\n",
      "Epoch 17, Step 18 Loss = 1.086226463317871\n",
      "Epoch 17, Step 19 Loss = 0.8019450902938843\n",
      "Epoch 17, Step 20 Loss = 0.41367441415786743\n",
      "Epoch 17, Step 21 Loss = 0.6943178772926331\n",
      "Epoch 17, Step 22 Loss = 0.8789893388748169\n",
      "Epoch 17, Step 23 Loss = 0.6150385737419128\n",
      "Epoch 17, Step 24 Loss = 0.6744866967201233\n",
      "Epoch 17, Step 25 Loss = 0.4940197765827179\n",
      "Epoch 17, Step 26 Loss = 0.677344560623169\n",
      "Epoch 17, Step 27 Loss = 0.84480881690979\n",
      "Epoch 17, Step 28 Loss = 0.9934144616127014\n",
      "Epoch 17, Step 29 Loss = 0.7945734262466431\n",
      "Epoch 17, Step 30 Loss = 0.7579171657562256\n",
      "Epoch 17, Step 31 Loss = 0.6846718192100525\n",
      "Epoch 17, Step 32 Loss = 0.9914627075195312\n",
      "Epoch 17, Step 33 Loss = 0.9349403977394104\n",
      "Epoch 17, Step 34 Loss = 0.7297534942626953\n",
      "Epoch 17, Step 35 Loss = 1.39070725440979\n",
      "Epoch 17, Step 36 Loss = 0.8812029361724854\n",
      "Epoch 17, Step 37 Loss = 0.9895218014717102\n",
      "Epoch 17, Step 38 Loss = 0.7873887419700623\n",
      "Epoch 17, Step 39 Loss = 1.0965681076049805\n",
      "Epoch 17, Step 40 Loss = 1.199855089187622\n",
      "Epoch 17, Step 41 Loss = 0.95503169298172\n",
      "Training loss: 0.7314\n",
      "Epoch 18/500\n",
      "Epoch 18, Step 1 Loss = 0.4550599157810211\n",
      "Epoch 18, Step 2 Loss = 0.5533716082572937\n",
      "Epoch 18, Step 3 Loss = 0.5658679008483887\n",
      "Epoch 18, Step 4 Loss = 0.8369936943054199\n",
      "Epoch 18, Step 5 Loss = 0.42037415504455566\n",
      "Epoch 18, Step 6 Loss = 0.3492969870567322\n",
      "Epoch 18, Step 7 Loss = 0.4360883831977844\n",
      "Epoch 18, Step 8 Loss = 0.3752039074897766\n",
      "Epoch 18, Step 9 Loss = 0.6592632532119751\n",
      "Epoch 18, Step 10 Loss = 0.5121250748634338\n",
      "Epoch 18, Step 11 Loss = 0.5268101692199707\n",
      "Epoch 18, Step 12 Loss = 0.5677447319030762\n",
      "Epoch 18, Step 13 Loss = 0.7529090642929077\n",
      "Epoch 18, Step 14 Loss = 0.8788172006607056\n",
      "Epoch 18, Step 15 Loss = 0.717444896697998\n",
      "Epoch 18, Step 16 Loss = 0.6654508709907532\n",
      "Epoch 18, Step 17 Loss = 0.992290198802948\n",
      "Epoch 18, Step 18 Loss = 0.9052852392196655\n",
      "Epoch 18, Step 19 Loss = 1.0017530918121338\n",
      "Epoch 18, Step 20 Loss = 1.0159931182861328\n",
      "Epoch 18, Step 21 Loss = 0.6255201101303101\n",
      "Epoch 18, Step 22 Loss = 0.6204291582107544\n",
      "Epoch 18, Step 23 Loss = 0.42788827419281006\n",
      "Epoch 18, Step 24 Loss = 0.9766623973846436\n",
      "Epoch 18, Step 25 Loss = 0.6509935259819031\n",
      "Epoch 18, Step 26 Loss = 1.123742699623108\n",
      "Epoch 18, Step 27 Loss = 0.5186234712600708\n",
      "Epoch 18, Step 28 Loss = 0.9090449810028076\n",
      "Epoch 18, Step 29 Loss = 0.541947603225708\n",
      "Epoch 18, Step 30 Loss = 0.8370105028152466\n",
      "Epoch 18, Step 31 Loss = 0.5620785355567932\n",
      "Epoch 18, Step 32 Loss = 0.5814874768257141\n",
      "Epoch 18, Step 33 Loss = 1.1400036811828613\n",
      "Epoch 18, Step 34 Loss = 1.146668553352356\n",
      "Epoch 18, Step 35 Loss = 1.0730875730514526\n",
      "Epoch 18, Step 36 Loss = 1.0573338270187378\n",
      "Epoch 18, Step 37 Loss = 0.8461852669715881\n",
      "Epoch 18, Step 38 Loss = 0.6504043936729431\n",
      "Epoch 18, Step 39 Loss = 0.41267162561416626\n",
      "Epoch 18, Step 40 Loss = 0.7955257892608643\n",
      "Epoch 18, Step 41 Loss = 1.0756016969680786\n",
      "Training loss: 0.7259\n",
      "Epoch 19/500\n",
      "Epoch 19, Step 1 Loss = 0.8775547742843628\n",
      "Epoch 19, Step 2 Loss = 0.7807244062423706\n",
      "Epoch 19, Step 3 Loss = 0.5012195110321045\n",
      "Epoch 19, Step 4 Loss = 0.5015642046928406\n",
      "Epoch 19, Step 5 Loss = 0.5685246586799622\n",
      "Epoch 19, Step 6 Loss = 0.5002952814102173\n",
      "Epoch 19, Step 7 Loss = 0.3435065746307373\n",
      "Epoch 19, Step 8 Loss = 0.468514084815979\n",
      "Epoch 19, Step 9 Loss = 0.3264552354812622\n",
      "Epoch 19, Step 10 Loss = 0.6180800199508667\n",
      "Epoch 19, Step 11 Loss = 0.5444478392601013\n",
      "Epoch 19, Step 12 Loss = 0.5836690068244934\n",
      "Epoch 19, Step 13 Loss = 0.787443995475769\n",
      "Epoch 19, Step 14 Loss = 0.7929714918136597\n",
      "Epoch 19, Step 15 Loss = 0.5933237075805664\n",
      "Epoch 19, Step 16 Loss = 0.5090246200561523\n",
      "Epoch 19, Step 17 Loss = 0.6343995332717896\n",
      "Epoch 19, Step 18 Loss = 0.6161035299301147\n",
      "Epoch 19, Step 19 Loss = 0.6230944395065308\n",
      "Epoch 19, Step 20 Loss = 1.1310007572174072\n",
      "Epoch 19, Step 21 Loss = 0.8057713508605957\n",
      "Epoch 19, Step 22 Loss = 0.1536485254764557\n",
      "Epoch 19, Step 23 Loss = 1.0857247114181519\n",
      "Epoch 19, Step 24 Loss = 0.46555912494659424\n",
      "Epoch 19, Step 25 Loss = 0.37900781631469727\n",
      "Epoch 19, Step 26 Loss = 0.8795472383499146\n",
      "Epoch 19, Step 27 Loss = 0.35765159130096436\n",
      "Epoch 19, Step 28 Loss = 0.8342908620834351\n",
      "Epoch 19, Step 29 Loss = 0.6602764129638672\n",
      "Epoch 19, Step 30 Loss = 0.6546330451965332\n",
      "Epoch 19, Step 31 Loss = 1.1411828994750977\n",
      "Epoch 19, Step 32 Loss = 1.0138537883758545\n",
      "Epoch 19, Step 33 Loss = 0.9054287672042847\n",
      "Epoch 19, Step 34 Loss = 1.5778746604919434\n",
      "Epoch 19, Step 35 Loss = 0.5712236166000366\n",
      "Epoch 19, Step 36 Loss = 0.7058849334716797\n",
      "Epoch 19, Step 37 Loss = 0.6535431146621704\n",
      "Epoch 19, Step 38 Loss = 0.942147433757782\n",
      "Epoch 19, Step 39 Loss = 1.2301766872406006\n",
      "Epoch 19, Step 40 Loss = 1.0230475664138794\n",
      "Epoch 19, Step 41 Loss = 0.5659767985343933\n",
      "Training loss: 0.7051\n",
      "Epoch 20/500\n",
      "Epoch 20, Step 1 Loss = 0.4104539155960083\n",
      "Epoch 20, Step 2 Loss = 0.3742328882217407\n",
      "Epoch 20, Step 3 Loss = 0.8251687288284302\n",
      "Epoch 20, Step 4 Loss = 0.6295912265777588\n",
      "Epoch 20, Step 5 Loss = 0.4664512574672699\n",
      "Epoch 20, Step 6 Loss = 0.35128411650657654\n",
      "Epoch 20, Step 7 Loss = 0.361905962228775\n",
      "Epoch 20, Step 8 Loss = 0.787721574306488\n",
      "Epoch 20, Step 9 Loss = 0.6575455069541931\n",
      "Epoch 20, Step 10 Loss = 0.6163047552108765\n",
      "Epoch 20, Step 11 Loss = 0.5047598481178284\n",
      "Epoch 20, Step 12 Loss = 0.5578643083572388\n",
      "Epoch 20, Step 13 Loss = 0.7917721271514893\n",
      "Epoch 20, Step 14 Loss = 0.47136855125427246\n",
      "Epoch 20, Step 15 Loss = 0.36257147789001465\n",
      "Epoch 20, Step 16 Loss = 0.7535961270332336\n",
      "Epoch 20, Step 17 Loss = 0.45292600989341736\n",
      "Epoch 20, Step 18 Loss = 0.43542638421058655\n",
      "Epoch 20, Step 19 Loss = 1.1659197807312012\n",
      "Epoch 20, Step 20 Loss = 0.877429723739624\n",
      "Epoch 20, Step 21 Loss = 0.7339740991592407\n",
      "Epoch 20, Step 22 Loss = 0.847549319267273\n",
      "Epoch 20, Step 23 Loss = 0.6600629091262817\n",
      "Epoch 20, Step 24 Loss = 0.3158904016017914\n",
      "Epoch 20, Step 25 Loss = 0.7821881771087646\n",
      "Epoch 20, Step 26 Loss = 0.7050284147262573\n",
      "Epoch 20, Step 27 Loss = 0.8825179934501648\n",
      "Epoch 20, Step 28 Loss = 0.8317074775695801\n",
      "Epoch 20, Step 29 Loss = 0.6425907611846924\n",
      "Epoch 20, Step 30 Loss = 0.6031361818313599\n",
      "Epoch 20, Step 31 Loss = 0.7725789546966553\n",
      "Epoch 20, Step 32 Loss = 0.7978885769844055\n",
      "Epoch 20, Step 33 Loss = 0.7520912885665894\n",
      "Epoch 20, Step 34 Loss = 1.2040095329284668\n",
      "Epoch 20, Step 35 Loss = 0.8051358461380005\n",
      "Epoch 20, Step 36 Loss = 0.9716460704803467\n",
      "Epoch 20, Step 37 Loss = 0.7256970405578613\n",
      "Epoch 20, Step 38 Loss = 1.1289082765579224\n",
      "Epoch 20, Step 39 Loss = 1.545902967453003\n",
      "Epoch 20, Step 40 Loss = 0.664460301399231\n",
      "Epoch 20, Step 41 Loss = 1.2045186758041382\n",
      "Training loss: 0.7178\n",
      "Epoch 21/500\n",
      "Epoch 21, Step 1 Loss = 0.8895576000213623\n",
      "Epoch 21, Step 2 Loss = 0.6383270025253296\n",
      "Epoch 21, Step 3 Loss = 0.4535646140575409\n",
      "Epoch 21, Step 4 Loss = 0.6683813333511353\n",
      "Epoch 21, Step 5 Loss = 0.7514814138412476\n",
      "Epoch 21, Step 6 Loss = 0.5486860275268555\n",
      "Epoch 21, Step 7 Loss = 0.3084484934806824\n",
      "Epoch 21, Step 8 Loss = 0.6853058338165283\n",
      "Epoch 21, Step 9 Loss = 0.4006257951259613\n",
      "Epoch 21, Step 10 Loss = 0.30099502205848694\n",
      "Epoch 21, Step 11 Loss = 0.7198060750961304\n",
      "Epoch 21, Step 12 Loss = 0.7641464471817017\n",
      "Epoch 21, Step 13 Loss = 0.8871898651123047\n",
      "Epoch 21, Step 14 Loss = 0.41550013422966003\n",
      "Epoch 21, Step 15 Loss = 0.7019871473312378\n",
      "Epoch 21, Step 16 Loss = 1.0356967449188232\n",
      "Epoch 21, Step 17 Loss = 0.6235139966011047\n",
      "Epoch 21, Step 18 Loss = 0.6439396142959595\n",
      "Epoch 21, Step 19 Loss = 0.9656373262405396\n",
      "Epoch 21, Step 20 Loss = 0.6959990859031677\n",
      "Epoch 21, Step 21 Loss = 0.7540478706359863\n",
      "Epoch 21, Step 22 Loss = 0.526117205619812\n",
      "Epoch 21, Step 23 Loss = 0.480021595954895\n",
      "Epoch 21, Step 24 Loss = 0.8867180347442627\n",
      "Epoch 21, Step 25 Loss = 0.4981688857078552\n",
      "Epoch 21, Step 26 Loss = 0.8131353259086609\n",
      "Epoch 21, Step 27 Loss = 0.5018483996391296\n",
      "Epoch 21, Step 28 Loss = 0.602931559085846\n",
      "Epoch 21, Step 29 Loss = 0.6305968761444092\n",
      "Epoch 21, Step 30 Loss = 0.841367781162262\n",
      "Epoch 21, Step 31 Loss = 0.8305220007896423\n",
      "Epoch 21, Step 32 Loss = 0.4841368496417999\n",
      "Epoch 21, Step 33 Loss = 0.6873087882995605\n",
      "Epoch 21, Step 34 Loss = 0.7831898927688599\n",
      "Epoch 21, Step 35 Loss = 0.8954187035560608\n",
      "Epoch 21, Step 36 Loss = 1.0288439989089966\n",
      "Epoch 21, Step 37 Loss = 0.9147987365722656\n",
      "Epoch 21, Step 38 Loss = 1.2287828922271729\n",
      "Epoch 21, Step 39 Loss = 0.42743000388145447\n",
      "Epoch 21, Step 40 Loss = 0.5744979381561279\n",
      "Epoch 21, Step 41 Loss = 1.2602550983428955\n",
      "Training loss: 0.7012\n",
      "Epoch 22/500\n",
      "Epoch 22, Step 1 Loss = 0.7846682071685791\n",
      "Epoch 22, Step 2 Loss = 0.449726402759552\n",
      "Epoch 22, Step 3 Loss = 0.5541234016418457\n",
      "Epoch 22, Step 4 Loss = 0.4091702103614807\n",
      "Epoch 22, Step 5 Loss = 0.7864348888397217\n",
      "Epoch 22, Step 6 Loss = 0.25535282492637634\n",
      "Epoch 22, Step 7 Loss = 0.6022046208381653\n",
      "Epoch 22, Step 8 Loss = 0.2843276560306549\n",
      "Epoch 22, Step 9 Loss = 0.49721837043762207\n",
      "Epoch 22, Step 10 Loss = 0.6030226945877075\n",
      "Epoch 22, Step 11 Loss = 0.49452945590019226\n",
      "Epoch 22, Step 12 Loss = 0.4257983863353729\n",
      "Epoch 22, Step 13 Loss = 0.6830732226371765\n",
      "Epoch 22, Step 14 Loss = 1.021335244178772\n",
      "Epoch 22, Step 15 Loss = 0.5259746313095093\n",
      "Epoch 22, Step 16 Loss = 0.5757600665092468\n",
      "Epoch 22, Step 17 Loss = 0.7710855007171631\n",
      "Epoch 22, Step 18 Loss = 0.5733267068862915\n",
      "Epoch 22, Step 19 Loss = 0.2723442018032074\n",
      "Epoch 22, Step 20 Loss = 0.7033230066299438\n",
      "Epoch 22, Step 21 Loss = 0.624478816986084\n",
      "Epoch 22, Step 22 Loss = 0.8657039403915405\n",
      "Epoch 22, Step 23 Loss = 0.6627910137176514\n",
      "Epoch 22, Step 24 Loss = 0.919937789440155\n",
      "Epoch 22, Step 25 Loss = 0.757168173789978\n",
      "Epoch 22, Step 26 Loss = 0.7093148827552795\n",
      "Epoch 22, Step 27 Loss = 0.6821978092193604\n",
      "Epoch 22, Step 28 Loss = 0.8241860866546631\n",
      "Epoch 22, Step 29 Loss = 1.251901626586914\n",
      "Epoch 22, Step 30 Loss = 0.7427114844322205\n",
      "Epoch 22, Step 31 Loss = 0.725568950176239\n",
      "Epoch 22, Step 32 Loss = 0.3617781102657318\n",
      "Epoch 22, Step 33 Loss = 0.541122555732727\n",
      "Epoch 22, Step 34 Loss = 0.9837728142738342\n",
      "Epoch 22, Step 35 Loss = 1.0083256959915161\n",
      "Epoch 22, Step 36 Loss = 0.7113454341888428\n",
      "Epoch 22, Step 37 Loss = 0.9021662473678589\n",
      "Epoch 22, Step 38 Loss = 0.7829167246818542\n",
      "Epoch 22, Step 39 Loss = 0.5140706896781921\n",
      "Epoch 22, Step 40 Loss = 0.6807311177253723\n",
      "Epoch 22, Step 41 Loss = 0.5172920227050781\n",
      "Training loss: 0.6596\n",
      "Epoch 23/500\n",
      "Epoch 23, Step 1 Loss = 0.7142961025238037\n",
      "Epoch 23, Step 2 Loss = 0.43535926938056946\n",
      "Epoch 23, Step 3 Loss = 0.8134060502052307\n",
      "Epoch 23, Step 4 Loss = 0.25897735357284546\n",
      "Epoch 23, Step 5 Loss = 0.3202592134475708\n",
      "Epoch 23, Step 6 Loss = 0.2999441623687744\n",
      "Epoch 23, Step 7 Loss = 0.5169165134429932\n",
      "Epoch 23, Step 8 Loss = 0.3379559814929962\n",
      "Epoch 23, Step 9 Loss = 0.5071451663970947\n",
      "Epoch 23, Step 10 Loss = 0.3423152267932892\n",
      "Epoch 23, Step 11 Loss = 0.4605051279067993\n",
      "Epoch 23, Step 12 Loss = 0.47379064559936523\n",
      "Epoch 23, Step 13 Loss = 0.46218979358673096\n",
      "Epoch 23, Step 14 Loss = 0.9045470952987671\n",
      "Epoch 23, Step 15 Loss = 0.7840384244918823\n",
      "Epoch 23, Step 16 Loss = 0.770536482334137\n",
      "Epoch 23, Step 17 Loss = 0.9128897786140442\n",
      "Epoch 23, Step 18 Loss = 0.6424336433410645\n",
      "Epoch 23, Step 19 Loss = 0.4441949725151062\n",
      "Epoch 23, Step 20 Loss = 0.737678587436676\n",
      "Epoch 23, Step 21 Loss = 0.9002505540847778\n",
      "Epoch 23, Step 22 Loss = 0.5438409447669983\n",
      "Epoch 23, Step 23 Loss = 0.9912096858024597\n",
      "Epoch 23, Step 24 Loss = 0.9455893635749817\n",
      "Epoch 23, Step 25 Loss = 0.7619872093200684\n",
      "Epoch 23, Step 26 Loss = 0.3335234522819519\n",
      "Epoch 23, Step 27 Loss = 0.6950505375862122\n",
      "Epoch 23, Step 28 Loss = 0.6287987232208252\n",
      "Epoch 23, Step 29 Loss = 0.5812699198722839\n",
      "Epoch 23, Step 30 Loss = 0.9717847108840942\n",
      "Epoch 23, Step 31 Loss = 0.890526294708252\n",
      "Epoch 23, Step 32 Loss = 0.518470287322998\n",
      "Epoch 23, Step 33 Loss = 1.0657615661621094\n",
      "Epoch 23, Step 34 Loss = 0.7725476622581482\n",
      "Epoch 23, Step 35 Loss = 1.23433256149292\n",
      "Epoch 23, Step 36 Loss = 0.6074886322021484\n",
      "Epoch 23, Step 37 Loss = 0.7427219152450562\n",
      "Epoch 23, Step 38 Loss = 1.0973179340362549\n",
      "Epoch 23, Step 39 Loss = 0.9139024019241333\n",
      "Epoch 23, Step 40 Loss = 0.588520348072052\n",
      "Epoch 23, Step 41 Loss = 0.9830507636070251\n",
      "Training loss: 0.6807\n",
      "Epoch 24/500\n",
      "Epoch 24, Step 1 Loss = 0.39377421140670776\n",
      "Epoch 24, Step 2 Loss = 0.43437135219573975\n",
      "Epoch 24, Step 3 Loss = 0.42702797055244446\n",
      "Epoch 24, Step 4 Loss = 0.5339639186859131\n",
      "Epoch 24, Step 5 Loss = 0.3214820623397827\n",
      "Epoch 24, Step 6 Loss = 0.45566046237945557\n",
      "Epoch 24, Step 7 Loss = 0.40576618909835815\n",
      "Epoch 24, Step 8 Loss = 0.7234416007995605\n",
      "Epoch 24, Step 9 Loss = 0.506386935710907\n",
      "Epoch 24, Step 10 Loss = 0.6529375910758972\n",
      "Epoch 24, Step 11 Loss = 0.8444793224334717\n",
      "Epoch 24, Step 12 Loss = 0.6480966806411743\n",
      "Epoch 24, Step 13 Loss = 0.4996209740638733\n",
      "Epoch 24, Step 14 Loss = 0.6705098152160645\n",
      "Epoch 24, Step 15 Loss = 0.732046902179718\n",
      "Epoch 24, Step 16 Loss = 0.7153211832046509\n",
      "Epoch 24, Step 17 Loss = 0.7725905179977417\n",
      "Epoch 24, Step 18 Loss = 0.5311353802680969\n",
      "Epoch 24, Step 19 Loss = 0.7853524684906006\n",
      "Epoch 24, Step 20 Loss = 1.0759377479553223\n",
      "Epoch 24, Step 21 Loss = 0.5134986042976379\n",
      "Epoch 24, Step 22 Loss = 1.2411668300628662\n",
      "Epoch 24, Step 23 Loss = 0.3627496659755707\n",
      "Epoch 24, Step 24 Loss = 0.4864691495895386\n",
      "Epoch 24, Step 25 Loss = 0.5216092467308044\n",
      "Epoch 24, Step 26 Loss = 0.829166829586029\n",
      "Epoch 24, Step 27 Loss = 0.44587159156799316\n",
      "Epoch 24, Step 28 Loss = 0.6183664798736572\n",
      "Epoch 24, Step 29 Loss = 0.6451290845870972\n",
      "Epoch 24, Step 30 Loss = 1.2094252109527588\n",
      "Epoch 24, Step 31 Loss = 0.5851361155509949\n",
      "Epoch 24, Step 32 Loss = 1.0372151136398315\n",
      "Epoch 24, Step 33 Loss = 0.687890350818634\n",
      "Epoch 24, Step 34 Loss = 0.8285219669342041\n",
      "Epoch 24, Step 35 Loss = 0.6359689235687256\n",
      "Epoch 24, Step 36 Loss = 0.8256046175956726\n",
      "Epoch 24, Step 37 Loss = 0.4368644952774048\n",
      "Epoch 24, Step 38 Loss = 0.9951103925704956\n",
      "Epoch 24, Step 39 Loss = 0.6819167137145996\n",
      "Epoch 24, Step 40 Loss = 0.9395001530647278\n",
      "Epoch 24, Step 41 Loss = 0.8258510828018188\n",
      "Training loss: 0.6703\n",
      "Epoch 25/500\n",
      "Epoch 25, Step 1 Loss = 0.6087780594825745\n",
      "Epoch 25, Step 2 Loss = 0.6974622011184692\n",
      "Epoch 25, Step 3 Loss = 0.5061392784118652\n",
      "Epoch 25, Step 4 Loss = 0.4863971471786499\n",
      "Epoch 25, Step 5 Loss = 0.5437501072883606\n",
      "Epoch 25, Step 6 Loss = 0.18188919126987457\n",
      "Epoch 25, Step 7 Loss = 0.6171114444732666\n",
      "Epoch 25, Step 8 Loss = 0.5898940563201904\n",
      "Epoch 25, Step 9 Loss = 0.6169484853744507\n",
      "Epoch 25, Step 10 Loss = 0.4860169589519501\n",
      "Epoch 25, Step 11 Loss = 0.4725920557975769\n",
      "Epoch 25, Step 12 Loss = 0.7964087724685669\n",
      "Epoch 25, Step 13 Loss = 0.6628296375274658\n",
      "Epoch 25, Step 14 Loss = 0.3339194059371948\n",
      "Epoch 25, Step 15 Loss = 0.4549667239189148\n",
      "Epoch 25, Step 16 Loss = 0.7286546230316162\n",
      "Epoch 25, Step 17 Loss = 0.9126787185668945\n",
      "Epoch 25, Step 18 Loss = 0.8155324459075928\n",
      "Epoch 25, Step 19 Loss = 0.8208516836166382\n",
      "Epoch 25, Step 20 Loss = 1.1390178203582764\n",
      "Epoch 25, Step 21 Loss = 0.47902366518974304\n",
      "Epoch 25, Step 22 Loss = 0.40680932998657227\n",
      "Epoch 25, Step 23 Loss = 0.624635636806488\n",
      "Epoch 25, Step 24 Loss = 0.5174620747566223\n",
      "Epoch 25, Step 25 Loss = 1.0846824645996094\n",
      "Epoch 25, Step 26 Loss = 0.9059270620346069\n",
      "Epoch 25, Step 27 Loss = 1.0500789880752563\n",
      "Epoch 25, Step 28 Loss = 0.938559353351593\n",
      "Epoch 25, Step 29 Loss = 0.6547814607620239\n",
      "Epoch 25, Step 30 Loss = 0.4228169918060303\n",
      "Epoch 25, Step 31 Loss = 0.5576685667037964\n",
      "Epoch 25, Step 32 Loss = 0.7452849745750427\n",
      "Epoch 25, Step 33 Loss = 0.8019449710845947\n",
      "Epoch 25, Step 34 Loss = 0.8658543229103088\n",
      "Epoch 25, Step 35 Loss = 0.9667432308197021\n",
      "Epoch 25, Step 36 Loss = 1.0090537071228027\n",
      "Epoch 25, Step 37 Loss = 0.5042214393615723\n",
      "Epoch 25, Step 38 Loss = 1.0948538780212402\n",
      "Epoch 25, Step 39 Loss = 0.2590964734554291\n",
      "Epoch 25, Step 40 Loss = 0.6772618889808655\n",
      "Epoch 25, Step 41 Loss = 0.9897138476371765\n",
      "Training loss: 0.6836\n",
      "Epoch 26/500\n",
      "Epoch 26, Step 1 Loss = 0.6354784369468689\n",
      "Epoch 26, Step 2 Loss = 0.43394461274147034\n",
      "Epoch 26, Step 3 Loss = 0.3799370527267456\n",
      "Epoch 26, Step 4 Loss = 0.47827351093292236\n",
      "Epoch 26, Step 5 Loss = 0.4626794755458832\n",
      "Epoch 26, Step 6 Loss = 0.3459685444831848\n",
      "Epoch 26, Step 7 Loss = 0.513197660446167\n",
      "Epoch 26, Step 8 Loss = 0.548967719078064\n",
      "Epoch 26, Step 9 Loss = 0.2777389585971832\n",
      "Epoch 26, Step 10 Loss = 0.5705479383468628\n",
      "Epoch 26, Step 11 Loss = 0.547356903553009\n",
      "Epoch 26, Step 12 Loss = 0.9166214466094971\n",
      "Epoch 26, Step 13 Loss = 0.45900824666023254\n",
      "Epoch 26, Step 14 Loss = 0.3184332251548767\n",
      "Epoch 26, Step 15 Loss = 0.7361268997192383\n",
      "Epoch 26, Step 16 Loss = 0.40109801292419434\n",
      "Epoch 26, Step 17 Loss = 0.5695921778678894\n",
      "Epoch 26, Step 18 Loss = 0.8816811442375183\n",
      "Epoch 26, Step 19 Loss = 0.43602269887924194\n",
      "Epoch 26, Step 20 Loss = 0.41166138648986816\n",
      "Epoch 26, Step 21 Loss = 0.7183246612548828\n",
      "Epoch 26, Step 22 Loss = 0.7831984162330627\n",
      "Epoch 26, Step 23 Loss = 0.3834823668003082\n",
      "Epoch 26, Step 24 Loss = 1.0714068412780762\n",
      "Epoch 26, Step 25 Loss = 0.6639482975006104\n",
      "Epoch 26, Step 26 Loss = 0.8905144929885864\n",
      "Epoch 26, Step 27 Loss = 0.3728391230106354\n",
      "Epoch 26, Step 28 Loss = 1.178935170173645\n",
      "Epoch 26, Step 29 Loss = 0.8061349987983704\n",
      "Epoch 26, Step 30 Loss = 0.8691139817237854\n",
      "Epoch 26, Step 31 Loss = 0.6796325445175171\n",
      "Epoch 26, Step 32 Loss = 0.7707298994064331\n",
      "Epoch 26, Step 33 Loss = 1.661417007446289\n",
      "Epoch 26, Step 34 Loss = 0.9146106839179993\n",
      "Epoch 26, Step 35 Loss = 0.9211097955703735\n",
      "Epoch 26, Step 36 Loss = 1.1370265483856201\n",
      "Epoch 26, Step 37 Loss = 0.8394773006439209\n",
      "Epoch 26, Step 38 Loss = 0.6671097278594971\n",
      "Epoch 26, Step 39 Loss = 0.7807328104972839\n",
      "Epoch 26, Step 40 Loss = 0.3723728060722351\n",
      "Epoch 26, Step 41 Loss = 0.7311055660247803\n",
      "Training loss: 0.6716\n",
      "Epoch 27/500\n",
      "Epoch 27, Step 1 Loss = 0.8854162693023682\n",
      "Epoch 27, Step 2 Loss = 0.6347770690917969\n",
      "Epoch 27, Step 3 Loss = 0.2617996037006378\n",
      "Epoch 27, Step 4 Loss = 0.40983664989471436\n",
      "Epoch 27, Step 5 Loss = 0.56454998254776\n",
      "Epoch 27, Step 6 Loss = 0.7520319223403931\n",
      "Epoch 27, Step 7 Loss = 0.5540550351142883\n",
      "Epoch 27, Step 8 Loss = 0.6134054064750671\n",
      "Epoch 27, Step 9 Loss = 0.4777704179286957\n",
      "Epoch 27, Step 10 Loss = 0.4308727979660034\n",
      "Epoch 27, Step 11 Loss = 0.2766243815422058\n",
      "Epoch 27, Step 12 Loss = 0.7842610478401184\n",
      "Epoch 27, Step 13 Loss = 0.5795224905014038\n",
      "Epoch 27, Step 14 Loss = 0.4259451627731323\n",
      "Epoch 27, Step 15 Loss = 0.8920174837112427\n",
      "Epoch 27, Step 16 Loss = 0.7362284064292908\n",
      "Epoch 27, Step 17 Loss = 0.9011736512184143\n",
      "Epoch 27, Step 18 Loss = 0.724807620048523\n",
      "Epoch 27, Step 19 Loss = 0.5330866575241089\n",
      "Epoch 27, Step 20 Loss = 0.39858299493789673\n",
      "Epoch 27, Step 21 Loss = 0.41800862550735474\n",
      "Epoch 27, Step 22 Loss = 0.506289005279541\n",
      "Epoch 27, Step 23 Loss = 0.8433361053466797\n",
      "Epoch 27, Step 24 Loss = 0.6422597169876099\n",
      "Epoch 27, Step 25 Loss = 0.8798407316207886\n",
      "Epoch 27, Step 26 Loss = 0.6739933490753174\n",
      "Epoch 27, Step 27 Loss = 0.5981616377830505\n",
      "Epoch 27, Step 28 Loss = 1.0778754949569702\n",
      "Epoch 27, Step 29 Loss = 1.2782776355743408\n",
      "Epoch 27, Step 30 Loss = 0.9996312856674194\n",
      "Epoch 27, Step 31 Loss = 0.9131436347961426\n",
      "Epoch 27, Step 32 Loss = 0.5568016767501831\n",
      "Epoch 27, Step 33 Loss = 0.589135468006134\n",
      "Epoch 27, Step 34 Loss = 0.44696444272994995\n",
      "Epoch 27, Step 35 Loss = 1.032768964767456\n",
      "Epoch 27, Step 36 Loss = 0.7189521193504333\n",
      "Epoch 27, Step 37 Loss = 0.9990053772926331\n",
      "Epoch 27, Step 38 Loss = 0.3948873281478882\n",
      "Epoch 27, Step 39 Loss = 0.6707337498664856\n",
      "Epoch 27, Step 40 Loss = 1.0368070602416992\n",
      "Epoch 27, Step 41 Loss = 0.952643871307373\n",
      "Training loss: 0.6845\n",
      "Epoch 28/500\n",
      "Epoch 28, Step 1 Loss = 0.38964200019836426\n",
      "Epoch 28, Step 2 Loss = 0.6188194751739502\n",
      "Epoch 28, Step 3 Loss = 0.5904827117919922\n",
      "Epoch 28, Step 4 Loss = 0.611562967300415\n",
      "Epoch 28, Step 5 Loss = 0.3320152461528778\n",
      "Epoch 28, Step 6 Loss = 0.624600887298584\n",
      "Epoch 28, Step 7 Loss = 0.6439360976219177\n",
      "Epoch 28, Step 8 Loss = 0.668450653553009\n",
      "Epoch 28, Step 9 Loss = 0.6046472787857056\n",
      "Epoch 28, Step 10 Loss = 0.6932879686355591\n",
      "Epoch 28, Step 11 Loss = 0.8752527236938477\n",
      "Epoch 28, Step 12 Loss = 0.616226077079773\n",
      "Epoch 28, Step 13 Loss = 0.6550860404968262\n",
      "Epoch 28, Step 14 Loss = 0.46081632375717163\n",
      "Epoch 28, Step 15 Loss = 0.5933781266212463\n",
      "Epoch 28, Step 16 Loss = 0.6204964518547058\n",
      "Epoch 28, Step 17 Loss = 0.6161010265350342\n",
      "Epoch 28, Step 18 Loss = 0.716823935508728\n",
      "Epoch 28, Step 19 Loss = 0.5894312858581543\n",
      "Epoch 28, Step 20 Loss = 0.5716144442558289\n",
      "Epoch 28, Step 21 Loss = 0.8538244962692261\n",
      "Epoch 28, Step 22 Loss = 0.5438803434371948\n",
      "Epoch 28, Step 23 Loss = 0.3628333806991577\n",
      "Epoch 28, Step 24 Loss = 0.5802470445632935\n",
      "Epoch 28, Step 25 Loss = 0.9750643968582153\n",
      "Epoch 28, Step 26 Loss = 0.5911473035812378\n",
      "Epoch 28, Step 27 Loss = 0.6229751110076904\n",
      "Epoch 28, Step 28 Loss = 0.9348851442337036\n",
      "Epoch 28, Step 29 Loss = 0.9917745590209961\n",
      "Epoch 28, Step 30 Loss = 0.33975881338119507\n",
      "Epoch 28, Step 31 Loss = 0.8196184635162354\n",
      "Epoch 28, Step 32 Loss = 0.8461655378341675\n",
      "Epoch 28, Step 33 Loss = 0.43801188468933105\n",
      "Epoch 28, Step 34 Loss = 0.7111393213272095\n",
      "Epoch 28, Step 35 Loss = 0.9003347754478455\n",
      "Epoch 28, Step 36 Loss = 1.1192913055419922\n",
      "Epoch 28, Step 37 Loss = 0.8084140419960022\n",
      "Epoch 28, Step 38 Loss = 0.7009007334709167\n",
      "Epoch 28, Step 39 Loss = 0.8114882707595825\n",
      "Epoch 28, Step 40 Loss = 0.49337640404701233\n",
      "Epoch 28, Step 41 Loss = 0.587644100189209\n",
      "Training loss: 0.6616\n",
      "Epoch 29/500\n",
      "Epoch 29, Step 1 Loss = 0.36256685853004456\n",
      "Epoch 29, Step 2 Loss = 0.3437524139881134\n",
      "Epoch 29, Step 3 Loss = 0.19245389103889465\n",
      "Epoch 29, Step 4 Loss = 0.43771278858184814\n",
      "Epoch 29, Step 5 Loss = 0.8221691846847534\n",
      "Epoch 29, Step 6 Loss = 0.441364586353302\n",
      "Epoch 29, Step 7 Loss = 0.8740979433059692\n",
      "Epoch 29, Step 8 Loss = 0.6537716388702393\n",
      "Epoch 29, Step 9 Loss = 0.7370115518569946\n",
      "Epoch 29, Step 10 Loss = 0.5506138205528259\n",
      "Epoch 29, Step 11 Loss = 1.0526976585388184\n",
      "Epoch 29, Step 12 Loss = 0.8855206966400146\n",
      "Epoch 29, Step 13 Loss = 0.5424827933311462\n",
      "Epoch 29, Step 14 Loss = 0.44496041536331177\n",
      "Epoch 29, Step 15 Loss = 0.7438774108886719\n",
      "Epoch 29, Step 16 Loss = 0.5837762355804443\n",
      "Epoch 29, Step 17 Loss = 0.28439703583717346\n",
      "Epoch 29, Step 18 Loss = 0.4771690368652344\n",
      "Epoch 29, Step 19 Loss = 0.7524758577346802\n",
      "Epoch 29, Step 20 Loss = 0.6404749155044556\n",
      "Epoch 29, Step 21 Loss = 0.5810781717300415\n",
      "Epoch 29, Step 22 Loss = 0.37684082984924316\n",
      "Epoch 29, Step 23 Loss = 0.5309453010559082\n",
      "Epoch 29, Step 24 Loss = 0.5007232427597046\n",
      "Epoch 29, Step 25 Loss = 0.31917744874954224\n",
      "Epoch 29, Step 26 Loss = 0.5869078040122986\n",
      "Epoch 29, Step 27 Loss = 0.8655908107757568\n",
      "Epoch 29, Step 28 Loss = 0.5282407999038696\n",
      "Epoch 29, Step 29 Loss = 0.3177197575569153\n",
      "Epoch 29, Step 30 Loss = 0.4930086135864258\n",
      "Epoch 29, Step 31 Loss = 0.9381651878356934\n",
      "Epoch 29, Step 32 Loss = 1.4446964263916016\n",
      "Epoch 29, Step 33 Loss = 0.79317307472229\n",
      "Epoch 29, Step 34 Loss = 0.8292779922485352\n",
      "Epoch 29, Step 35 Loss = 1.159186601638794\n",
      "Epoch 29, Step 36 Loss = 0.8258957862854004\n",
      "Epoch 29, Step 37 Loss = 0.6126197576522827\n",
      "Epoch 29, Step 38 Loss = 0.9589999914169312\n",
      "Epoch 29, Step 39 Loss = 1.114420771598816\n",
      "Epoch 29, Step 40 Loss = 0.5566771030426025\n",
      "Epoch 29, Step 41 Loss = 0.6664735078811646\n",
      "Training loss: 0.6542\n",
      "Epoch 30/500\n",
      "Epoch 30, Step 1 Loss = 0.4997490644454956\n",
      "Epoch 30, Step 2 Loss = 0.7561479806900024\n",
      "Epoch 30, Step 3 Loss = 0.6589049100875854\n",
      "Epoch 30, Step 4 Loss = 0.8982617855072021\n",
      "Epoch 30, Step 5 Loss = 0.32286542654037476\n",
      "Epoch 30, Step 6 Loss = 0.6113614439964294\n",
      "Epoch 30, Step 7 Loss = 0.5842711925506592\n",
      "Epoch 30, Step 8 Loss = 0.5877553224563599\n",
      "Epoch 30, Step 9 Loss = 0.44151049852371216\n",
      "Epoch 30, Step 10 Loss = 0.5628746151924133\n",
      "Epoch 30, Step 11 Loss = 0.5935272574424744\n",
      "Epoch 30, Step 12 Loss = 0.3380798101425171\n",
      "Epoch 30, Step 13 Loss = 0.5240073800086975\n",
      "Epoch 30, Step 14 Loss = 0.6413664817810059\n",
      "Epoch 30, Step 15 Loss = 0.7885051369667053\n",
      "Epoch 30, Step 16 Loss = 0.6226940155029297\n",
      "Epoch 30, Step 17 Loss = 0.5773923397064209\n",
      "Epoch 30, Step 18 Loss = 0.5091984868049622\n",
      "Epoch 30, Step 19 Loss = 0.3786604702472687\n",
      "Epoch 30, Step 20 Loss = 0.755955159664154\n",
      "Epoch 30, Step 21 Loss = 0.7207615971565247\n",
      "Epoch 30, Step 22 Loss = 0.622536301612854\n",
      "Epoch 30, Step 23 Loss = 0.6795864701271057\n",
      "Epoch 30, Step 24 Loss = 0.7250880002975464\n",
      "Epoch 30, Step 25 Loss = 0.6933388710021973\n",
      "Epoch 30, Step 26 Loss = 0.6185983419418335\n",
      "Epoch 30, Step 27 Loss = 0.6044122576713562\n",
      "Epoch 30, Step 28 Loss = 0.746825635433197\n",
      "Epoch 30, Step 29 Loss = 0.6734174489974976\n",
      "Epoch 30, Step 30 Loss = 0.3945084810256958\n",
      "Epoch 30, Step 31 Loss = 0.739734411239624\n",
      "Epoch 30, Step 32 Loss = 0.710957407951355\n",
      "Epoch 30, Step 33 Loss = 1.050856113433838\n",
      "Epoch 30, Step 34 Loss = 0.8485708832740784\n",
      "Epoch 30, Step 35 Loss = 0.8691737651824951\n",
      "Epoch 30, Step 36 Loss = 0.9513915777206421\n",
      "Epoch 30, Step 37 Loss = 1.2236117124557495\n",
      "Epoch 30, Step 38 Loss = 1.1375091075897217\n",
      "Epoch 30, Step 39 Loss = 0.5781981945037842\n",
      "Epoch 30, Step 40 Loss = 1.1391355991363525\n",
      "Epoch 30, Step 41 Loss = 0.5689030289649963\n",
      "Training loss: 0.6817\n",
      "Epoch 31/500\n",
      "Epoch 31, Step 1 Loss = 0.43186041712760925\n",
      "Epoch 31, Step 2 Loss = 0.2075606882572174\n",
      "Epoch 31, Step 3 Loss = 0.5308111906051636\n",
      "Epoch 31, Step 4 Loss = 0.6820265054702759\n",
      "Epoch 31, Step 5 Loss = 0.45366156101226807\n",
      "Epoch 31, Step 6 Loss = 0.7411354184150696\n",
      "Epoch 31, Step 7 Loss = 0.3943427503108978\n",
      "Epoch 31, Step 8 Loss = 0.7217748165130615\n",
      "Epoch 31, Step 9 Loss = 0.46160197257995605\n",
      "Epoch 31, Step 10 Loss = 0.3041502833366394\n",
      "Epoch 31, Step 11 Loss = 0.5965251326560974\n",
      "Epoch 31, Step 12 Loss = 0.3302616477012634\n",
      "Epoch 31, Step 13 Loss = 0.4946904480457306\n",
      "Epoch 31, Step 14 Loss = 0.610328733921051\n",
      "Epoch 31, Step 15 Loss = 0.7813564538955688\n",
      "Epoch 31, Step 16 Loss = 0.5929348468780518\n",
      "Epoch 31, Step 17 Loss = 0.6037033796310425\n",
      "Epoch 31, Step 18 Loss = 0.5858302712440491\n",
      "Epoch 31, Step 19 Loss = 0.5948468446731567\n",
      "Epoch 31, Step 20 Loss = 0.7172369956970215\n",
      "Epoch 31, Step 21 Loss = 0.8083076477050781\n",
      "Epoch 31, Step 22 Loss = 0.7957350015640259\n",
      "Epoch 31, Step 23 Loss = 0.697296142578125\n",
      "Epoch 31, Step 24 Loss = 0.6171497106552124\n",
      "Epoch 31, Step 25 Loss = 0.8854163885116577\n",
      "Epoch 31, Step 26 Loss = 0.854313850402832\n",
      "Epoch 31, Step 27 Loss = 0.6506884694099426\n",
      "Epoch 31, Step 28 Loss = 1.1427931785583496\n",
      "Epoch 31, Step 29 Loss = 0.7843031883239746\n",
      "Epoch 31, Step 30 Loss = 0.6863038539886475\n",
      "Epoch 31, Step 31 Loss = 0.5188894271850586\n",
      "Epoch 31, Step 32 Loss = 1.069666862487793\n",
      "Epoch 31, Step 33 Loss = 0.4525235891342163\n",
      "Epoch 31, Step 34 Loss = 0.5808612704277039\n",
      "Epoch 31, Step 35 Loss = 0.881568431854248\n",
      "Epoch 31, Step 36 Loss = 0.6694872379302979\n",
      "Epoch 31, Step 37 Loss = 0.8439403176307678\n",
      "Epoch 31, Step 38 Loss = 0.7120482921600342\n",
      "Epoch 31, Step 39 Loss = 0.583352267742157\n",
      "Epoch 31, Step 40 Loss = 0.7834569215774536\n",
      "Epoch 31, Step 41 Loss = 0.7850912809371948\n",
      "Training loss: 0.6498\n",
      "Epoch 32/500\n",
      "Epoch 32, Step 1 Loss = 0.5127037167549133\n",
      "Epoch 32, Step 2 Loss = 0.6729788780212402\n",
      "Epoch 32, Step 3 Loss = 0.7729072570800781\n",
      "Epoch 32, Step 4 Loss = 0.5963817834854126\n",
      "Epoch 32, Step 5 Loss = 0.36593425273895264\n",
      "Epoch 32, Step 6 Loss = 0.3683878779411316\n",
      "Epoch 32, Step 7 Loss = 0.8866037130355835\n",
      "Epoch 32, Step 8 Loss = 0.7122753858566284\n",
      "Epoch 32, Step 9 Loss = 0.5365711450576782\n",
      "Epoch 32, Step 10 Loss = 0.5219855904579163\n",
      "Epoch 32, Step 11 Loss = 0.6380506157875061\n",
      "Epoch 32, Step 12 Loss = 0.3399200737476349\n",
      "Epoch 32, Step 13 Loss = 0.4291786551475525\n",
      "Epoch 32, Step 14 Loss = 0.5088804960250854\n",
      "Epoch 32, Step 15 Loss = 0.5625144243240356\n",
      "Epoch 32, Step 16 Loss = 0.6363499164581299\n",
      "Epoch 32, Step 17 Loss = 0.6216279864311218\n",
      "Epoch 32, Step 18 Loss = 0.8411705493927002\n",
      "Epoch 32, Step 19 Loss = 0.6842783689498901\n",
      "Epoch 32, Step 20 Loss = 0.4319048225879669\n",
      "Epoch 32, Step 21 Loss = 0.938744068145752\n",
      "Epoch 32, Step 22 Loss = 0.6970865726470947\n",
      "Epoch 32, Step 23 Loss = 0.8561527729034424\n",
      "Epoch 32, Step 24 Loss = 0.6627141237258911\n",
      "Epoch 32, Step 25 Loss = 0.3354984521865845\n",
      "Epoch 32, Step 26 Loss = 0.9643876552581787\n",
      "Epoch 32, Step 27 Loss = 0.3375121057033539\n",
      "Epoch 32, Step 28 Loss = 0.7376871109008789\n",
      "Epoch 32, Step 29 Loss = 0.5019800662994385\n",
      "Epoch 32, Step 30 Loss = 0.47241532802581787\n",
      "Epoch 32, Step 31 Loss = 0.8356296420097351\n",
      "Epoch 32, Step 32 Loss = 0.7193038463592529\n",
      "Epoch 32, Step 33 Loss = 0.9049475193023682\n",
      "Epoch 32, Step 34 Loss = 0.9588979482650757\n",
      "Epoch 32, Step 35 Loss = 0.7713327407836914\n",
      "Epoch 32, Step 36 Loss = 0.7279276251792908\n",
      "Epoch 32, Step 37 Loss = 1.0221984386444092\n",
      "Epoch 32, Step 38 Loss = 0.9728671908378601\n",
      "Epoch 32, Step 39 Loss = 0.2562498152256012\n",
      "Epoch 32, Step 40 Loss = 1.0221366882324219\n",
      "Epoch 32, Step 41 Loss = 0.8468875288963318\n",
      "Training loss: 0.6630\n",
      "Epoch 33/500\n",
      "Epoch 33, Step 1 Loss = 0.3393424451351166\n",
      "Epoch 33, Step 2 Loss = 0.5642123818397522\n",
      "Epoch 33, Step 3 Loss = 0.2285803109407425\n",
      "Epoch 33, Step 4 Loss = 0.6873186826705933\n",
      "Epoch 33, Step 5 Loss = 0.7927675843238831\n",
      "Epoch 33, Step 6 Loss = 0.38767576217651367\n",
      "Epoch 33, Step 7 Loss = 0.7041223049163818\n",
      "Epoch 33, Step 8 Loss = 0.6257983446121216\n",
      "Epoch 33, Step 9 Loss = 0.5101165771484375\n",
      "Epoch 33, Step 10 Loss = 0.5929528474807739\n",
      "Epoch 33, Step 11 Loss = 0.7889512777328491\n",
      "Epoch 33, Step 12 Loss = 0.43049749732017517\n",
      "Epoch 33, Step 13 Loss = 0.6272363662719727\n",
      "Epoch 33, Step 14 Loss = 0.3337082862854004\n",
      "Epoch 33, Step 15 Loss = 0.9048954248428345\n",
      "Epoch 33, Step 16 Loss = 0.844017744064331\n",
      "Epoch 33, Step 17 Loss = 0.6097856163978577\n",
      "Epoch 33, Step 18 Loss = 0.8025253415107727\n",
      "Epoch 33, Step 19 Loss = 0.6045496463775635\n",
      "Epoch 33, Step 20 Loss = 0.6337689161300659\n",
      "Epoch 33, Step 21 Loss = 0.794318437576294\n",
      "Epoch 33, Step 22 Loss = 1.0665407180786133\n",
      "Epoch 33, Step 23 Loss = 0.6011164784431458\n",
      "Epoch 33, Step 24 Loss = 0.3199305236339569\n",
      "Epoch 33, Step 25 Loss = 1.3244540691375732\n",
      "Epoch 33, Step 26 Loss = 0.20650425553321838\n",
      "Epoch 33, Step 27 Loss = 0.22824056446552277\n",
      "Epoch 33, Step 28 Loss = 0.35322514176368713\n",
      "Epoch 33, Step 29 Loss = 0.5190850496292114\n",
      "Epoch 33, Step 30 Loss = 0.8607685565948486\n",
      "Epoch 33, Step 31 Loss = 0.3685966730117798\n",
      "Epoch 33, Step 32 Loss = 0.6071015000343323\n",
      "Epoch 33, Step 33 Loss = 0.9406455755233765\n",
      "Epoch 33, Step 34 Loss = 0.8443160057067871\n",
      "Epoch 33, Step 35 Loss = 1.217193365097046\n",
      "Epoch 33, Step 36 Loss = 1.1354329586029053\n",
      "Epoch 33, Step 37 Loss = 0.7515764236450195\n",
      "Epoch 33, Step 38 Loss = 0.5723171830177307\n",
      "Epoch 33, Step 39 Loss = 0.5669442415237427\n",
      "Epoch 33, Step 40 Loss = 1.0091543197631836\n",
      "Epoch 33, Step 41 Loss = 0.6154419183731079\n",
      "Training loss: 0.6565\n",
      "Epoch 34/500\n",
      "Epoch 34, Step 1 Loss = 0.5493487119674683\n",
      "Epoch 34, Step 2 Loss = 0.6030636429786682\n",
      "Epoch 34, Step 3 Loss = 0.4728015065193176\n",
      "Epoch 34, Step 4 Loss = 0.32792994379997253\n",
      "Epoch 34, Step 5 Loss = 0.7020584344863892\n",
      "Epoch 34, Step 6 Loss = 0.9343073964118958\n",
      "Epoch 34, Step 7 Loss = 0.5885688066482544\n",
      "Epoch 34, Step 8 Loss = 0.8785034418106079\n",
      "Epoch 34, Step 9 Loss = 0.760327160358429\n",
      "Epoch 34, Step 10 Loss = 0.5969271063804626\n",
      "Epoch 34, Step 11 Loss = 0.35886648297309875\n",
      "Epoch 34, Step 12 Loss = 0.4414632022380829\n",
      "Epoch 34, Step 13 Loss = 0.7652169466018677\n",
      "Epoch 34, Step 14 Loss = 0.4417392611503601\n",
      "Epoch 34, Step 15 Loss = 0.4201313257217407\n",
      "Epoch 34, Step 16 Loss = 0.665998101234436\n",
      "Epoch 34, Step 17 Loss = 0.4129163324832916\n",
      "Epoch 34, Step 18 Loss = 0.37354210019111633\n",
      "Epoch 34, Step 19 Loss = 0.6547814011573792\n",
      "Epoch 34, Step 20 Loss = 0.43424564599990845\n",
      "Epoch 34, Step 21 Loss = 0.4870332181453705\n",
      "Epoch 34, Step 22 Loss = 0.6724772453308105\n",
      "Epoch 34, Step 23 Loss = 0.39028504490852356\n",
      "Epoch 34, Step 24 Loss = 0.8261064291000366\n",
      "Epoch 34, Step 25 Loss = 0.7848729491233826\n",
      "Epoch 34, Step 26 Loss = 0.6812878847122192\n",
      "Epoch 34, Step 27 Loss = 0.4930455684661865\n",
      "Epoch 34, Step 28 Loss = 0.8221246600151062\n",
      "Epoch 34, Step 29 Loss = 0.5321914553642273\n",
      "Epoch 34, Step 30 Loss = 0.7992304563522339\n",
      "Epoch 34, Step 31 Loss = 0.3809857666492462\n",
      "Epoch 34, Step 32 Loss = 1.180722713470459\n",
      "Epoch 34, Step 33 Loss = 0.9253054857254028\n",
      "Epoch 34, Step 34 Loss = 0.9525679349899292\n",
      "Epoch 34, Step 35 Loss = 0.6098572015762329\n",
      "Epoch 34, Step 36 Loss = 0.8296528458595276\n",
      "Epoch 34, Step 37 Loss = 0.9067173004150391\n",
      "Epoch 34, Step 38 Loss = 0.9930119514465332\n",
      "Epoch 34, Step 39 Loss = 0.6159946918487549\n",
      "Epoch 34, Step 40 Loss = 0.9518969058990479\n",
      "Epoch 34, Step 41 Loss = 0.9491270780563354\n",
      "Training loss: 0.6626\n",
      "Epoch 35/500\n",
      "Epoch 35, Step 1 Loss = 0.3075433373451233\n",
      "Epoch 35, Step 2 Loss = 1.031301736831665\n",
      "Epoch 35, Step 3 Loss = 0.6105168461799622\n",
      "Epoch 35, Step 4 Loss = 0.24168120324611664\n",
      "Epoch 35, Step 5 Loss = 0.2797364294528961\n",
      "Epoch 35, Step 6 Loss = 0.36728155612945557\n",
      "Epoch 35, Step 7 Loss = 0.4817352294921875\n",
      "Epoch 35, Step 8 Loss = 0.4421383738517761\n",
      "Epoch 35, Step 9 Loss = 0.5468964576721191\n",
      "Epoch 35, Step 10 Loss = 0.46227002143859863\n",
      "Epoch 35, Step 11 Loss = 0.5511059165000916\n",
      "Epoch 35, Step 12 Loss = 0.5642902851104736\n",
      "Epoch 35, Step 13 Loss = 0.6767393350601196\n",
      "Epoch 35, Step 14 Loss = 0.5547726154327393\n",
      "Epoch 35, Step 15 Loss = 0.8619005680084229\n",
      "Epoch 35, Step 16 Loss = 0.4142910838127136\n",
      "Epoch 35, Step 17 Loss = 0.4692225158214569\n",
      "Epoch 35, Step 18 Loss = 0.24849729239940643\n",
      "Epoch 35, Step 19 Loss = 0.574144184589386\n",
      "Epoch 35, Step 20 Loss = 0.37903159856796265\n",
      "Epoch 35, Step 21 Loss = 0.9278271198272705\n",
      "Epoch 35, Step 22 Loss = 0.9174653887748718\n",
      "Epoch 35, Step 23 Loss = 0.9471264481544495\n",
      "Epoch 35, Step 24 Loss = 0.5527973175048828\n",
      "Epoch 35, Step 25 Loss = 1.1314036846160889\n",
      "Epoch 35, Step 26 Loss = 1.1564996242523193\n",
      "Epoch 35, Step 27 Loss = 0.5668118596076965\n",
      "Epoch 35, Step 28 Loss = 0.6918221712112427\n",
      "Epoch 35, Step 29 Loss = 0.7367864847183228\n",
      "Epoch 35, Step 30 Loss = 0.8000674843788147\n",
      "Epoch 35, Step 31 Loss = 0.9316608309745789\n",
      "Epoch 35, Step 32 Loss = 0.5788407325744629\n",
      "Epoch 35, Step 33 Loss = 0.6890610456466675\n",
      "Epoch 35, Step 34 Loss = 0.8441393971443176\n",
      "Epoch 35, Step 35 Loss = 0.7870582342147827\n",
      "Epoch 35, Step 36 Loss = 0.6207655668258667\n",
      "Epoch 35, Step 37 Loss = 0.6929914951324463\n",
      "Epoch 35, Step 38 Loss = 0.7339022159576416\n",
      "Epoch 35, Step 39 Loss = 1.027116298675537\n",
      "Epoch 35, Step 40 Loss = 0.5240525007247925\n",
      "Epoch 35, Step 41 Loss = 0.8090204000473022\n",
      "Training loss: 0.6520\n",
      "Epoch 36/500\n",
      "Epoch 36, Step 1 Loss = 0.5879212617874146\n",
      "Epoch 36, Step 2 Loss = 0.3795645236968994\n",
      "Epoch 36, Step 3 Loss = 0.47710883617401123\n",
      "Epoch 36, Step 4 Loss = 0.4560719132423401\n",
      "Epoch 36, Step 5 Loss = 0.39106905460357666\n",
      "Epoch 36, Step 6 Loss = 0.7071352005004883\n",
      "Epoch 36, Step 7 Loss = 0.5389419794082642\n",
      "Epoch 36, Step 8 Loss = 0.26262468099594116\n",
      "Epoch 36, Step 9 Loss = 0.3288438320159912\n",
      "Epoch 36, Step 10 Loss = 0.7217302322387695\n",
      "Epoch 36, Step 11 Loss = 0.5927373170852661\n",
      "Epoch 36, Step 12 Loss = 0.3741961717605591\n",
      "Epoch 36, Step 13 Loss = 0.4804525077342987\n",
      "Epoch 36, Step 14 Loss = 0.5824256539344788\n",
      "Epoch 36, Step 15 Loss = 0.7299574613571167\n",
      "Epoch 36, Step 16 Loss = 0.6225484013557434\n",
      "Epoch 36, Step 17 Loss = 0.6957641839981079\n",
      "Epoch 36, Step 18 Loss = 0.8995518684387207\n",
      "Epoch 36, Step 19 Loss = 0.8206105828285217\n",
      "Epoch 36, Step 20 Loss = 0.7500492334365845\n",
      "Epoch 36, Step 21 Loss = 0.6133445501327515\n",
      "Epoch 36, Step 22 Loss = 0.5768736600875854\n",
      "Epoch 36, Step 23 Loss = 0.4069768190383911\n",
      "Epoch 36, Step 24 Loss = 1.0177820920944214\n",
      "Epoch 36, Step 25 Loss = 0.8375405669212341\n",
      "Epoch 36, Step 26 Loss = 1.0316212177276611\n",
      "Epoch 36, Step 27 Loss = 0.4593519866466522\n",
      "Epoch 36, Step 28 Loss = 0.5822380781173706\n",
      "Epoch 36, Step 29 Loss = 1.015133261680603\n",
      "Epoch 36, Step 30 Loss = 0.7608146071434021\n",
      "Epoch 36, Step 31 Loss = 0.26206955313682556\n",
      "Epoch 36, Step 32 Loss = 0.8866391777992249\n",
      "Epoch 36, Step 33 Loss = 1.0265511274337769\n",
      "Epoch 36, Step 34 Loss = 0.8363176584243774\n",
      "Epoch 36, Step 35 Loss = 0.9233335256576538\n",
      "Epoch 36, Step 36 Loss = 0.46914035081863403\n",
      "Epoch 36, Step 37 Loss = 0.8318527340888977\n",
      "Epoch 36, Step 38 Loss = 0.34444206953048706\n",
      "Epoch 36, Step 39 Loss = 0.8941627740859985\n",
      "Epoch 36, Step 40 Loss = 0.7406342625617981\n",
      "Epoch 36, Step 41 Loss = 0.9234950542449951\n",
      "Training loss: 0.6546\n",
      "Epoch 37/500\n",
      "Epoch 37, Step 1 Loss = 0.5121995210647583\n",
      "Epoch 37, Step 2 Loss = 0.6724026203155518\n",
      "Epoch 37, Step 3 Loss = 0.35964804887771606\n",
      "Epoch 37, Step 4 Loss = 0.5704063177108765\n",
      "Epoch 37, Step 5 Loss = 0.42114803194999695\n",
      "Epoch 37, Step 6 Loss = 0.6445958018302917\n",
      "Epoch 37, Step 7 Loss = 0.6920375823974609\n",
      "Epoch 37, Step 8 Loss = 0.37487801909446716\n",
      "Epoch 37, Step 9 Loss = 0.37846577167510986\n",
      "Epoch 37, Step 10 Loss = 0.46433836221694946\n",
      "Epoch 37, Step 11 Loss = 0.5009822845458984\n",
      "Epoch 37, Step 12 Loss = 0.5603864192962646\n",
      "Epoch 37, Step 13 Loss = 0.49249035120010376\n",
      "Epoch 37, Step 14 Loss = 0.7079777121543884\n",
      "Epoch 37, Step 15 Loss = 0.575883150100708\n",
      "Epoch 37, Step 16 Loss = 0.4471966028213501\n",
      "Epoch 37, Step 17 Loss = 0.7126479148864746\n",
      "Epoch 37, Step 18 Loss = 0.8376638889312744\n",
      "Epoch 37, Step 19 Loss = 0.5630472898483276\n",
      "Epoch 37, Step 20 Loss = 0.5030534267425537\n",
      "Epoch 37, Step 21 Loss = 0.567430317401886\n",
      "Epoch 37, Step 22 Loss = 0.9378089308738708\n",
      "Epoch 37, Step 23 Loss = 0.7916927337646484\n",
      "Epoch 37, Step 24 Loss = 0.22666820883750916\n",
      "Epoch 37, Step 25 Loss = 0.9651337265968323\n",
      "Epoch 37, Step 26 Loss = 0.9395740628242493\n",
      "Epoch 37, Step 27 Loss = 0.5918065309524536\n",
      "Epoch 37, Step 28 Loss = 0.866780161857605\n",
      "Epoch 37, Step 29 Loss = 0.8233925104141235\n",
      "Epoch 37, Step 30 Loss = 0.8349431753158569\n",
      "Epoch 37, Step 31 Loss = 0.35466668009757996\n",
      "Epoch 37, Step 32 Loss = 0.7338001728057861\n",
      "Epoch 37, Step 33 Loss = 0.5724084973335266\n",
      "Epoch 37, Step 34 Loss = 0.6438525915145874\n",
      "Epoch 37, Step 35 Loss = 0.5248092412948608\n",
      "Epoch 37, Step 36 Loss = 0.8784396648406982\n",
      "Epoch 37, Step 37 Loss = 0.6298366785049438\n",
      "Epoch 37, Step 38 Loss = 1.1144734621047974\n",
      "Epoch 37, Step 39 Loss = 0.6370949745178223\n",
      "Epoch 37, Step 40 Loss = 1.1617872714996338\n",
      "Epoch 37, Step 41 Loss = 1.1900544166564941\n",
      "Training loss: 0.6580\n",
      "Epoch 38/500\n",
      "Epoch 38, Step 1 Loss = 0.39335694909095764\n",
      "Epoch 38, Step 2 Loss = 0.6616604328155518\n",
      "Epoch 38, Step 3 Loss = 0.5087646245956421\n",
      "Epoch 38, Step 4 Loss = 0.6350761651992798\n",
      "Epoch 38, Step 5 Loss = 0.36712905764579773\n",
      "Epoch 38, Step 6 Loss = 0.7243999242782593\n",
      "Epoch 38, Step 7 Loss = 0.5252296328544617\n",
      "Epoch 38, Step 8 Loss = 0.6387439966201782\n",
      "Epoch 38, Step 9 Loss = 0.3879225254058838\n",
      "Epoch 38, Step 10 Loss = 0.482107013463974\n",
      "Epoch 38, Step 11 Loss = 0.6062719821929932\n",
      "Epoch 38, Step 12 Loss = 0.4933883547782898\n",
      "Epoch 38, Step 13 Loss = 0.40291526913642883\n",
      "Epoch 38, Step 14 Loss = 0.7656956911087036\n",
      "Epoch 38, Step 15 Loss = 0.6492587327957153\n",
      "Epoch 38, Step 16 Loss = 0.5371473431587219\n",
      "Epoch 38, Step 17 Loss = 0.33419352769851685\n",
      "Epoch 38, Step 18 Loss = 0.596767008304596\n",
      "Epoch 38, Step 19 Loss = 0.8523476719856262\n",
      "Epoch 38, Step 20 Loss = 0.5285118818283081\n",
      "Epoch 38, Step 21 Loss = 0.7478166818618774\n",
      "Epoch 38, Step 22 Loss = 0.6840505599975586\n",
      "Epoch 38, Step 23 Loss = 0.6749482750892639\n",
      "Epoch 38, Step 24 Loss = 0.8627225160598755\n",
      "Epoch 38, Step 25 Loss = 0.7199897766113281\n",
      "Epoch 38, Step 26 Loss = 0.7916536331176758\n",
      "Epoch 38, Step 27 Loss = 0.7699800729751587\n",
      "Epoch 38, Step 28 Loss = 0.8892236948013306\n",
      "Epoch 38, Step 29 Loss = 0.5989977717399597\n",
      "Epoch 38, Step 30 Loss = 0.3425924479961395\n",
      "Epoch 38, Step 31 Loss = 0.6062610149383545\n",
      "Epoch 38, Step 32 Loss = 0.8542804718017578\n",
      "Epoch 38, Step 33 Loss = 0.6746730804443359\n",
      "Epoch 38, Step 34 Loss = 0.3844897150993347\n",
      "Epoch 38, Step 35 Loss = 0.36961978673934937\n",
      "Epoch 38, Step 36 Loss = 0.8819437026977539\n",
      "Epoch 38, Step 37 Loss = 0.5935207009315491\n",
      "Epoch 38, Step 38 Loss = 0.5252008438110352\n",
      "Epoch 38, Step 39 Loss = 0.7734524607658386\n",
      "Epoch 38, Step 40 Loss = 0.7489790916442871\n",
      "Epoch 38, Step 41 Loss = 0.8311545848846436\n",
      "Training loss: 0.6199\n",
      "Epoch 39/500\n",
      "Epoch 39, Step 1 Loss = 0.6858835220336914\n",
      "Epoch 39, Step 2 Loss = 0.6639726161956787\n",
      "Epoch 39, Step 3 Loss = 0.3591158390045166\n",
      "Epoch 39, Step 4 Loss = 0.50135338306427\n",
      "Epoch 39, Step 5 Loss = 0.583803117275238\n",
      "Epoch 39, Step 6 Loss = 0.6680976152420044\n",
      "Epoch 39, Step 7 Loss = 0.7860992550849915\n",
      "Epoch 39, Step 8 Loss = 0.44282013177871704\n",
      "Epoch 39, Step 9 Loss = 1.1352514028549194\n",
      "Epoch 39, Step 10 Loss = 0.7053643465042114\n",
      "Epoch 39, Step 11 Loss = 0.4777050018310547\n",
      "Epoch 39, Step 12 Loss = 0.4982205629348755\n",
      "Epoch 39, Step 13 Loss = 0.44499167799949646\n",
      "Epoch 39, Step 14 Loss = 0.7020910978317261\n",
      "Epoch 39, Step 15 Loss = 0.4361768066883087\n",
      "Epoch 39, Step 16 Loss = 0.3148936629295349\n",
      "Epoch 39, Step 17 Loss = 0.5750853419303894\n",
      "Epoch 39, Step 18 Loss = 0.3710809051990509\n",
      "Epoch 39, Step 19 Loss = 0.6028193831443787\n",
      "Epoch 39, Step 20 Loss = 0.5528470277786255\n",
      "Epoch 39, Step 21 Loss = 0.5434267520904541\n",
      "Epoch 39, Step 22 Loss = 0.8822497725486755\n",
      "Epoch 39, Step 23 Loss = 0.8923028111457825\n",
      "Epoch 39, Step 24 Loss = 0.22013181447982788\n",
      "Epoch 39, Step 25 Loss = 0.6194571256637573\n",
      "Epoch 39, Step 26 Loss = 1.018660545349121\n",
      "Epoch 39, Step 27 Loss = 0.718302845954895\n",
      "Epoch 39, Step 28 Loss = 0.3098656237125397\n",
      "Epoch 39, Step 29 Loss = 0.79986971616745\n",
      "Epoch 39, Step 30 Loss = 0.7109887599945068\n",
      "Epoch 39, Step 31 Loss = 0.4418458342552185\n",
      "Epoch 39, Step 32 Loss = 0.8099876642227173\n",
      "Epoch 39, Step 33 Loss = 0.670921802520752\n",
      "Epoch 39, Step 34 Loss = 0.6470942497253418\n",
      "Epoch 39, Step 35 Loss = 0.7054842710494995\n",
      "Epoch 39, Step 36 Loss = 0.44620856642723083\n",
      "Epoch 39, Step 37 Loss = 0.970567524433136\n",
      "Epoch 39, Step 38 Loss = 1.2412793636322021\n",
      "Epoch 39, Step 39 Loss = 0.7929880619049072\n",
      "Epoch 39, Step 40 Loss = 0.7103294730186462\n",
      "Epoch 39, Step 41 Loss = 0.5480972528457642\n",
      "Training loss: 0.6392\n",
      "Epoch 40/500\n",
      "Epoch 40, Step 1 Loss = 0.4478633403778076\n",
      "Epoch 40, Step 2 Loss = 0.385186105966568\n",
      "Epoch 40, Step 3 Loss = 0.4795892834663391\n",
      "Epoch 40, Step 4 Loss = 0.4817137122154236\n",
      "Epoch 40, Step 5 Loss = 0.476845383644104\n",
      "Epoch 40, Step 6 Loss = 0.4595533013343811\n",
      "Epoch 40, Step 7 Loss = 0.4793297052383423\n",
      "Epoch 40, Step 8 Loss = 0.6019800305366516\n",
      "Epoch 40, Step 9 Loss = 0.6791207790374756\n",
      "Epoch 40, Step 10 Loss = 0.5971221923828125\n",
      "Epoch 40, Step 11 Loss = 0.44741290807724\n",
      "Epoch 40, Step 12 Loss = 0.43201786279678345\n",
      "Epoch 40, Step 13 Loss = 0.2812957167625427\n",
      "Epoch 40, Step 14 Loss = 0.4433212876319885\n",
      "Epoch 40, Step 15 Loss = 0.3803374171257019\n",
      "Epoch 40, Step 16 Loss = 0.49727457761764526\n",
      "Epoch 40, Step 17 Loss = 0.4155483841896057\n",
      "Epoch 40, Step 18 Loss = 0.4221787452697754\n",
      "Epoch 40, Step 19 Loss = 0.5199390649795532\n",
      "Epoch 40, Step 20 Loss = 0.722231388092041\n",
      "Epoch 40, Step 21 Loss = 0.3899343013763428\n",
      "Epoch 40, Step 22 Loss = 1.1720483303070068\n",
      "Epoch 40, Step 23 Loss = 0.6301475763320923\n",
      "Epoch 40, Step 24 Loss = 0.6623319387435913\n",
      "Epoch 40, Step 25 Loss = 1.1456316709518433\n",
      "Epoch 40, Step 26 Loss = 0.8786013126373291\n",
      "Epoch 40, Step 27 Loss = 0.43097802996635437\n",
      "Epoch 40, Step 28 Loss = 0.42419227957725525\n",
      "Epoch 40, Step 29 Loss = 1.0484607219696045\n",
      "Epoch 40, Step 30 Loss = 0.5633666515350342\n",
      "Epoch 40, Step 31 Loss = 0.9958302974700928\n",
      "Epoch 40, Step 32 Loss = 0.4927174150943756\n",
      "Epoch 40, Step 33 Loss = 0.9085046648979187\n",
      "Epoch 40, Step 34 Loss = 0.8454066514968872\n",
      "Epoch 40, Step 35 Loss = 0.7568588852882385\n",
      "Epoch 40, Step 36 Loss = 0.6499420404434204\n",
      "Epoch 40, Step 37 Loss = 0.575707733631134\n",
      "Epoch 40, Step 38 Loss = 1.1602774858474731\n",
      "Epoch 40, Step 39 Loss = 0.9591431021690369\n",
      "Epoch 40, Step 40 Loss = 1.0347950458526611\n",
      "Epoch 40, Step 41 Loss = 0.6793794631958008\n",
      "Training loss: 0.6355\n",
      "Epoch 41/500\n",
      "Epoch 41, Step 1 Loss = 0.22950232028961182\n",
      "Epoch 41, Step 2 Loss = 0.43666189908981323\n",
      "Epoch 41, Step 3 Loss = 0.7083303928375244\n",
      "Epoch 41, Step 4 Loss = 0.5272034406661987\n",
      "Epoch 41, Step 5 Loss = 0.45918771624565125\n",
      "Epoch 41, Step 6 Loss = 0.3609752655029297\n",
      "Epoch 41, Step 7 Loss = 0.7924712300300598\n",
      "Epoch 41, Step 8 Loss = 0.6135128736495972\n",
      "Epoch 41, Step 9 Loss = 0.5213660001754761\n",
      "Epoch 41, Step 10 Loss = 0.439201295375824\n",
      "Epoch 41, Step 11 Loss = 0.39928823709487915\n",
      "Epoch 41, Step 12 Loss = 0.7266308069229126\n",
      "Epoch 41, Step 13 Loss = 0.38347360491752625\n",
      "Epoch 41, Step 14 Loss = 0.5878434181213379\n",
      "Epoch 41, Step 15 Loss = 0.307448148727417\n",
      "Epoch 41, Step 16 Loss = 0.6722208857536316\n",
      "Epoch 41, Step 17 Loss = 0.5772572755813599\n",
      "Epoch 41, Step 18 Loss = 0.5700213313102722\n",
      "Epoch 41, Step 19 Loss = 0.8682518601417542\n",
      "Epoch 41, Step 20 Loss = 0.6003831624984741\n",
      "Epoch 41, Step 21 Loss = 0.5895880460739136\n",
      "Epoch 41, Step 22 Loss = 0.7155593633651733\n",
      "Epoch 41, Step 23 Loss = 0.4996107220649719\n",
      "Epoch 41, Step 24 Loss = 0.7142548561096191\n",
      "Epoch 41, Step 25 Loss = 0.6002489328384399\n",
      "Epoch 41, Step 26 Loss = 0.8099165558815002\n",
      "Epoch 41, Step 27 Loss = 0.6763858795166016\n",
      "Epoch 41, Step 28 Loss = 0.5800314545631409\n",
      "Epoch 41, Step 29 Loss = 0.8066644072532654\n",
      "Epoch 41, Step 30 Loss = 0.24474194645881653\n",
      "Epoch 41, Step 31 Loss = 1.2761883735656738\n",
      "Epoch 41, Step 32 Loss = 0.5043524503707886\n",
      "Epoch 41, Step 33 Loss = 1.1423851251602173\n",
      "Epoch 41, Step 34 Loss = 0.5991900563240051\n",
      "Epoch 41, Step 35 Loss = 0.5045328140258789\n",
      "Epoch 41, Step 36 Loss = 0.907139241695404\n",
      "Epoch 41, Step 37 Loss = 0.5461527705192566\n",
      "Epoch 41, Step 38 Loss = 1.0052008628845215\n",
      "Epoch 41, Step 39 Loss = 0.8401210904121399\n",
      "Epoch 41, Step 40 Loss = 1.0090608596801758\n",
      "Epoch 41, Step 41 Loss = 1.123327374458313\n",
      "Training loss: 0.6458\n",
      "Epoch 42/500\n",
      "Epoch 42, Step 1 Loss = 0.4692990779876709\n",
      "Epoch 42, Step 2 Loss = 0.4216420352458954\n",
      "Epoch 42, Step 3 Loss = 0.42756402492523193\n",
      "Epoch 42, Step 4 Loss = 0.394797146320343\n",
      "Epoch 42, Step 5 Loss = 0.7409293055534363\n",
      "Epoch 42, Step 6 Loss = 0.7077348232269287\n",
      "Epoch 42, Step 7 Loss = 0.5627235770225525\n",
      "Epoch 42, Step 8 Loss = 0.4121552109718323\n",
      "Epoch 42, Step 9 Loss = 0.502711832523346\n",
      "Epoch 42, Step 10 Loss = 0.753718912601471\n",
      "Epoch 42, Step 11 Loss = 0.5579633712768555\n",
      "Epoch 42, Step 12 Loss = 0.3138846457004547\n",
      "Epoch 42, Step 13 Loss = 0.5679055452346802\n",
      "Epoch 42, Step 14 Loss = 0.4426848292350769\n",
      "Epoch 42, Step 15 Loss = 0.5672417879104614\n",
      "Epoch 42, Step 16 Loss = 0.588851273059845\n",
      "Epoch 42, Step 17 Loss = 0.6140877604484558\n",
      "Epoch 42, Step 18 Loss = 0.5963851809501648\n",
      "Epoch 42, Step 19 Loss = 0.7247075438499451\n",
      "Epoch 42, Step 20 Loss = 0.5942550301551819\n",
      "Epoch 42, Step 21 Loss = 0.5297996997833252\n",
      "Epoch 42, Step 22 Loss = 0.8162870407104492\n",
      "Epoch 42, Step 23 Loss = 0.8660671710968018\n",
      "Epoch 42, Step 24 Loss = 0.7108333110809326\n",
      "Epoch 42, Step 25 Loss = 0.6522663831710815\n",
      "Epoch 42, Step 26 Loss = 0.7595679759979248\n",
      "Epoch 42, Step 27 Loss = 0.6708455085754395\n",
      "Epoch 42, Step 28 Loss = 0.7447830438613892\n",
      "Epoch 42, Step 29 Loss = 0.880517840385437\n",
      "Epoch 42, Step 30 Loss = 0.8541027307510376\n",
      "Epoch 42, Step 31 Loss = 0.5200809240341187\n",
      "Epoch 42, Step 32 Loss = 0.379503071308136\n",
      "Epoch 42, Step 33 Loss = 0.7807009220123291\n",
      "Epoch 42, Step 34 Loss = 0.825732409954071\n",
      "Epoch 42, Step 35 Loss = 0.523918092250824\n",
      "Epoch 42, Step 36 Loss = 0.9410336017608643\n",
      "Epoch 42, Step 37 Loss = 0.6532691121101379\n",
      "Epoch 42, Step 38 Loss = 0.9856213331222534\n",
      "Epoch 42, Step 39 Loss = 0.7051429152488708\n",
      "Epoch 42, Step 40 Loss = 0.5113562345504761\n",
      "Epoch 42, Step 41 Loss = 0.7192354798316956\n",
      "Training loss: 0.6339\n",
      "Epoch 43/500\n",
      "Epoch 43, Step 1 Loss = 0.7128332257270813\n",
      "Epoch 43, Step 2 Loss = 0.7189862132072449\n",
      "Epoch 43, Step 3 Loss = 0.32751739025115967\n",
      "Epoch 43, Step 4 Loss = 0.6120400428771973\n",
      "Epoch 43, Step 5 Loss = 0.3870658278465271\n",
      "Epoch 43, Step 6 Loss = 0.4054756760597229\n",
      "Epoch 43, Step 7 Loss = 0.7277826070785522\n",
      "Epoch 43, Step 8 Loss = 0.27754703164100647\n",
      "Epoch 43, Step 9 Loss = 0.6520010828971863\n",
      "Epoch 43, Step 10 Loss = 0.7391113638877869\n",
      "Epoch 43, Step 11 Loss = 0.5451157093048096\n",
      "Epoch 43, Step 12 Loss = 0.8102494478225708\n",
      "Epoch 43, Step 13 Loss = 0.8230488300323486\n",
      "Epoch 43, Step 14 Loss = 0.6228269338607788\n",
      "Epoch 43, Step 15 Loss = 0.5636504888534546\n",
      "Epoch 43, Step 16 Loss = 0.39046692848205566\n",
      "Epoch 43, Step 17 Loss = 0.48711273074150085\n",
      "Epoch 43, Step 18 Loss = 0.6226778626441956\n",
      "Epoch 43, Step 19 Loss = 0.5861807465553284\n",
      "Epoch 43, Step 20 Loss = 0.5566133260726929\n",
      "Epoch 43, Step 21 Loss = 0.7845441102981567\n",
      "Epoch 43, Step 22 Loss = 0.43479210138320923\n",
      "Epoch 43, Step 23 Loss = 0.4580506384372711\n",
      "Epoch 43, Step 24 Loss = 0.6635949611663818\n",
      "Epoch 43, Step 25 Loss = 0.3721997141838074\n",
      "Epoch 43, Step 26 Loss = 0.619817852973938\n",
      "Epoch 43, Step 27 Loss = 0.38482537865638733\n",
      "Epoch 43, Step 28 Loss = 1.2559317350387573\n",
      "Epoch 43, Step 29 Loss = 0.8708839416503906\n",
      "Epoch 43, Step 30 Loss = 0.682622492313385\n",
      "Epoch 43, Step 31 Loss = 0.9973691701889038\n",
      "Epoch 43, Step 32 Loss = 0.512675404548645\n",
      "Epoch 43, Step 33 Loss = 0.769895076751709\n",
      "Epoch 43, Step 34 Loss = 0.2903738021850586\n",
      "Epoch 43, Step 35 Loss = 0.8327998518943787\n",
      "Epoch 43, Step 36 Loss = 0.8281400203704834\n",
      "Epoch 43, Step 37 Loss = 0.6171407699584961\n",
      "Epoch 43, Step 38 Loss = 0.853036642074585\n",
      "Epoch 43, Step 39 Loss = 0.6908969879150391\n",
      "Epoch 43, Step 40 Loss = 0.9441196918487549\n",
      "Epoch 43, Step 41 Loss = 0.40877145528793335\n",
      "Training loss: 0.6303\n",
      "Epoch 44/500\n",
      "Epoch 44, Step 1 Loss = 0.5994802713394165\n",
      "Epoch 44, Step 2 Loss = 0.3450918197631836\n",
      "Epoch 44, Step 3 Loss = 0.39280956983566284\n",
      "Epoch 44, Step 4 Loss = 0.36871904134750366\n",
      "Epoch 44, Step 5 Loss = 0.5466084480285645\n",
      "Epoch 44, Step 6 Loss = 0.6902819275856018\n",
      "Epoch 44, Step 7 Loss = 0.9737628698348999\n",
      "Epoch 44, Step 8 Loss = 0.5712083578109741\n",
      "Epoch 44, Step 9 Loss = 0.5698182582855225\n",
      "Epoch 44, Step 10 Loss = 0.42282116413116455\n",
      "Epoch 44, Step 11 Loss = 0.22281374037265778\n",
      "Epoch 44, Step 12 Loss = 0.6317083239555359\n",
      "Epoch 44, Step 13 Loss = 0.7444822788238525\n",
      "Epoch 44, Step 14 Loss = 0.5023547410964966\n",
      "Epoch 44, Step 15 Loss = 0.5039277076721191\n",
      "Epoch 44, Step 16 Loss = 0.589499831199646\n",
      "Epoch 44, Step 17 Loss = 0.741243302822113\n",
      "Epoch 44, Step 18 Loss = 0.5834903717041016\n",
      "Epoch 44, Step 19 Loss = 0.6948148012161255\n",
      "Epoch 44, Step 20 Loss = 0.45825183391571045\n",
      "Epoch 44, Step 21 Loss = 0.4854199290275574\n",
      "Epoch 44, Step 22 Loss = 0.507972240447998\n",
      "Epoch 44, Step 23 Loss = 0.8532434701919556\n",
      "Epoch 44, Step 24 Loss = 0.8112678527832031\n",
      "Epoch 44, Step 25 Loss = 0.4806516766548157\n",
      "Epoch 44, Step 26 Loss = 0.563144862651825\n",
      "Epoch 44, Step 27 Loss = 0.6520133018493652\n",
      "Epoch 44, Step 28 Loss = 0.5699268579483032\n",
      "Epoch 44, Step 29 Loss = 0.9752283692359924\n",
      "Epoch 44, Step 30 Loss = 0.8790026903152466\n",
      "Epoch 44, Step 31 Loss = 0.5647739171981812\n",
      "Epoch 44, Step 32 Loss = 0.7756377458572388\n",
      "Epoch 44, Step 33 Loss = 1.000097393989563\n",
      "Epoch 44, Step 34 Loss = 0.603736400604248\n",
      "Epoch 44, Step 35 Loss = 0.983111560344696\n",
      "Epoch 44, Step 36 Loss = 0.825468897819519\n",
      "Epoch 44, Step 37 Loss = 0.5755094289779663\n",
      "Epoch 44, Step 38 Loss = 0.8897030353546143\n",
      "Epoch 44, Step 39 Loss = 0.47390344738960266\n",
      "Epoch 44, Step 40 Loss = 0.7586989998817444\n",
      "Epoch 44, Step 41 Loss = 0.4589807987213135\n",
      "Training loss: 0.6303\n",
      "Epoch 45/500\n",
      "Epoch 45, Step 1 Loss = 0.49885234236717224\n",
      "Epoch 45, Step 2 Loss = 0.487373411655426\n",
      "Epoch 45, Step 3 Loss = 0.404452919960022\n",
      "Epoch 45, Step 4 Loss = 0.5453623533248901\n",
      "Epoch 45, Step 5 Loss = 0.6813573837280273\n",
      "Epoch 45, Step 6 Loss = 0.3694734275341034\n",
      "Epoch 45, Step 7 Loss = 0.7857059836387634\n",
      "Epoch 45, Step 8 Loss = 0.6041242480278015\n",
      "Epoch 45, Step 9 Loss = 0.5611333250999451\n",
      "Epoch 45, Step 10 Loss = 0.4500465989112854\n",
      "Epoch 45, Step 11 Loss = 0.4489021301269531\n",
      "Epoch 45, Step 12 Loss = 0.4968659281730652\n",
      "Epoch 45, Step 13 Loss = 0.48096945881843567\n",
      "Epoch 45, Step 14 Loss = 0.6629167795181274\n",
      "Epoch 45, Step 15 Loss = 0.6299929022789001\n",
      "Epoch 45, Step 16 Loss = 0.8747289180755615\n",
      "Epoch 45, Step 17 Loss = 0.6990909576416016\n",
      "Epoch 45, Step 18 Loss = 0.5728403925895691\n",
      "Epoch 45, Step 19 Loss = 0.5397165417671204\n",
      "Epoch 45, Step 20 Loss = 0.7686514854431152\n",
      "Epoch 45, Step 21 Loss = 0.34914207458496094\n",
      "Epoch 45, Step 22 Loss = 0.7116574048995972\n",
      "Epoch 45, Step 23 Loss = 0.5323531627655029\n",
      "Epoch 45, Step 24 Loss = 0.906323254108429\n",
      "Epoch 45, Step 25 Loss = 0.669548511505127\n",
      "Epoch 45, Step 26 Loss = 0.6096439957618713\n",
      "Epoch 45, Step 27 Loss = 0.5813115239143372\n",
      "Epoch 45, Step 28 Loss = 0.5184954404830933\n",
      "Epoch 45, Step 29 Loss = 0.8802844285964966\n",
      "Epoch 45, Step 30 Loss = 0.5644803643226624\n",
      "Epoch 45, Step 31 Loss = 0.7699720859527588\n",
      "Epoch 45, Step 32 Loss = 1.0243003368377686\n",
      "Epoch 45, Step 33 Loss = 0.9501821994781494\n",
      "Epoch 45, Step 34 Loss = 0.6675915718078613\n",
      "Epoch 45, Step 35 Loss = 0.4483490288257599\n",
      "Epoch 45, Step 36 Loss = 0.4399702548980713\n",
      "Epoch 45, Step 37 Loss = 0.782779335975647\n",
      "Epoch 45, Step 38 Loss = 0.8793327808380127\n",
      "Epoch 45, Step 39 Loss = 0.9480686783790588\n",
      "Epoch 45, Step 40 Loss = 0.5037036538124084\n",
      "Epoch 45, Step 41 Loss = 0.36981362104415894\n",
      "Training loss: 0.6261\n",
      "Epoch 46/500\n",
      "Epoch 46, Step 1 Loss = 0.5518301725387573\n",
      "Epoch 46, Step 2 Loss = 0.6067672967910767\n",
      "Epoch 46, Step 3 Loss = 0.4779587388038635\n",
      "Epoch 46, Step 4 Loss = 0.3193126320838928\n",
      "Epoch 46, Step 5 Loss = 0.6764540076255798\n",
      "Epoch 46, Step 6 Loss = 0.827399730682373\n",
      "Epoch 46, Step 7 Loss = 0.3454442620277405\n",
      "Epoch 46, Step 8 Loss = 0.8370552659034729\n",
      "Epoch 46, Step 9 Loss = 0.5972691178321838\n",
      "Epoch 46, Step 10 Loss = 0.30212754011154175\n",
      "Epoch 46, Step 11 Loss = 0.45842933654785156\n",
      "Epoch 46, Step 12 Loss = 0.5366106033325195\n",
      "Epoch 46, Step 13 Loss = 0.3942156434059143\n",
      "Epoch 46, Step 14 Loss = 0.43881091475486755\n",
      "Epoch 46, Step 15 Loss = 0.3988495171070099\n",
      "Epoch 46, Step 16 Loss = 0.48004627227783203\n",
      "Epoch 46, Step 17 Loss = 0.7668982744216919\n",
      "Epoch 46, Step 18 Loss = 0.40092259645462036\n",
      "Epoch 46, Step 19 Loss = 0.6939384937286377\n",
      "Epoch 46, Step 20 Loss = 0.4189137816429138\n",
      "Epoch 46, Step 21 Loss = 0.649751603603363\n",
      "Epoch 46, Step 22 Loss = 0.671814501285553\n",
      "Epoch 46, Step 23 Loss = 0.7874541878700256\n",
      "Epoch 46, Step 24 Loss = 0.4679613709449768\n",
      "Epoch 46, Step 25 Loss = 0.46416544914245605\n",
      "Epoch 46, Step 26 Loss = 0.48647695779800415\n",
      "Epoch 46, Step 27 Loss = 0.7106512784957886\n",
      "Epoch 46, Step 28 Loss = 0.6173381805419922\n",
      "Epoch 46, Step 29 Loss = 0.5997745990753174\n",
      "Epoch 46, Step 30 Loss = 0.976978600025177\n",
      "Epoch 46, Step 31 Loss = 0.7770659327507019\n",
      "Epoch 46, Step 32 Loss = 0.6018479466438293\n",
      "Epoch 46, Step 33 Loss = 1.107591986656189\n",
      "Epoch 46, Step 34 Loss = 0.4887583553791046\n",
      "Epoch 46, Step 35 Loss = 0.8792126774787903\n",
      "Epoch 46, Step 36 Loss = 0.6688176393508911\n",
      "Epoch 46, Step 37 Loss = 1.335057020187378\n",
      "Epoch 46, Step 38 Loss = 0.8113284707069397\n",
      "Epoch 46, Step 39 Loss = 0.420773983001709\n",
      "Epoch 46, Step 40 Loss = 0.7696294784545898\n",
      "Epoch 46, Step 41 Loss = 0.7432820200920105\n",
      "Training loss: 0.6235\n",
      "Epoch 47/500\n",
      "Epoch 47, Step 1 Loss = 0.30610302090644836\n",
      "Epoch 47, Step 2 Loss = 0.565414547920227\n",
      "Epoch 47, Step 3 Loss = 0.4762181043624878\n",
      "Epoch 47, Step 4 Loss = 0.4375787675380707\n",
      "Epoch 47, Step 5 Loss = 0.4981762766838074\n",
      "Epoch 47, Step 6 Loss = 0.369948148727417\n",
      "Epoch 47, Step 7 Loss = 0.4604010581970215\n",
      "Epoch 47, Step 8 Loss = 0.4727357029914856\n",
      "Epoch 47, Step 9 Loss = 0.524541437625885\n",
      "Epoch 47, Step 10 Loss = 0.13117502629756927\n",
      "Epoch 47, Step 11 Loss = 0.656420886516571\n",
      "Epoch 47, Step 12 Loss = 0.8484150171279907\n",
      "Epoch 47, Step 13 Loss = 0.6583048701286316\n",
      "Epoch 47, Step 14 Loss = 0.7256857752799988\n",
      "Epoch 47, Step 15 Loss = 0.4762080907821655\n",
      "Epoch 47, Step 16 Loss = 0.6088005304336548\n",
      "Epoch 47, Step 17 Loss = 0.5619858503341675\n",
      "Epoch 47, Step 18 Loss = 0.4555710554122925\n",
      "Epoch 47, Step 19 Loss = 0.32859230041503906\n",
      "Epoch 47, Step 20 Loss = 0.5279896259307861\n",
      "Epoch 47, Step 21 Loss = 0.5936499238014221\n",
      "Epoch 47, Step 22 Loss = 0.7576295137405396\n",
      "Epoch 47, Step 23 Loss = 1.2829822301864624\n",
      "Epoch 47, Step 24 Loss = 0.4655771255493164\n",
      "Epoch 47, Step 25 Loss = 0.7035129070281982\n",
      "Epoch 47, Step 26 Loss = 0.6843206882476807\n",
      "Epoch 47, Step 27 Loss = 0.6388781070709229\n",
      "Epoch 47, Step 28 Loss = 1.1429460048675537\n",
      "Epoch 47, Step 29 Loss = 0.6224671602249146\n",
      "Epoch 47, Step 30 Loss = 0.4969445466995239\n",
      "Epoch 47, Step 31 Loss = 0.7240244746208191\n",
      "Epoch 47, Step 32 Loss = 0.5896412134170532\n",
      "Epoch 47, Step 33 Loss = 0.7541939616203308\n",
      "Epoch 47, Step 34 Loss = 0.6640232801437378\n",
      "Epoch 47, Step 35 Loss = 0.5368949174880981\n",
      "Epoch 47, Step 36 Loss = 0.6546021699905396\n",
      "Epoch 47, Step 37 Loss = 0.7416875958442688\n",
      "Epoch 47, Step 38 Loss = 0.9124448299407959\n",
      "Epoch 47, Step 39 Loss = 0.8386473655700684\n",
      "Epoch 47, Step 40 Loss = 1.3707666397094727\n",
      "Epoch 47, Step 41 Loss = 0.9299976825714111\n",
      "Training loss: 0.6389\n",
      "Epoch 48/500\n",
      "Epoch 48, Step 1 Loss = 0.46128860116004944\n",
      "Epoch 48, Step 2 Loss = 0.5082721710205078\n",
      "Epoch 48, Step 3 Loss = 0.6174001693725586\n",
      "Epoch 48, Step 4 Loss = 0.517852783203125\n",
      "Epoch 48, Step 5 Loss = 0.21190613508224487\n",
      "Epoch 48, Step 6 Loss = 0.4131154716014862\n",
      "Epoch 48, Step 7 Loss = 0.6751335859298706\n",
      "Epoch 48, Step 8 Loss = 0.49401313066482544\n",
      "Epoch 48, Step 9 Loss = 0.501259446144104\n",
      "Epoch 48, Step 10 Loss = 0.7105827331542969\n",
      "Epoch 48, Step 11 Loss = 0.5165838003158569\n",
      "Epoch 48, Step 12 Loss = 0.6257485151290894\n",
      "Epoch 48, Step 13 Loss = 0.744896650314331\n",
      "Epoch 48, Step 14 Loss = 0.7210090160369873\n",
      "Epoch 48, Step 15 Loss = 0.48974907398223877\n",
      "Epoch 48, Step 16 Loss = 0.7808523774147034\n",
      "Epoch 48, Step 17 Loss = 0.8205665946006775\n",
      "Epoch 48, Step 18 Loss = 0.4023737907409668\n",
      "Epoch 48, Step 19 Loss = 0.5582268834114075\n",
      "Epoch 48, Step 20 Loss = 0.6947023868560791\n",
      "Epoch 48, Step 21 Loss = 0.43786782026290894\n",
      "Epoch 48, Step 22 Loss = 0.4910742938518524\n",
      "Epoch 48, Step 23 Loss = 1.1618890762329102\n",
      "Epoch 48, Step 24 Loss = 0.5063369274139404\n",
      "Epoch 48, Step 25 Loss = 0.40421658754348755\n",
      "Epoch 48, Step 26 Loss = 0.421580970287323\n",
      "Epoch 48, Step 27 Loss = 0.4420757591724396\n",
      "Epoch 48, Step 28 Loss = 0.4589250087738037\n",
      "Epoch 48, Step 29 Loss = 0.6025246381759644\n",
      "Epoch 48, Step 30 Loss = 0.7534035444259644\n",
      "Epoch 48, Step 31 Loss = 0.9997838735580444\n",
      "Epoch 48, Step 32 Loss = 0.9321895241737366\n",
      "Epoch 48, Step 33 Loss = 0.9398077726364136\n",
      "Epoch 48, Step 34 Loss = 0.5277588367462158\n",
      "Epoch 48, Step 35 Loss = 0.7583593130111694\n",
      "Epoch 48, Step 36 Loss = 1.0201637744903564\n",
      "Epoch 48, Step 37 Loss = 0.7096935510635376\n",
      "Epoch 48, Step 38 Loss = 0.4690869152545929\n",
      "Epoch 48, Step 39 Loss = 0.9050660729408264\n",
      "Epoch 48, Step 40 Loss = 0.7982445955276489\n",
      "Epoch 48, Step 41 Loss = 0.571289598941803\n",
      "Training loss: 0.6287\n",
      "Epoch 49/500\n",
      "Epoch 49, Step 1 Loss = 0.5503610372543335\n",
      "Epoch 49, Step 2 Loss = 0.686636209487915\n",
      "Epoch 49, Step 3 Loss = 0.571523904800415\n",
      "Epoch 49, Step 4 Loss = 0.333832710981369\n",
      "Epoch 49, Step 5 Loss = 0.24931003153324127\n",
      "Epoch 49, Step 6 Loss = 0.529830813407898\n",
      "Epoch 49, Step 7 Loss = 0.433561772108078\n",
      "Epoch 49, Step 8 Loss = 0.5802135467529297\n",
      "Epoch 49, Step 9 Loss = 0.41109296679496765\n",
      "Epoch 49, Step 10 Loss = 0.5872512459754944\n",
      "Epoch 49, Step 11 Loss = 0.39178118109703064\n",
      "Epoch 49, Step 12 Loss = 0.3711079955101013\n",
      "Epoch 49, Step 13 Loss = 0.4084409475326538\n",
      "Epoch 49, Step 14 Loss = 0.5773874521255493\n",
      "Epoch 49, Step 15 Loss = 0.8707824945449829\n",
      "Epoch 49, Step 16 Loss = 0.45099687576293945\n",
      "Epoch 49, Step 17 Loss = 0.5453952550888062\n",
      "Epoch 49, Step 18 Loss = 0.5421862602233887\n",
      "Epoch 49, Step 19 Loss = 0.7350437641143799\n",
      "Epoch 49, Step 20 Loss = 0.45314860343933105\n",
      "Epoch 49, Step 21 Loss = 1.0281682014465332\n",
      "Epoch 49, Step 22 Loss = 0.6734223365783691\n",
      "Epoch 49, Step 23 Loss = 0.7987046241760254\n",
      "Epoch 49, Step 24 Loss = 0.2540919780731201\n",
      "Epoch 49, Step 25 Loss = 1.2806025743484497\n",
      "Epoch 49, Step 26 Loss = 0.3109644651412964\n",
      "Epoch 49, Step 27 Loss = 0.6121463775634766\n",
      "Epoch 49, Step 28 Loss = 0.7321581840515137\n",
      "Epoch 49, Step 29 Loss = 0.6412631273269653\n",
      "Epoch 49, Step 30 Loss = 0.7697492241859436\n",
      "Epoch 49, Step 31 Loss = 0.7235763072967529\n",
      "Epoch 49, Step 32 Loss = 0.86831134557724\n",
      "Epoch 49, Step 33 Loss = 0.5134778022766113\n",
      "Epoch 49, Step 34 Loss = 0.9961079359054565\n",
      "Epoch 49, Step 35 Loss = 0.45750272274017334\n",
      "Epoch 49, Step 36 Loss = 0.9745531678199768\n",
      "Epoch 49, Step 37 Loss = 1.0916764736175537\n",
      "Epoch 49, Step 38 Loss = 0.9246543645858765\n",
      "Epoch 49, Step 39 Loss = 0.6052239537239075\n",
      "Epoch 49, Step 40 Loss = 0.835671067237854\n",
      "Epoch 49, Step 41 Loss = 0.6041162014007568\n",
      "Training loss: 0.6336\n",
      "Epoch 50/500\n",
      "Epoch 50, Step 1 Loss = 0.35794565081596375\n",
      "Epoch 50, Step 2 Loss = 0.5688329339027405\n",
      "Epoch 50, Step 3 Loss = 0.7815648317337036\n",
      "Epoch 50, Step 4 Loss = 0.6114612221717834\n",
      "Epoch 50, Step 5 Loss = 0.3629421889781952\n",
      "Epoch 50, Step 6 Loss = 0.8781418800354004\n",
      "Epoch 50, Step 7 Loss = 0.48423531651496887\n",
      "Epoch 50, Step 8 Loss = 0.45173534750938416\n",
      "Epoch 50, Step 9 Loss = 0.2995927929878235\n",
      "Epoch 50, Step 10 Loss = 0.5176568031311035\n",
      "Epoch 50, Step 11 Loss = 0.4888378083705902\n",
      "Epoch 50, Step 12 Loss = 0.29948925971984863\n",
      "Epoch 50, Step 13 Loss = 0.8827534914016724\n",
      "Epoch 50, Step 14 Loss = 0.6103585958480835\n",
      "Epoch 50, Step 15 Loss = 0.49947917461395264\n",
      "Epoch 50, Step 16 Loss = 0.40909552574157715\n",
      "Epoch 50, Step 17 Loss = 0.6643947958946228\n",
      "Epoch 50, Step 18 Loss = 0.4573636054992676\n",
      "Epoch 50, Step 19 Loss = 0.6543221473693848\n",
      "Epoch 50, Step 20 Loss = 0.7838377952575684\n",
      "Epoch 50, Step 21 Loss = 0.5866353511810303\n",
      "Epoch 50, Step 22 Loss = 0.8367622494697571\n",
      "Epoch 50, Step 23 Loss = 0.5320179462432861\n",
      "Epoch 50, Step 24 Loss = 0.52024245262146\n",
      "Epoch 50, Step 25 Loss = 0.7978926301002502\n",
      "Epoch 50, Step 26 Loss = 0.41033506393432617\n",
      "Epoch 50, Step 27 Loss = 0.9386986494064331\n",
      "Epoch 50, Step 28 Loss = 0.28215518593788147\n",
      "Epoch 50, Step 29 Loss = 0.9965901970863342\n",
      "Epoch 50, Step 30 Loss = 0.8559321761131287\n",
      "Epoch 50, Step 31 Loss = 0.46875444054603577\n",
      "Epoch 50, Step 32 Loss = 0.593144953250885\n",
      "Epoch 50, Step 33 Loss = 0.6592077016830444\n",
      "Epoch 50, Step 34 Loss = 0.7029074430465698\n",
      "Epoch 50, Step 35 Loss = 0.4510170817375183\n",
      "Epoch 50, Step 36 Loss = 0.8184731006622314\n",
      "Epoch 50, Step 37 Loss = 0.835289716720581\n",
      "Epoch 50, Step 38 Loss = 0.8436283469200134\n",
      "Epoch 50, Step 39 Loss = 0.6120495796203613\n",
      "Epoch 50, Step 40 Loss = 0.6171951293945312\n",
      "Epoch 50, Step 41 Loss = 0.7768023610115051\n",
      "Training loss: 0.6146\n",
      "Epoch 51/500\n",
      "Epoch 51, Step 1 Loss = 0.5006428360939026\n",
      "Epoch 51, Step 2 Loss = 0.6554507613182068\n",
      "Epoch 51, Step 3 Loss = 0.4589926600456238\n",
      "Epoch 51, Step 4 Loss = 0.5228656530380249\n",
      "Epoch 51, Step 5 Loss = 0.5558090806007385\n",
      "Epoch 51, Step 6 Loss = 0.3934222459793091\n",
      "Epoch 51, Step 7 Loss = 0.18868054449558258\n",
      "Epoch 51, Step 8 Loss = 0.6058251857757568\n",
      "Epoch 51, Step 9 Loss = 0.5554326176643372\n",
      "Epoch 51, Step 10 Loss = 0.6012407541275024\n",
      "Epoch 51, Step 11 Loss = 0.42244797945022583\n",
      "Epoch 51, Step 12 Loss = 0.8201800584793091\n",
      "Epoch 51, Step 13 Loss = 0.3008653521537781\n",
      "Epoch 51, Step 14 Loss = 0.5830956697463989\n",
      "Epoch 51, Step 15 Loss = 0.3557056784629822\n",
      "Epoch 51, Step 16 Loss = 0.6542189121246338\n",
      "Epoch 51, Step 17 Loss = 0.4161009192466736\n",
      "Epoch 51, Step 18 Loss = 0.36617976427078247\n",
      "Epoch 51, Step 19 Loss = 0.5671839714050293\n",
      "Epoch 51, Step 20 Loss = 0.2578344941139221\n",
      "Epoch 51, Step 21 Loss = 0.5658763647079468\n",
      "Epoch 51, Step 22 Loss = 0.4175119996070862\n",
      "Epoch 51, Step 23 Loss = 0.4246482849121094\n",
      "Epoch 51, Step 24 Loss = 0.7852655649185181\n",
      "Epoch 51, Step 25 Loss = 0.6319364309310913\n",
      "Epoch 51, Step 26 Loss = 1.0668344497680664\n",
      "Epoch 51, Step 27 Loss = 0.6675788164138794\n",
      "Epoch 51, Step 28 Loss = 0.7574072480201721\n",
      "Epoch 51, Step 29 Loss = 0.8026866316795349\n",
      "Epoch 51, Step 30 Loss = 0.9247875809669495\n",
      "Epoch 51, Step 31 Loss = 0.8363902568817139\n",
      "Epoch 51, Step 32 Loss = 0.8998039960861206\n",
      "Epoch 51, Step 33 Loss = 0.6815796494483948\n",
      "Epoch 51, Step 34 Loss = 0.8445558547973633\n",
      "Epoch 51, Step 35 Loss = 0.5728522539138794\n",
      "Epoch 51, Step 36 Loss = 1.0166645050048828\n",
      "Epoch 51, Step 37 Loss = 1.4751615524291992\n",
      "Epoch 51, Step 38 Loss = 0.613620936870575\n",
      "Epoch 51, Step 39 Loss = 0.5412440299987793\n",
      "Epoch 51, Step 40 Loss = 0.7868027687072754\n",
      "Epoch 51, Step 41 Loss = 0.8444808125495911\n",
      "Training loss: 0.6327\n",
      "Epoch 52/500\n",
      "Epoch 52, Step 1 Loss = 0.5920802354812622\n",
      "Epoch 52, Step 2 Loss = 0.443486750125885\n",
      "Epoch 52, Step 3 Loss = 0.7582504153251648\n",
      "Epoch 52, Step 4 Loss = 0.4216578006744385\n",
      "Epoch 52, Step 5 Loss = 0.3224870264530182\n",
      "Epoch 52, Step 6 Loss = 0.4992607831954956\n",
      "Epoch 52, Step 7 Loss = 0.6099639534950256\n",
      "Epoch 52, Step 8 Loss = 0.1409134715795517\n",
      "Epoch 52, Step 9 Loss = 0.5156340599060059\n",
      "Epoch 52, Step 10 Loss = 0.5177874565124512\n",
      "Epoch 52, Step 11 Loss = 0.3329758048057556\n",
      "Epoch 52, Step 12 Loss = 0.43117570877075195\n",
      "Epoch 52, Step 13 Loss = 0.7276045680046082\n",
      "Epoch 52, Step 14 Loss = 0.6031553745269775\n",
      "Epoch 52, Step 15 Loss = 0.4632675349712372\n",
      "Epoch 52, Step 16 Loss = 0.5830599069595337\n",
      "Epoch 52, Step 17 Loss = 0.7624005675315857\n",
      "Epoch 52, Step 18 Loss = 0.8167459964752197\n",
      "Epoch 52, Step 19 Loss = 0.3177303075790405\n",
      "Epoch 52, Step 20 Loss = 0.9337958693504333\n",
      "Epoch 52, Step 21 Loss = 0.43851691484451294\n",
      "Epoch 52, Step 22 Loss = 0.5441015958786011\n",
      "Epoch 52, Step 23 Loss = 0.8106939792633057\n",
      "Epoch 52, Step 24 Loss = 1.0355907678604126\n",
      "Epoch 52, Step 25 Loss = 0.8294899463653564\n",
      "Epoch 52, Step 26 Loss = 0.9419768452644348\n",
      "Epoch 52, Step 27 Loss = 0.5672082901000977\n",
      "Epoch 52, Step 28 Loss = 0.5958658456802368\n",
      "Epoch 52, Step 29 Loss = 0.9674052000045776\n",
      "Epoch 52, Step 30 Loss = 0.7902359962463379\n",
      "Epoch 52, Step 31 Loss = 0.550957977771759\n",
      "Epoch 52, Step 32 Loss = 0.2910199761390686\n",
      "Epoch 52, Step 33 Loss = 1.0830655097961426\n",
      "Epoch 52, Step 34 Loss = 0.5695567727088928\n",
      "Epoch 52, Step 35 Loss = 0.4388175308704376\n",
      "Epoch 52, Step 36 Loss = 0.4665626287460327\n",
      "Epoch 52, Step 37 Loss = 0.25770777463912964\n",
      "Epoch 52, Step 38 Loss = 0.7147344350814819\n",
      "Epoch 52, Step 39 Loss = 0.5904104709625244\n",
      "Epoch 52, Step 40 Loss = 1.0859010219573975\n",
      "Epoch 52, Step 41 Loss = 1.0314648151397705\n",
      "Training loss: 0.6194\n",
      "Epoch 53/500\n",
      "Epoch 53, Step 1 Loss = 0.5553655624389648\n",
      "Epoch 53, Step 2 Loss = 0.9558722972869873\n",
      "Epoch 53, Step 3 Loss = 0.43491417169570923\n",
      "Epoch 53, Step 4 Loss = 0.458312451839447\n",
      "Epoch 53, Step 5 Loss = 0.6271547079086304\n",
      "Epoch 53, Step 6 Loss = 0.6579928994178772\n",
      "Epoch 53, Step 7 Loss = 0.4408084452152252\n",
      "Epoch 53, Step 8 Loss = 0.5681040287017822\n",
      "Epoch 53, Step 9 Loss = 0.47172194719314575\n",
      "Epoch 53, Step 10 Loss = 0.5465768575668335\n",
      "Epoch 53, Step 11 Loss = 0.7194702625274658\n",
      "Epoch 53, Step 12 Loss = 0.4299958348274231\n",
      "Epoch 53, Step 13 Loss = 0.47421401739120483\n",
      "Epoch 53, Step 14 Loss = 0.5426892042160034\n",
      "Epoch 53, Step 15 Loss = 0.5683292150497437\n",
      "Epoch 53, Step 16 Loss = 0.9149906039237976\n",
      "Epoch 53, Step 17 Loss = 0.6004968881607056\n",
      "Epoch 53, Step 18 Loss = 0.6736235618591309\n",
      "Epoch 53, Step 19 Loss = 0.4574624300003052\n",
      "Epoch 53, Step 20 Loss = 0.21228620409965515\n",
      "Epoch 53, Step 21 Loss = 0.6880173087120056\n",
      "Epoch 53, Step 22 Loss = 0.9060198068618774\n",
      "Epoch 53, Step 23 Loss = 0.37195074558258057\n",
      "Epoch 53, Step 24 Loss = 0.6630017757415771\n",
      "Epoch 53, Step 25 Loss = 0.9162172079086304\n",
      "Epoch 53, Step 26 Loss = 0.7524199485778809\n",
      "Epoch 53, Step 27 Loss = 0.34645187854766846\n",
      "Epoch 53, Step 28 Loss = 0.5709466934204102\n",
      "Epoch 53, Step 29 Loss = 0.5527002215385437\n",
      "Epoch 53, Step 30 Loss = 1.088045358657837\n",
      "Epoch 53, Step 31 Loss = 0.3377612829208374\n",
      "Epoch 53, Step 32 Loss = 0.4939895272254944\n",
      "Epoch 53, Step 33 Loss = 0.6543220281600952\n",
      "Epoch 53, Step 34 Loss = 0.29544323682785034\n",
      "Epoch 53, Step 35 Loss = 0.6704341769218445\n",
      "Epoch 53, Step 36 Loss = 0.6885281801223755\n",
      "Epoch 53, Step 37 Loss = 0.8559174537658691\n",
      "Epoch 53, Step 38 Loss = 0.613114595413208\n",
      "Epoch 53, Step 39 Loss = 1.0282514095306396\n",
      "Epoch 53, Step 40 Loss = 0.7601483464241028\n",
      "Epoch 53, Step 41 Loss = 0.5351091623306274\n",
      "Training loss: 0.6122\n",
      "Epoch 54/500\n",
      "Epoch 54, Step 1 Loss = 0.5852459073066711\n",
      "Epoch 54, Step 2 Loss = 0.3867288827896118\n",
      "Epoch 54, Step 3 Loss = 0.5582900047302246\n",
      "Epoch 54, Step 4 Loss = 0.4856698513031006\n",
      "Epoch 54, Step 5 Loss = 0.6813977360725403\n",
      "Epoch 54, Step 6 Loss = 0.6062922477722168\n",
      "Epoch 54, Step 7 Loss = 0.4213489294052124\n",
      "Epoch 54, Step 8 Loss = 0.4859132468700409\n",
      "Epoch 54, Step 9 Loss = 0.983877420425415\n",
      "Epoch 54, Step 10 Loss = 0.6375895142555237\n",
      "Epoch 54, Step 11 Loss = 0.6200388073921204\n",
      "Epoch 54, Step 12 Loss = 0.6230452060699463\n",
      "Epoch 54, Step 13 Loss = 0.6779130101203918\n",
      "Epoch 54, Step 14 Loss = 0.5416451692581177\n",
      "Epoch 54, Step 15 Loss = 0.5311552286148071\n",
      "Epoch 54, Step 16 Loss = 0.4182717204093933\n",
      "Epoch 54, Step 17 Loss = 0.8077816963195801\n",
      "Epoch 54, Step 18 Loss = 0.45593011379241943\n",
      "Epoch 54, Step 19 Loss = 0.6277561187744141\n",
      "Epoch 54, Step 20 Loss = 0.6480101943016052\n",
      "Epoch 54, Step 21 Loss = 0.5577749013900757\n",
      "Epoch 54, Step 22 Loss = 0.7454504370689392\n",
      "Epoch 54, Step 23 Loss = 0.7514904141426086\n",
      "Epoch 54, Step 24 Loss = 0.2713159918785095\n",
      "Epoch 54, Step 25 Loss = 0.9162743091583252\n",
      "Epoch 54, Step 26 Loss = 0.8351074457168579\n",
      "Epoch 54, Step 27 Loss = 1.2213234901428223\n",
      "Epoch 54, Step 28 Loss = 0.560348629951477\n",
      "Epoch 54, Step 29 Loss = 0.8631397485733032\n",
      "Epoch 54, Step 30 Loss = 0.4436190724372864\n",
      "Epoch 54, Step 31 Loss = 0.34730589389801025\n",
      "Epoch 54, Step 32 Loss = 0.6911026239395142\n",
      "Epoch 54, Step 33 Loss = 0.615966796875\n",
      "Epoch 54, Step 34 Loss = 0.23592188954353333\n",
      "Epoch 54, Step 35 Loss = 0.9101424217224121\n",
      "Epoch 54, Step 36 Loss = 1.232848048210144\n",
      "Epoch 54, Step 37 Loss = 0.8268245458602905\n",
      "Epoch 54, Step 38 Loss = 0.7818053960800171\n",
      "Epoch 54, Step 39 Loss = 0.4292025566101074\n",
      "Epoch 54, Step 40 Loss = 0.5764844417572021\n",
      "Epoch 54, Step 41 Loss = 0.3796743154525757\n",
      "Training loss: 0.6336\n",
      "Epoch 55/500\n",
      "Epoch 55, Step 1 Loss = 0.405018150806427\n",
      "Epoch 55, Step 2 Loss = 0.5318611860275269\n",
      "Epoch 55, Step 3 Loss = 0.7575309872627258\n",
      "Epoch 55, Step 4 Loss = 0.41973012685775757\n",
      "Epoch 55, Step 5 Loss = 0.4861948490142822\n",
      "Epoch 55, Step 6 Loss = 0.5203325748443604\n",
      "Epoch 55, Step 7 Loss = 0.5721265077590942\n",
      "Epoch 55, Step 8 Loss = 0.30747324228286743\n",
      "Epoch 55, Step 9 Loss = 0.46693241596221924\n",
      "Epoch 55, Step 10 Loss = 0.5871686339378357\n",
      "Epoch 55, Step 11 Loss = 0.4186217188835144\n",
      "Epoch 55, Step 12 Loss = 0.43859589099884033\n",
      "Epoch 55, Step 13 Loss = 0.48081016540527344\n",
      "Epoch 55, Step 14 Loss = 0.8424140810966492\n",
      "Epoch 55, Step 15 Loss = 0.49135643243789673\n",
      "Epoch 55, Step 16 Loss = 1.0422899723052979\n",
      "Epoch 55, Step 17 Loss = 0.5752763748168945\n",
      "Epoch 55, Step 18 Loss = 0.7193397283554077\n",
      "Epoch 55, Step 19 Loss = 0.8801475763320923\n",
      "Epoch 55, Step 20 Loss = 0.4176524579524994\n",
      "Epoch 55, Step 21 Loss = 0.808124303817749\n",
      "Epoch 55, Step 22 Loss = 0.323233962059021\n",
      "Epoch 55, Step 23 Loss = 0.8648142218589783\n",
      "Epoch 55, Step 24 Loss = 0.6576725244522095\n",
      "Epoch 55, Step 25 Loss = 0.7118735313415527\n",
      "Epoch 55, Step 26 Loss = 0.3981236219406128\n",
      "Epoch 55, Step 27 Loss = 0.608452558517456\n",
      "Epoch 55, Step 28 Loss = 0.936495304107666\n",
      "Epoch 55, Step 29 Loss = 0.7087202668190002\n",
      "Epoch 55, Step 30 Loss = 0.5390051603317261\n",
      "Epoch 55, Step 31 Loss = 0.4903467297554016\n",
      "Epoch 55, Step 32 Loss = 0.4659861624240875\n",
      "Epoch 55, Step 33 Loss = 0.6247003674507141\n",
      "Epoch 55, Step 34 Loss = 0.6357927322387695\n",
      "Epoch 55, Step 35 Loss = 0.9002695083618164\n",
      "Epoch 55, Step 36 Loss = 0.5080360174179077\n",
      "Epoch 55, Step 37 Loss = 0.7888785600662231\n",
      "Epoch 55, Step 38 Loss = 0.8725334405899048\n",
      "Epoch 55, Step 39 Loss = 0.6138612031936646\n",
      "Epoch 55, Step 40 Loss = 0.9865669012069702\n",
      "Epoch 55, Step 41 Loss = 0.6082402467727661\n",
      "Training loss: 0.6198\n",
      "Epoch 56/500\n",
      "Epoch 56, Step 1 Loss = 0.7567406296730042\n",
      "Epoch 56, Step 2 Loss = 0.40251556038856506\n",
      "Epoch 56, Step 3 Loss = 0.5999127626419067\n",
      "Epoch 56, Step 4 Loss = 0.2368590533733368\n",
      "Epoch 56, Step 5 Loss = 0.3818637728691101\n",
      "Epoch 56, Step 6 Loss = 0.3831547498703003\n",
      "Epoch 56, Step 7 Loss = 0.40361765027046204\n",
      "Epoch 56, Step 8 Loss = 0.5699309706687927\n",
      "Epoch 56, Step 9 Loss = 0.7173067331314087\n",
      "Epoch 56, Step 10 Loss = 0.5204139351844788\n",
      "Epoch 56, Step 11 Loss = 0.4583123028278351\n",
      "Epoch 56, Step 12 Loss = 0.6044723987579346\n",
      "Epoch 56, Step 13 Loss = 0.4662458896636963\n",
      "Epoch 56, Step 14 Loss = 0.6837413907051086\n",
      "Epoch 56, Step 15 Loss = 0.7198043465614319\n",
      "Epoch 56, Step 16 Loss = 0.36402738094329834\n",
      "Epoch 56, Step 17 Loss = 0.5238755941390991\n",
      "Epoch 56, Step 18 Loss = 0.44949597120285034\n",
      "Epoch 56, Step 19 Loss = 0.4399116039276123\n",
      "Epoch 56, Step 20 Loss = 0.4844403564929962\n",
      "Epoch 56, Step 21 Loss = 0.9391102194786072\n",
      "Epoch 56, Step 22 Loss = 0.7137097120285034\n",
      "Epoch 56, Step 23 Loss = 0.3841293752193451\n",
      "Epoch 56, Step 24 Loss = 0.47857075929641724\n",
      "Epoch 56, Step 25 Loss = 0.819354772567749\n",
      "Epoch 56, Step 26 Loss = 0.5367710590362549\n",
      "Epoch 56, Step 27 Loss = 0.8677976727485657\n",
      "Epoch 56, Step 28 Loss = 0.9886486530303955\n",
      "Epoch 56, Step 29 Loss = 1.0343670845031738\n",
      "Epoch 56, Step 30 Loss = 1.012571930885315\n",
      "Epoch 56, Step 31 Loss = 0.4382358193397522\n",
      "Epoch 56, Step 32 Loss = 0.6744076013565063\n",
      "Epoch 56, Step 33 Loss = 0.8993677496910095\n",
      "Epoch 56, Step 34 Loss = 0.6310107707977295\n",
      "Epoch 56, Step 35 Loss = 0.7196884155273438\n",
      "Epoch 56, Step 36 Loss = 1.0390640497207642\n",
      "Epoch 56, Step 37 Loss = 0.6268965005874634\n",
      "Epoch 56, Step 38 Loss = 0.5204077959060669\n",
      "Epoch 56, Step 39 Loss = 0.745537281036377\n",
      "Epoch 56, Step 40 Loss = 0.7604206800460815\n",
      "Epoch 56, Step 41 Loss = 0.5726896524429321\n",
      "Training loss: 0.6236\n",
      "Epoch 57/500\n",
      "Epoch 57, Step 1 Loss = 0.4942275881767273\n",
      "Epoch 57, Step 2 Loss = 0.5055535435676575\n",
      "Epoch 57, Step 3 Loss = 0.5504837036132812\n",
      "Epoch 57, Step 4 Loss = 0.5288512110710144\n",
      "Epoch 57, Step 5 Loss = 0.4811103343963623\n",
      "Epoch 57, Step 6 Loss = 0.6625387668609619\n",
      "Epoch 57, Step 7 Loss = 0.6490679979324341\n",
      "Epoch 57, Step 8 Loss = 0.5563890933990479\n",
      "Epoch 57, Step 9 Loss = 0.4677736759185791\n",
      "Epoch 57, Step 10 Loss = 0.4262312352657318\n",
      "Epoch 57, Step 11 Loss = 0.8022578954696655\n",
      "Epoch 57, Step 12 Loss = 0.31746622920036316\n",
      "Epoch 57, Step 13 Loss = 0.24719637632369995\n",
      "Epoch 57, Step 14 Loss = 0.516848087310791\n",
      "Epoch 57, Step 15 Loss = 0.4650048613548279\n",
      "Epoch 57, Step 16 Loss = 0.7929801940917969\n",
      "Epoch 57, Step 17 Loss = 0.23098793625831604\n",
      "Epoch 57, Step 18 Loss = 0.5608456134796143\n",
      "Epoch 57, Step 19 Loss = 0.7890176773071289\n",
      "Epoch 57, Step 20 Loss = 0.8590410351753235\n",
      "Epoch 57, Step 21 Loss = 0.41832292079925537\n",
      "Epoch 57, Step 22 Loss = 0.431753545999527\n",
      "Epoch 57, Step 23 Loss = 0.4549574851989746\n",
      "Epoch 57, Step 24 Loss = 0.39930108189582825\n",
      "Epoch 57, Step 25 Loss = 0.6157582998275757\n",
      "Epoch 57, Step 26 Loss = 0.6700260639190674\n",
      "Epoch 57, Step 27 Loss = 0.725674033164978\n",
      "Epoch 57, Step 28 Loss = 0.4930911064147949\n",
      "Epoch 57, Step 29 Loss = 0.8791938424110413\n",
      "Epoch 57, Step 30 Loss = 0.46612033247947693\n",
      "Epoch 57, Step 31 Loss = 1.1527009010314941\n",
      "Epoch 57, Step 32 Loss = 1.0407085418701172\n",
      "Epoch 57, Step 33 Loss = 0.6236858367919922\n",
      "Epoch 57, Step 34 Loss = 0.7252767086029053\n",
      "Epoch 57, Step 35 Loss = 0.8300792574882507\n",
      "Epoch 57, Step 36 Loss = 0.9649479389190674\n",
      "Epoch 57, Step 37 Loss = 0.5182994604110718\n",
      "Epoch 57, Step 38 Loss = 0.6948019862174988\n",
      "Epoch 57, Step 39 Loss = 0.588423490524292\n",
      "Epoch 57, Step 40 Loss = 0.8959718942642212\n",
      "Epoch 57, Step 41 Loss = 0.8475920557975769\n",
      "Training loss: 0.6181\n",
      "Epoch 58/500\n",
      "Epoch 58, Step 1 Loss = 0.4431804418563843\n",
      "Epoch 58, Step 2 Loss = 0.7900282144546509\n",
      "Epoch 58, Step 3 Loss = 0.5599657297134399\n",
      "Epoch 58, Step 4 Loss = 0.541380763053894\n",
      "Epoch 58, Step 5 Loss = 0.6650669574737549\n",
      "Epoch 58, Step 6 Loss = 0.4647517800331116\n",
      "Epoch 58, Step 7 Loss = 0.6904523968696594\n",
      "Epoch 58, Step 8 Loss = 0.5285847187042236\n",
      "Epoch 58, Step 9 Loss = 0.49001094698905945\n",
      "Epoch 58, Step 10 Loss = 0.5253512263298035\n",
      "Epoch 58, Step 11 Loss = 0.7702738046646118\n",
      "Epoch 58, Step 12 Loss = 0.39833545684814453\n",
      "Epoch 58, Step 13 Loss = 0.6530777812004089\n",
      "Epoch 58, Step 14 Loss = 0.4013671278953552\n",
      "Epoch 58, Step 15 Loss = 0.5824021697044373\n",
      "Epoch 58, Step 16 Loss = 0.5251734256744385\n",
      "Epoch 58, Step 17 Loss = 0.34897172451019287\n",
      "Epoch 58, Step 18 Loss = 0.3686290383338928\n",
      "Epoch 58, Step 19 Loss = 0.5982304811477661\n",
      "Epoch 58, Step 20 Loss = 0.6063274145126343\n",
      "Epoch 58, Step 21 Loss = 1.106917142868042\n",
      "Epoch 58, Step 22 Loss = 0.3854162395000458\n",
      "Epoch 58, Step 23 Loss = 0.829171359539032\n",
      "Epoch 58, Step 24 Loss = 0.7593356966972351\n",
      "Epoch 58, Step 25 Loss = 0.7541483640670776\n",
      "Epoch 58, Step 26 Loss = 0.6881011724472046\n",
      "Epoch 58, Step 27 Loss = 0.3919558823108673\n",
      "Epoch 58, Step 28 Loss = 0.6687222719192505\n",
      "Epoch 58, Step 29 Loss = 0.9289774894714355\n",
      "Epoch 58, Step 30 Loss = 0.5294128060340881\n",
      "Epoch 58, Step 31 Loss = 0.6031274795532227\n",
      "Epoch 58, Step 32 Loss = 0.6183173060417175\n",
      "Epoch 58, Step 33 Loss = 0.6594215631484985\n",
      "Epoch 58, Step 34 Loss = 0.8173757791519165\n",
      "Epoch 58, Step 35 Loss = 0.4284203350543976\n",
      "Epoch 58, Step 36 Loss = 0.390112042427063\n",
      "Epoch 58, Step 37 Loss = 0.6526590585708618\n",
      "Epoch 58, Step 38 Loss = 0.5798680782318115\n",
      "Epoch 58, Step 39 Loss = 0.6954068541526794\n",
      "Epoch 58, Step 40 Loss = 1.1824908256530762\n",
      "Epoch 58, Step 41 Loss = 0.7239644527435303\n",
      "Training loss: 0.6182\n",
      "Epoch 59/500\n",
      "Epoch 59, Step 1 Loss = 0.6382122039794922\n",
      "Epoch 59, Step 2 Loss = 0.8934696912765503\n",
      "Epoch 59, Step 3 Loss = 0.47105473279953003\n",
      "Epoch 59, Step 4 Loss = 0.4648367464542389\n",
      "Epoch 59, Step 5 Loss = 0.4084668755531311\n",
      "Epoch 59, Step 6 Loss = 0.6635538339614868\n",
      "Epoch 59, Step 7 Loss = 0.5034911632537842\n",
      "Epoch 59, Step 8 Loss = 0.32453805208206177\n",
      "Epoch 59, Step 9 Loss = 0.2832379937171936\n",
      "Epoch 59, Step 10 Loss = 0.4042476415634155\n",
      "Epoch 59, Step 11 Loss = 0.5257658362388611\n",
      "Epoch 59, Step 12 Loss = 0.2871752381324768\n",
      "Epoch 59, Step 13 Loss = 0.6245465278625488\n",
      "Epoch 59, Step 14 Loss = 0.6068422794342041\n",
      "Epoch 59, Step 15 Loss = 0.27162966132164\n",
      "Epoch 59, Step 16 Loss = 0.5146080255508423\n",
      "Epoch 59, Step 17 Loss = 0.3833603262901306\n",
      "Epoch 59, Step 18 Loss = 0.6340361833572388\n",
      "Epoch 59, Step 19 Loss = 0.37163597345352173\n",
      "Epoch 59, Step 20 Loss = 0.797603964805603\n",
      "Epoch 59, Step 21 Loss = 0.4639005661010742\n",
      "Epoch 59, Step 22 Loss = 0.5436109900474548\n",
      "Epoch 59, Step 23 Loss = 0.6575946807861328\n",
      "Epoch 59, Step 24 Loss = 0.7885204553604126\n",
      "Epoch 59, Step 25 Loss = 0.3303477466106415\n",
      "Epoch 59, Step 26 Loss = 0.6147084832191467\n",
      "Epoch 59, Step 27 Loss = 0.9072027206420898\n",
      "Epoch 59, Step 28 Loss = 0.8488214015960693\n",
      "Epoch 59, Step 29 Loss = 0.9094517230987549\n",
      "Epoch 59, Step 30 Loss = 1.2991732358932495\n",
      "Epoch 59, Step 31 Loss = 0.7079324722290039\n",
      "Epoch 59, Step 32 Loss = 0.5508592128753662\n",
      "Epoch 59, Step 33 Loss = 0.6858216524124146\n",
      "Epoch 59, Step 34 Loss = 0.3108194172382355\n",
      "Epoch 59, Step 35 Loss = 0.40634769201278687\n",
      "Epoch 59, Step 36 Loss = 0.9970794320106506\n",
      "Epoch 59, Step 37 Loss = 1.0452653169631958\n",
      "Epoch 59, Step 38 Loss = 0.7542153596878052\n",
      "Epoch 59, Step 39 Loss = 0.6348905563354492\n",
      "Epoch 59, Step 40 Loss = 0.9344221353530884\n",
      "Epoch 59, Step 41 Loss = 0.4478555917739868\n",
      "Training loss: 0.6076\n",
      "Epoch 60/500\n",
      "Epoch 60, Step 1 Loss = 0.1653062403202057\n",
      "Epoch 60, Step 2 Loss = 0.3648357391357422\n",
      "Epoch 60, Step 3 Loss = 0.348421186208725\n",
      "Epoch 60, Step 4 Loss = 0.3993532359600067\n",
      "Epoch 60, Step 5 Loss = 0.8196223974227905\n",
      "Epoch 60, Step 6 Loss = 0.5885441899299622\n",
      "Epoch 60, Step 7 Loss = 0.589523196220398\n",
      "Epoch 60, Step 8 Loss = 0.3741607666015625\n",
      "Epoch 60, Step 9 Loss = 0.522322416305542\n",
      "Epoch 60, Step 10 Loss = 0.3363924026489258\n",
      "Epoch 60, Step 11 Loss = 0.9235086441040039\n",
      "Epoch 60, Step 12 Loss = 0.7878403663635254\n",
      "Epoch 60, Step 13 Loss = 0.7898657917976379\n",
      "Epoch 60, Step 14 Loss = 0.2968370318412781\n",
      "Epoch 60, Step 15 Loss = 0.7179514765739441\n",
      "Epoch 60, Step 16 Loss = 0.7409127950668335\n",
      "Epoch 60, Step 17 Loss = 0.6103854775428772\n",
      "Epoch 60, Step 18 Loss = 0.26638805866241455\n",
      "Epoch 60, Step 19 Loss = 0.6308756470680237\n",
      "Epoch 60, Step 20 Loss = 0.7497915029525757\n",
      "Epoch 60, Step 21 Loss = 0.9134382009506226\n",
      "Epoch 60, Step 22 Loss = 0.3359408676624298\n",
      "Epoch 60, Step 23 Loss = 0.5881573557853699\n",
      "Epoch 60, Step 24 Loss = 0.41233453154563904\n",
      "Epoch 60, Step 25 Loss = 0.5107308626174927\n",
      "Epoch 60, Step 26 Loss = 0.7240222692489624\n",
      "Epoch 60, Step 27 Loss = 0.6515942215919495\n",
      "Epoch 60, Step 28 Loss = 0.4380970895290375\n",
      "Epoch 60, Step 29 Loss = 0.4389023780822754\n",
      "Epoch 60, Step 30 Loss = 0.7106645703315735\n",
      "Epoch 60, Step 31 Loss = 0.8202553391456604\n",
      "Epoch 60, Step 32 Loss = 0.6663781404495239\n",
      "Epoch 60, Step 33 Loss = 0.8401786088943481\n",
      "Epoch 60, Step 34 Loss = 0.8459086418151855\n",
      "Epoch 60, Step 35 Loss = 0.6097379922866821\n",
      "Epoch 60, Step 36 Loss = 0.8177121877670288\n",
      "Epoch 60, Step 37 Loss = 0.3601359724998474\n",
      "Epoch 60, Step 38 Loss = 0.9489922523498535\n",
      "Epoch 60, Step 39 Loss = 0.692977249622345\n",
      "Epoch 60, Step 40 Loss = 0.8767614364624023\n",
      "Epoch 60, Step 41 Loss = 0.7774096727371216\n",
      "Training loss: 0.6098\n",
      "Epoch 61/500\n",
      "Epoch 61, Step 1 Loss = 0.49835431575775146\n",
      "Epoch 61, Step 2 Loss = 0.15770858526229858\n",
      "Epoch 61, Step 3 Loss = 0.6508910655975342\n",
      "Epoch 61, Step 4 Loss = 0.25489455461502075\n",
      "Epoch 61, Step 5 Loss = 0.47706741094589233\n",
      "Epoch 61, Step 6 Loss = 0.2504141926765442\n",
      "Epoch 61, Step 7 Loss = 0.6643542051315308\n",
      "Epoch 61, Step 8 Loss = 0.6095227003097534\n",
      "Epoch 61, Step 9 Loss = 0.6685782670974731\n",
      "Epoch 61, Step 10 Loss = 0.5706199407577515\n",
      "Epoch 61, Step 11 Loss = 0.4408174157142639\n",
      "Epoch 61, Step 12 Loss = 0.5905898213386536\n",
      "Epoch 61, Step 13 Loss = 0.3806174397468567\n",
      "Epoch 61, Step 14 Loss = 0.4897896647453308\n",
      "Epoch 61, Step 15 Loss = 1.1493719816207886\n",
      "Epoch 61, Step 16 Loss = 0.6643005609512329\n",
      "Epoch 61, Step 17 Loss = 0.7188918590545654\n",
      "Epoch 61, Step 18 Loss = 0.6321900486946106\n",
      "Epoch 61, Step 19 Loss = 0.5554641485214233\n",
      "Epoch 61, Step 20 Loss = 0.4774222671985626\n",
      "Epoch 61, Step 21 Loss = 0.5441564321517944\n",
      "Epoch 61, Step 22 Loss = 0.6951295137405396\n",
      "Epoch 61, Step 23 Loss = 0.6116376519203186\n",
      "Epoch 61, Step 24 Loss = 0.6371510028839111\n",
      "Epoch 61, Step 25 Loss = 0.5307894945144653\n",
      "Epoch 61, Step 26 Loss = 0.7055890560150146\n",
      "Epoch 61, Step 27 Loss = 1.0784733295440674\n",
      "Epoch 61, Step 28 Loss = 0.7984220385551453\n",
      "Epoch 61, Step 29 Loss = 0.7421742677688599\n",
      "Epoch 61, Step 30 Loss = 0.7000463604927063\n",
      "Epoch 61, Step 31 Loss = 0.47508078813552856\n",
      "Epoch 61, Step 32 Loss = 0.52090984582901\n",
      "Epoch 61, Step 33 Loss = 0.5841386318206787\n",
      "Epoch 61, Step 34 Loss = 0.5671825408935547\n",
      "Epoch 61, Step 35 Loss = 0.7497486472129822\n",
      "Epoch 61, Step 36 Loss = 1.0670812129974365\n",
      "Epoch 61, Step 37 Loss = 0.6553899645805359\n",
      "Epoch 61, Step 38 Loss = 0.8324688673019409\n",
      "Epoch 61, Step 39 Loss = 0.5203078389167786\n",
      "Epoch 61, Step 40 Loss = 0.6768167614936829\n",
      "Epoch 61, Step 41 Loss = 0.9870184659957886\n",
      "Training loss: 0.6239\n",
      "Epoch 62/500\n",
      "Epoch 62, Step 1 Loss = 0.8568417429924011\n",
      "Epoch 62, Step 2 Loss = 0.4387836456298828\n",
      "Epoch 62, Step 3 Loss = 0.4580885171890259\n",
      "Epoch 62, Step 4 Loss = 0.3255794942378998\n",
      "Epoch 62, Step 5 Loss = 0.6952990889549255\n",
      "Epoch 62, Step 6 Loss = 0.6774255037307739\n",
      "Epoch 62, Step 7 Loss = 0.3730587363243103\n",
      "Epoch 62, Step 8 Loss = 0.515453577041626\n",
      "Epoch 62, Step 9 Loss = 0.475525438785553\n",
      "Epoch 62, Step 10 Loss = 0.373934805393219\n",
      "Epoch 62, Step 11 Loss = 0.628688395023346\n",
      "Epoch 62, Step 12 Loss = 0.5527285933494568\n",
      "Epoch 62, Step 13 Loss = 0.4528120458126068\n",
      "Epoch 62, Step 14 Loss = 0.41039684414863586\n",
      "Epoch 62, Step 15 Loss = 0.36855262517929077\n",
      "Epoch 62, Step 16 Loss = 0.6439610719680786\n",
      "Epoch 62, Step 17 Loss = 0.5930758714675903\n",
      "Epoch 62, Step 18 Loss = 0.4222174286842346\n",
      "Epoch 62, Step 19 Loss = 0.958396315574646\n",
      "Epoch 62, Step 20 Loss = 0.4911756217479706\n",
      "Epoch 62, Step 21 Loss = 0.6277949810028076\n",
      "Epoch 62, Step 22 Loss = 0.38994935154914856\n",
      "Epoch 62, Step 23 Loss = 0.701439619064331\n",
      "Epoch 62, Step 24 Loss = 0.9745084643363953\n",
      "Epoch 62, Step 25 Loss = 0.6321048736572266\n",
      "Epoch 62, Step 26 Loss = 0.3800594210624695\n",
      "Epoch 62, Step 27 Loss = 0.8565556406974792\n",
      "Epoch 62, Step 28 Loss = 0.5473671555519104\n",
      "Epoch 62, Step 29 Loss = 0.6004092693328857\n",
      "Epoch 62, Step 30 Loss = 0.7365143299102783\n",
      "Epoch 62, Step 31 Loss = 0.6095774173736572\n",
      "Epoch 62, Step 32 Loss = 0.6312921643257141\n",
      "Epoch 62, Step 33 Loss = 0.6151793003082275\n",
      "Epoch 62, Step 34 Loss = 0.5506194233894348\n",
      "Epoch 62, Step 35 Loss = 0.958289384841919\n",
      "Epoch 62, Step 36 Loss = 0.7705167531967163\n",
      "Epoch 62, Step 37 Loss = 1.0209453105926514\n",
      "Epoch 62, Step 38 Loss = 0.9067521095275879\n",
      "Epoch 62, Step 39 Loss = 1.0553666353225708\n",
      "Epoch 62, Step 40 Loss = 0.4370238184928894\n",
      "Epoch 62, Step 41 Loss = 0.5635263919830322\n",
      "Training loss: 0.6165\n",
      "Epoch 63/500\n",
      "Epoch 63, Step 1 Loss = 0.25290611386299133\n",
      "Epoch 63, Step 2 Loss = 0.625724196434021\n",
      "Epoch 63, Step 3 Loss = 0.47828367352485657\n",
      "Epoch 63, Step 4 Loss = 0.19118019938468933\n",
      "Epoch 63, Step 5 Loss = 0.5967563986778259\n",
      "Epoch 63, Step 6 Loss = 0.5361911058425903\n",
      "Epoch 63, Step 7 Loss = 0.4401518404483795\n",
      "Epoch 63, Step 8 Loss = 0.7832390666007996\n",
      "Epoch 63, Step 9 Loss = 1.209413766860962\n",
      "Epoch 63, Step 10 Loss = 0.2916657030582428\n",
      "Epoch 63, Step 11 Loss = 0.8216984272003174\n",
      "Epoch 63, Step 12 Loss = 0.6431329846382141\n",
      "Epoch 63, Step 13 Loss = 0.3452356457710266\n",
      "Epoch 63, Step 14 Loss = 0.8432742357254028\n",
      "Epoch 63, Step 15 Loss = 0.41171956062316895\n",
      "Epoch 63, Step 16 Loss = 0.36730167269706726\n",
      "Epoch 63, Step 17 Loss = 0.3381372094154358\n",
      "Epoch 63, Step 18 Loss = 0.5468451976776123\n",
      "Epoch 63, Step 19 Loss = 0.871776819229126\n",
      "Epoch 63, Step 20 Loss = 0.7832624912261963\n",
      "Epoch 63, Step 21 Loss = 0.3454466760158539\n",
      "Epoch 63, Step 22 Loss = 0.38858944177627563\n",
      "Epoch 63, Step 23 Loss = 0.4486117660999298\n",
      "Epoch 63, Step 24 Loss = 0.6849979162216187\n",
      "Epoch 63, Step 25 Loss = 0.4211338758468628\n",
      "Epoch 63, Step 26 Loss = 0.628855288028717\n",
      "Epoch 63, Step 27 Loss = 0.6004905104637146\n",
      "Epoch 63, Step 28 Loss = 0.544602632522583\n",
      "Epoch 63, Step 29 Loss = 0.6562843322753906\n",
      "Epoch 63, Step 30 Loss = 1.0552525520324707\n",
      "Epoch 63, Step 31 Loss = 0.6451452970504761\n",
      "Epoch 63, Step 32 Loss = 0.7330726385116577\n",
      "Epoch 63, Step 33 Loss = 0.2142031192779541\n",
      "Epoch 63, Step 34 Loss = 0.4361518621444702\n",
      "Epoch 63, Step 35 Loss = 0.9163880348205566\n",
      "Epoch 63, Step 36 Loss = 0.7313479781150818\n",
      "Epoch 63, Step 37 Loss = 1.017079472541809\n",
      "Epoch 63, Step 38 Loss = 0.7244481444358826\n",
      "Epoch 63, Step 39 Loss = 0.5785478353500366\n",
      "Epoch 63, Step 40 Loss = 0.9436582326889038\n",
      "Epoch 63, Step 41 Loss = 0.7302031517028809\n",
      "Training loss: 0.6054\n",
      "Epoch 64/500\n",
      "Epoch 64, Step 1 Loss = 0.39524969458580017\n",
      "Epoch 64, Step 2 Loss = 0.4772705137729645\n",
      "Epoch 64, Step 3 Loss = 0.304862916469574\n",
      "Epoch 64, Step 4 Loss = 0.582161009311676\n",
      "Epoch 64, Step 5 Loss = 0.41116875410079956\n",
      "Epoch 64, Step 6 Loss = 0.6590643525123596\n",
      "Epoch 64, Step 7 Loss = 0.6135022640228271\n",
      "Epoch 64, Step 8 Loss = 0.5885058045387268\n",
      "Epoch 64, Step 9 Loss = 0.38916027545928955\n",
      "Epoch 64, Step 10 Loss = 0.2600679099559784\n",
      "Epoch 64, Step 11 Loss = 0.6912604570388794\n",
      "Epoch 64, Step 12 Loss = 0.6714470386505127\n",
      "Epoch 64, Step 13 Loss = 0.5964275598526001\n",
      "Epoch 64, Step 14 Loss = 0.5120370388031006\n",
      "Epoch 64, Step 15 Loss = 0.7100390195846558\n",
      "Epoch 64, Step 16 Loss = 0.6115869283676147\n",
      "Epoch 64, Step 17 Loss = 0.46503549814224243\n",
      "Epoch 64, Step 18 Loss = 0.7793615460395813\n",
      "Epoch 64, Step 19 Loss = 0.6148645877838135\n",
      "Epoch 64, Step 20 Loss = 0.5459237098693848\n",
      "Epoch 64, Step 21 Loss = 0.43228667974472046\n",
      "Epoch 64, Step 22 Loss = 0.926908016204834\n",
      "Epoch 64, Step 23 Loss = 0.37372058629989624\n",
      "Epoch 64, Step 24 Loss = 0.385468453168869\n",
      "Epoch 64, Step 25 Loss = 0.5323600172996521\n",
      "Epoch 64, Step 26 Loss = 0.386249303817749\n",
      "Epoch 64, Step 27 Loss = 0.7459312677383423\n",
      "Epoch 64, Step 28 Loss = 0.6569541692733765\n",
      "Epoch 64, Step 29 Loss = 0.3993985056877136\n",
      "Epoch 64, Step 30 Loss = 0.6442273259162903\n",
      "Epoch 64, Step 31 Loss = 0.38389986753463745\n",
      "Epoch 64, Step 32 Loss = 0.6502044200897217\n",
      "Epoch 64, Step 33 Loss = 0.4636237919330597\n",
      "Epoch 64, Step 34 Loss = 0.6964765787124634\n",
      "Epoch 64, Step 35 Loss = 0.4938907325267792\n",
      "Epoch 64, Step 36 Loss = 1.060250997543335\n",
      "Epoch 64, Step 37 Loss = 0.8935827016830444\n",
      "Epoch 64, Step 38 Loss = 0.6464841961860657\n",
      "Epoch 64, Step 39 Loss = 1.066011667251587\n",
      "Epoch 64, Step 40 Loss = 1.0261013507843018\n",
      "Epoch 64, Step 41 Loss = 0.9799051880836487\n",
      "Training loss: 0.6030\n",
      "Epoch 65/500\n",
      "Epoch 65, Step 1 Loss = 0.598768413066864\n",
      "Epoch 65, Step 2 Loss = 0.45980304479599\n",
      "Epoch 65, Step 3 Loss = 0.49876153469085693\n",
      "Epoch 65, Step 4 Loss = 0.5825021862983704\n",
      "Epoch 65, Step 5 Loss = 0.500694215297699\n",
      "Epoch 65, Step 6 Loss = 0.6861879229545593\n",
      "Epoch 65, Step 7 Loss = 0.6748175621032715\n",
      "Epoch 65, Step 8 Loss = 0.6896620392799377\n",
      "Epoch 65, Step 9 Loss = 0.6404023766517639\n",
      "Epoch 65, Step 10 Loss = 0.3869311511516571\n",
      "Epoch 65, Step 11 Loss = 0.5092605948448181\n",
      "Epoch 65, Step 12 Loss = 0.36359918117523193\n",
      "Epoch 65, Step 13 Loss = 0.6382215023040771\n",
      "Epoch 65, Step 14 Loss = 0.561316728591919\n",
      "Epoch 65, Step 15 Loss = 0.455471396446228\n",
      "Epoch 65, Step 16 Loss = 0.24986332654953003\n",
      "Epoch 65, Step 17 Loss = 0.5331652164459229\n",
      "Epoch 65, Step 18 Loss = 0.7686081528663635\n",
      "Epoch 65, Step 19 Loss = 0.4020935893058777\n",
      "Epoch 65, Step 20 Loss = 0.2884364724159241\n",
      "Epoch 65, Step 21 Loss = 0.5631004571914673\n",
      "Epoch 65, Step 22 Loss = 0.7721484899520874\n",
      "Epoch 65, Step 23 Loss = 0.7319045066833496\n",
      "Epoch 65, Step 24 Loss = 0.7067373991012573\n",
      "Epoch 65, Step 25 Loss = 0.5531026721000671\n",
      "Epoch 65, Step 26 Loss = 0.7511991262435913\n",
      "Epoch 65, Step 27 Loss = 0.8443582653999329\n",
      "Epoch 65, Step 28 Loss = 0.6882197260856628\n",
      "Epoch 65, Step 29 Loss = 0.4953952729701996\n",
      "Epoch 65, Step 30 Loss = 0.9032635688781738\n",
      "Epoch 65, Step 31 Loss = 0.5825286507606506\n",
      "Epoch 65, Step 32 Loss = 0.5720551013946533\n",
      "Epoch 65, Step 33 Loss = 0.8644165992736816\n",
      "Epoch 65, Step 34 Loss = 0.591819167137146\n",
      "Epoch 65, Step 35 Loss = 0.6543698906898499\n",
      "Epoch 65, Step 36 Loss = 1.0967023372650146\n",
      "Epoch 65, Step 37 Loss = 0.8732784986495972\n",
      "Epoch 65, Step 38 Loss = 0.7321988344192505\n",
      "Epoch 65, Step 39 Loss = 0.6871713399887085\n",
      "Epoch 65, Step 40 Loss = 0.2546912431716919\n",
      "Epoch 65, Step 41 Loss = 0.641655683517456\n",
      "Training loss: 0.6109\n",
      "Epoch 66/500\n",
      "Epoch 66, Step 1 Loss = 0.5964556932449341\n",
      "Epoch 66, Step 2 Loss = 0.3410416841506958\n",
      "Epoch 66, Step 3 Loss = 0.5345484614372253\n",
      "Epoch 66, Step 4 Loss = 0.4178091287612915\n",
      "Epoch 66, Step 5 Loss = 0.4773455262184143\n",
      "Epoch 66, Step 6 Loss = 0.44436782598495483\n",
      "Epoch 66, Step 7 Loss = 0.6589377522468567\n",
      "Epoch 66, Step 8 Loss = 0.5790503621101379\n",
      "Epoch 66, Step 9 Loss = 0.35484373569488525\n",
      "Epoch 66, Step 10 Loss = 0.5062435865402222\n",
      "Epoch 66, Step 11 Loss = 0.4020876884460449\n",
      "Epoch 66, Step 12 Loss = 0.26237723231315613\n",
      "Epoch 66, Step 13 Loss = 0.8543701171875\n",
      "Epoch 66, Step 14 Loss = 0.38819271326065063\n",
      "Epoch 66, Step 15 Loss = 0.5210872292518616\n",
      "Epoch 66, Step 16 Loss = 0.47553133964538574\n",
      "Epoch 66, Step 17 Loss = 0.8369231224060059\n",
      "Epoch 66, Step 18 Loss = 0.7319895029067993\n",
      "Epoch 66, Step 19 Loss = 0.7066317796707153\n",
      "Epoch 66, Step 20 Loss = 0.8081454038619995\n",
      "Epoch 66, Step 21 Loss = 0.4074593782424927\n",
      "Epoch 66, Step 22 Loss = 1.0842552185058594\n",
      "Epoch 66, Step 23 Loss = 0.5872935652732849\n",
      "Epoch 66, Step 24 Loss = 0.623049259185791\n",
      "Epoch 66, Step 25 Loss = 0.5739412307739258\n",
      "Epoch 66, Step 26 Loss = 1.0686123371124268\n",
      "Epoch 66, Step 27 Loss = 0.7839942574501038\n",
      "Epoch 66, Step 28 Loss = 0.8054478764533997\n",
      "Epoch 66, Step 29 Loss = 0.5713580846786499\n",
      "Epoch 66, Step 30 Loss = 0.2923353612422943\n",
      "Epoch 66, Step 31 Loss = 0.4607025384902954\n",
      "Epoch 66, Step 32 Loss = 1.0778475999832153\n",
      "Epoch 66, Step 33 Loss = 0.8774121999740601\n",
      "Epoch 66, Step 34 Loss = 0.5166187286376953\n",
      "Epoch 66, Step 35 Loss = 0.8880949020385742\n",
      "Epoch 66, Step 36 Loss = 0.2779436409473419\n",
      "Epoch 66, Step 37 Loss = 0.7738469243049622\n",
      "Epoch 66, Step 38 Loss = 0.8490915298461914\n",
      "Epoch 66, Step 39 Loss = 0.3939506709575653\n",
      "Epoch 66, Step 40 Loss = 0.7498331069946289\n",
      "Epoch 66, Step 41 Loss = 0.9883576035499573\n",
      "Training loss: 0.6232\n",
      "Epoch 67/500\n",
      "Epoch 67, Step 1 Loss = 0.2390023171901703\n",
      "Epoch 67, Step 2 Loss = 0.5319135189056396\n",
      "Epoch 67, Step 3 Loss = 0.3838067650794983\n",
      "Epoch 67, Step 4 Loss = 0.30451613664627075\n",
      "Epoch 67, Step 5 Loss = 0.3060561418533325\n",
      "Epoch 67, Step 6 Loss = 0.4553317427635193\n",
      "Epoch 67, Step 7 Loss = 0.5870660543441772\n",
      "Epoch 67, Step 8 Loss = 0.41970303654670715\n",
      "Epoch 67, Step 9 Loss = 0.4045047163963318\n",
      "Epoch 67, Step 10 Loss = 0.28385475277900696\n",
      "Epoch 67, Step 11 Loss = 0.15175271034240723\n",
      "Epoch 67, Step 12 Loss = 0.6741309762001038\n",
      "Epoch 67, Step 13 Loss = 0.37981247901916504\n",
      "Epoch 67, Step 14 Loss = 1.048181176185608\n",
      "Epoch 67, Step 15 Loss = 0.2479238212108612\n",
      "Epoch 67, Step 16 Loss = 0.5064328908920288\n",
      "Epoch 67, Step 17 Loss = 0.4795905649662018\n",
      "Epoch 67, Step 18 Loss = 1.1067407131195068\n",
      "Epoch 67, Step 19 Loss = 0.8318814039230347\n",
      "Epoch 67, Step 20 Loss = 0.6232420206069946\n",
      "Epoch 67, Step 21 Loss = 0.47329002618789673\n",
      "Epoch 67, Step 22 Loss = 0.5647596716880798\n",
      "Epoch 67, Step 23 Loss = 0.819422721862793\n",
      "Epoch 67, Step 24 Loss = 0.6628310680389404\n",
      "Epoch 67, Step 25 Loss = 0.7251559495925903\n",
      "Epoch 67, Step 26 Loss = 0.47002729773521423\n",
      "Epoch 67, Step 27 Loss = 0.5740923881530762\n",
      "Epoch 67, Step 28 Loss = 0.5544861555099487\n",
      "Epoch 67, Step 29 Loss = 0.7207025289535522\n",
      "Epoch 67, Step 30 Loss = 0.80991530418396\n",
      "Epoch 67, Step 31 Loss = 0.7947894930839539\n",
      "Epoch 67, Step 32 Loss = 0.4597547650337219\n",
      "Epoch 67, Step 33 Loss = 0.9019039869308472\n",
      "Epoch 67, Step 34 Loss = 0.8811063170433044\n",
      "Epoch 67, Step 35 Loss = 1.1498606204986572\n",
      "Epoch 67, Step 36 Loss = 0.7768862247467041\n",
      "Epoch 67, Step 37 Loss = 0.6360694169998169\n",
      "Epoch 67, Step 38 Loss = 0.574877142906189\n",
      "Epoch 67, Step 39 Loss = 0.6343512535095215\n",
      "Epoch 67, Step 40 Loss = 0.8061373233795166\n",
      "Epoch 67, Step 41 Loss = 0.7173249125480652\n",
      "Training loss: 0.6018\n",
      "Epoch 68/500\n",
      "Epoch 68, Step 1 Loss = 0.23029959201812744\n",
      "Epoch 68, Step 2 Loss = 0.33842432498931885\n",
      "Epoch 68, Step 3 Loss = 0.650693953037262\n",
      "Epoch 68, Step 4 Loss = 0.3989485204219818\n",
      "Epoch 68, Step 5 Loss = 0.6007909774780273\n",
      "Epoch 68, Step 6 Loss = 0.4175580143928528\n",
      "Epoch 68, Step 7 Loss = 0.3371904492378235\n",
      "Epoch 68, Step 8 Loss = 0.37348315119743347\n",
      "Epoch 68, Step 9 Loss = 0.5048496723175049\n",
      "Epoch 68, Step 10 Loss = 0.5810799598693848\n",
      "Epoch 68, Step 11 Loss = 0.6452943086624146\n",
      "Epoch 68, Step 12 Loss = 0.36889782547950745\n",
      "Epoch 68, Step 13 Loss = 0.7163938283920288\n",
      "Epoch 68, Step 14 Loss = 0.7267370223999023\n",
      "Epoch 68, Step 15 Loss = 0.5073902606964111\n",
      "Epoch 68, Step 16 Loss = 0.7104036211967468\n",
      "Epoch 68, Step 17 Loss = 0.5286697745323181\n",
      "Epoch 68, Step 18 Loss = 0.5876955389976501\n",
      "Epoch 68, Step 19 Loss = 0.5533413290977478\n",
      "Epoch 68, Step 20 Loss = 0.5655327439308167\n",
      "Epoch 68, Step 21 Loss = 0.17453894019126892\n",
      "Epoch 68, Step 22 Loss = 0.8281451463699341\n",
      "Epoch 68, Step 23 Loss = 0.2984529733657837\n",
      "Epoch 68, Step 24 Loss = 0.9772260189056396\n",
      "Epoch 68, Step 25 Loss = 0.157425656914711\n",
      "Epoch 68, Step 26 Loss = 0.4434434175491333\n",
      "Epoch 68, Step 27 Loss = 0.511329174041748\n",
      "Epoch 68, Step 28 Loss = 0.5818817615509033\n",
      "Epoch 68, Step 29 Loss = 1.1271977424621582\n",
      "Epoch 68, Step 30 Loss = 0.5094194412231445\n",
      "Epoch 68, Step 31 Loss = 1.0368096828460693\n",
      "Epoch 68, Step 32 Loss = 0.7745208740234375\n",
      "Epoch 68, Step 33 Loss = 0.9737808704376221\n",
      "Epoch 68, Step 34 Loss = 0.44906479120254517\n",
      "Epoch 68, Step 35 Loss = 0.6742165088653564\n",
      "Epoch 68, Step 36 Loss = 0.9222748875617981\n",
      "Epoch 68, Step 37 Loss = 0.45437657833099365\n",
      "Epoch 68, Step 38 Loss = 1.2446463108062744\n",
      "Epoch 68, Step 39 Loss = 0.6769406795501709\n",
      "Epoch 68, Step 40 Loss = 0.9547463655471802\n",
      "Epoch 68, Step 41 Loss = 0.5161973834037781\n",
      "Training loss: 0.6007\n",
      "Epoch 69/500\n",
      "Epoch 69, Step 1 Loss = 0.5289428234100342\n",
      "Epoch 69, Step 2 Loss = 0.435851514339447\n",
      "Epoch 69, Step 3 Loss = 0.4311734437942505\n",
      "Epoch 69, Step 4 Loss = 0.3842931091785431\n",
      "Epoch 69, Step 5 Loss = 0.3471863865852356\n",
      "Epoch 69, Step 6 Loss = 0.709414005279541\n",
      "Epoch 69, Step 7 Loss = 0.47057318687438965\n",
      "Epoch 69, Step 8 Loss = 0.4099719524383545\n",
      "Epoch 69, Step 9 Loss = 0.6819506883621216\n",
      "Epoch 69, Step 10 Loss = 0.5336595773696899\n",
      "Epoch 69, Step 11 Loss = 0.2973598837852478\n",
      "Epoch 69, Step 12 Loss = 0.5173566937446594\n",
      "Epoch 69, Step 13 Loss = 0.3469204902648926\n",
      "Epoch 69, Step 14 Loss = 0.6193209290504456\n",
      "Epoch 69, Step 15 Loss = 0.686880350112915\n",
      "Epoch 69, Step 16 Loss = 0.5667946338653564\n",
      "Epoch 69, Step 17 Loss = 0.5483893752098083\n",
      "Epoch 69, Step 18 Loss = 0.7733741998672485\n",
      "Epoch 69, Step 19 Loss = 0.623933732509613\n",
      "Epoch 69, Step 20 Loss = 0.38576745986938477\n",
      "Epoch 69, Step 21 Loss = 0.5119832754135132\n",
      "Epoch 69, Step 22 Loss = 0.6175227165222168\n",
      "Epoch 69, Step 23 Loss = 0.8517590761184692\n",
      "Epoch 69, Step 24 Loss = 0.8430383205413818\n",
      "Epoch 69, Step 25 Loss = 0.8621254563331604\n",
      "Epoch 69, Step 26 Loss = 0.4379623830318451\n",
      "Epoch 69, Step 27 Loss = 0.41814446449279785\n",
      "Epoch 69, Step 28 Loss = 0.2751677632331848\n",
      "Epoch 69, Step 29 Loss = 0.44571352005004883\n",
      "Epoch 69, Step 30 Loss = 0.8004156351089478\n",
      "Epoch 69, Step 31 Loss = 0.6621462106704712\n",
      "Epoch 69, Step 32 Loss = 0.8558328151702881\n",
      "Epoch 69, Step 33 Loss = 1.0953682661056519\n",
      "Epoch 69, Step 34 Loss = 0.8298022747039795\n",
      "Epoch 69, Step 35 Loss = 1.0572417974472046\n",
      "Epoch 69, Step 36 Loss = 0.4848279356956482\n",
      "Epoch 69, Step 37 Loss = 0.647158682346344\n",
      "Epoch 69, Step 38 Loss = 0.7961329221725464\n",
      "Epoch 69, Step 39 Loss = 0.6265782117843628\n",
      "Epoch 69, Step 40 Loss = 0.5217163562774658\n",
      "Epoch 69, Step 41 Loss = 0.9265779256820679\n",
      "Training loss: 0.6065\n",
      "Epoch 70/500\n",
      "Epoch 70, Step 1 Loss = 0.49149006605148315\n",
      "Epoch 70, Step 2 Loss = 0.4495512843132019\n",
      "Epoch 70, Step 3 Loss = 0.6400952339172363\n",
      "Epoch 70, Step 4 Loss = 0.41004055738449097\n",
      "Epoch 70, Step 5 Loss = 0.5699285864830017\n",
      "Epoch 70, Step 6 Loss = 0.4836394190788269\n",
      "Epoch 70, Step 7 Loss = 0.6394176483154297\n",
      "Epoch 70, Step 8 Loss = 0.2973555028438568\n",
      "Epoch 70, Step 9 Loss = 0.5191863775253296\n",
      "Epoch 70, Step 10 Loss = 0.41339564323425293\n",
      "Epoch 70, Step 11 Loss = 0.5348430275917053\n",
      "Epoch 70, Step 12 Loss = 0.6511584520339966\n",
      "Epoch 70, Step 13 Loss = 0.37347567081451416\n",
      "Epoch 70, Step 14 Loss = 0.32486093044281006\n",
      "Epoch 70, Step 15 Loss = 0.8046166896820068\n",
      "Epoch 70, Step 16 Loss = 0.20162427425384521\n",
      "Epoch 70, Step 17 Loss = 0.6209919452667236\n",
      "Epoch 70, Step 18 Loss = 0.8268182277679443\n",
      "Epoch 70, Step 19 Loss = 0.8307837843894958\n",
      "Epoch 70, Step 20 Loss = 0.9603710174560547\n",
      "Epoch 70, Step 21 Loss = 0.7063825130462646\n",
      "Epoch 70, Step 22 Loss = 0.5315097570419312\n",
      "Epoch 70, Step 23 Loss = 0.35673895478248596\n",
      "Epoch 70, Step 24 Loss = 0.6565130949020386\n",
      "Epoch 70, Step 25 Loss = 0.45445144176483154\n",
      "Epoch 70, Step 26 Loss = 0.42356663942337036\n",
      "Epoch 70, Step 27 Loss = 0.7477840185165405\n",
      "Epoch 70, Step 28 Loss = 0.731687068939209\n",
      "Epoch 70, Step 29 Loss = 0.6448666453361511\n",
      "Epoch 70, Step 30 Loss = 0.3266480267047882\n",
      "Epoch 70, Step 31 Loss = 0.4725281000137329\n",
      "Epoch 70, Step 32 Loss = 0.8252224922180176\n",
      "Epoch 70, Step 33 Loss = 0.509609580039978\n",
      "Epoch 70, Step 34 Loss = 1.0363471508026123\n",
      "Epoch 70, Step 35 Loss = 0.614526093006134\n",
      "Epoch 70, Step 36 Loss = 0.9004743099212646\n",
      "Epoch 70, Step 37 Loss = 0.6811836957931519\n",
      "Epoch 70, Step 38 Loss = 0.4558393061161041\n",
      "Epoch 70, Step 39 Loss = 0.5177420377731323\n",
      "Epoch 70, Step 40 Loss = 0.9298155307769775\n",
      "Epoch 70, Step 41 Loss = 0.7274901866912842\n",
      "Training loss: 0.5926\n",
      "Epoch 71/500\n",
      "Epoch 71, Step 1 Loss = 0.3158718943595886\n",
      "Epoch 71, Step 2 Loss = 0.5615890026092529\n",
      "Epoch 71, Step 3 Loss = 0.47718530893325806\n",
      "Epoch 71, Step 4 Loss = 0.4390133321285248\n",
      "Epoch 71, Step 5 Loss = 0.390363484621048\n",
      "Epoch 71, Step 6 Loss = 0.5173150897026062\n",
      "Epoch 71, Step 7 Loss = 0.7851202487945557\n",
      "Epoch 71, Step 8 Loss = 0.4515996277332306\n",
      "Epoch 71, Step 9 Loss = 0.6875050067901611\n",
      "Epoch 71, Step 10 Loss = 0.3335266709327698\n",
      "Epoch 71, Step 11 Loss = 0.5979087948799133\n",
      "Epoch 71, Step 12 Loss = 0.5177209973335266\n",
      "Epoch 71, Step 13 Loss = 0.5869405269622803\n",
      "Epoch 71, Step 14 Loss = 0.879761815071106\n",
      "Epoch 71, Step 15 Loss = 0.2716827988624573\n",
      "Epoch 71, Step 16 Loss = 0.37669917941093445\n",
      "Epoch 71, Step 17 Loss = 0.6675926446914673\n",
      "Epoch 71, Step 18 Loss = 0.5987234711647034\n",
      "Epoch 71, Step 19 Loss = 0.43537789583206177\n",
      "Epoch 71, Step 20 Loss = 0.7508044242858887\n",
      "Epoch 71, Step 21 Loss = 0.8638437390327454\n",
      "Epoch 71, Step 22 Loss = 0.7604775428771973\n",
      "Epoch 71, Step 23 Loss = 0.6711832284927368\n",
      "Epoch 71, Step 24 Loss = 0.5805686712265015\n",
      "Epoch 71, Step 25 Loss = 0.778257429599762\n",
      "Epoch 71, Step 26 Loss = 0.4610332250595093\n",
      "Epoch 71, Step 27 Loss = 0.8086308240890503\n",
      "Epoch 71, Step 28 Loss = 0.4096921980381012\n",
      "Epoch 71, Step 29 Loss = 0.6031618118286133\n",
      "Epoch 71, Step 30 Loss = 0.7287091612815857\n",
      "Epoch 71, Step 31 Loss = 0.5816653966903687\n",
      "Epoch 71, Step 32 Loss = 0.9389772415161133\n",
      "Epoch 71, Step 33 Loss = 0.7031662464141846\n",
      "Epoch 71, Step 34 Loss = 0.8826586008071899\n",
      "Epoch 71, Step 35 Loss = 0.24291172623634338\n",
      "Epoch 71, Step 36 Loss = 1.0684189796447754\n",
      "Epoch 71, Step 37 Loss = 0.5227507948875427\n",
      "Epoch 71, Step 38 Loss = 0.38845041394233704\n",
      "Epoch 71, Step 39 Loss = 0.4341312646865845\n",
      "Epoch 71, Step 40 Loss = 0.7308313846588135\n",
      "Epoch 71, Step 41 Loss = 1.0261850357055664\n",
      "Training loss: 0.6056\n",
      "Epoch 72/500\n",
      "Epoch 72, Step 1 Loss = 0.25212275981903076\n",
      "Epoch 72, Step 2 Loss = 0.26684296131134033\n",
      "Epoch 72, Step 3 Loss = 0.9788637161254883\n",
      "Epoch 72, Step 4 Loss = 0.6200833320617676\n",
      "Epoch 72, Step 5 Loss = 0.5905122756958008\n",
      "Epoch 72, Step 6 Loss = 0.3915742337703705\n",
      "Epoch 72, Step 7 Loss = 0.7102283239364624\n",
      "Epoch 72, Step 8 Loss = 0.2718066871166229\n",
      "Epoch 72, Step 9 Loss = 0.8675045967102051\n",
      "Epoch 72, Step 10 Loss = 0.3293720483779907\n",
      "Epoch 72, Step 11 Loss = 0.6412074565887451\n",
      "Epoch 72, Step 12 Loss = 0.5448302626609802\n",
      "Epoch 72, Step 13 Loss = 0.4129064083099365\n",
      "Epoch 72, Step 14 Loss = 0.4033069610595703\n",
      "Epoch 72, Step 15 Loss = 0.5961712002754211\n",
      "Epoch 72, Step 16 Loss = 0.4665753245353699\n",
      "Epoch 72, Step 17 Loss = 0.5935978889465332\n",
      "Epoch 72, Step 18 Loss = 0.4835371971130371\n",
      "Epoch 72, Step 19 Loss = 0.886263906955719\n",
      "Epoch 72, Step 20 Loss = 0.5474629402160645\n",
      "Epoch 72, Step 21 Loss = 0.6981067657470703\n",
      "Epoch 72, Step 22 Loss = 0.48835980892181396\n",
      "Epoch 72, Step 23 Loss = 0.4465055465698242\n",
      "Epoch 72, Step 24 Loss = 0.5499848127365112\n",
      "Epoch 72, Step 25 Loss = 0.38227546215057373\n",
      "Epoch 72, Step 26 Loss = 0.9136996269226074\n",
      "Epoch 72, Step 27 Loss = 0.6992769241333008\n",
      "Epoch 72, Step 28 Loss = 0.6362673044204712\n",
      "Epoch 72, Step 29 Loss = 0.6112996935844421\n",
      "Epoch 72, Step 30 Loss = 0.694441020488739\n",
      "Epoch 72, Step 31 Loss = 0.4165615439414978\n",
      "Epoch 72, Step 32 Loss = 1.0063060522079468\n",
      "Epoch 72, Step 33 Loss = 0.7315552234649658\n",
      "Epoch 72, Step 34 Loss = 0.5398465394973755\n",
      "Epoch 72, Step 35 Loss = 0.8259693384170532\n",
      "Epoch 72, Step 36 Loss = 0.8419829607009888\n",
      "Epoch 72, Step 37 Loss = 0.9194104671478271\n",
      "Epoch 72, Step 38 Loss = 0.7905054688453674\n",
      "Epoch 72, Step 39 Loss = 1.144862413406372\n",
      "Epoch 72, Step 40 Loss = 0.5056084990501404\n",
      "Epoch 72, Step 41 Loss = 0.7404612302780151\n",
      "Training loss: 0.6204\n",
      "Epoch 73/500\n",
      "Epoch 73, Step 1 Loss = 0.48788607120513916\n",
      "Epoch 73, Step 2 Loss = 0.6240874528884888\n",
      "Epoch 73, Step 3 Loss = 0.49336865544319153\n",
      "Epoch 73, Step 4 Loss = 0.5188947916030884\n",
      "Epoch 73, Step 5 Loss = 0.35358113050460815\n",
      "Epoch 73, Step 6 Loss = 0.43670549988746643\n",
      "Epoch 73, Step 7 Loss = 0.5659325122833252\n",
      "Epoch 73, Step 8 Loss = 0.7511197328567505\n",
      "Epoch 73, Step 9 Loss = 0.45542269945144653\n",
      "Epoch 73, Step 10 Loss = 0.5600239038467407\n",
      "Epoch 73, Step 11 Loss = 0.5526351928710938\n",
      "Epoch 73, Step 12 Loss = 0.6625617742538452\n",
      "Epoch 73, Step 13 Loss = 0.40239518880844116\n",
      "Epoch 73, Step 14 Loss = 0.38146674633026123\n",
      "Epoch 73, Step 15 Loss = 0.577057421207428\n",
      "Epoch 73, Step 16 Loss = 0.46330153942108154\n",
      "Epoch 73, Step 17 Loss = 0.449485182762146\n",
      "Epoch 73, Step 18 Loss = 0.44777849316596985\n",
      "Epoch 73, Step 19 Loss = 0.46287018060684204\n",
      "Epoch 73, Step 20 Loss = 0.47876542806625366\n",
      "Epoch 73, Step 21 Loss = 0.6392223238945007\n",
      "Epoch 73, Step 22 Loss = 0.603643000125885\n",
      "Epoch 73, Step 23 Loss = 0.5151824951171875\n",
      "Epoch 73, Step 24 Loss = 0.6835305690765381\n",
      "Epoch 73, Step 25 Loss = 0.6942112445831299\n",
      "Epoch 73, Step 26 Loss = 0.8844776153564453\n",
      "Epoch 73, Step 27 Loss = 0.5404369831085205\n",
      "Epoch 73, Step 28 Loss = 0.6672011613845825\n",
      "Epoch 73, Step 29 Loss = 1.0010008811950684\n",
      "Epoch 73, Step 30 Loss = 0.6942788362503052\n",
      "Epoch 73, Step 31 Loss = 0.6501908302307129\n",
      "Epoch 73, Step 32 Loss = 0.4891483187675476\n",
      "Epoch 73, Step 33 Loss = 0.554961085319519\n",
      "Epoch 73, Step 34 Loss = 0.5046038031578064\n",
      "Epoch 73, Step 35 Loss = 0.679924726486206\n",
      "Epoch 73, Step 36 Loss = 0.9208272695541382\n",
      "Epoch 73, Step 37 Loss = 0.8225269317626953\n",
      "Epoch 73, Step 38 Loss = 0.8869706988334656\n",
      "Epoch 73, Step 39 Loss = 0.8719997406005859\n",
      "Epoch 73, Step 40 Loss = 0.7918930649757385\n",
      "Epoch 73, Step 41 Loss = 0.7541393637657166\n",
      "Training loss: 0.6092\n",
      "Epoch 74/500\n",
      "Epoch 74, Step 1 Loss = 0.6619304418563843\n",
      "Epoch 74, Step 2 Loss = 0.305724173784256\n",
      "Epoch 74, Step 3 Loss = 0.7044346928596497\n",
      "Epoch 74, Step 4 Loss = 0.44581449031829834\n",
      "Epoch 74, Step 5 Loss = 0.692282497882843\n",
      "Epoch 74, Step 6 Loss = 0.585419774055481\n",
      "Epoch 74, Step 7 Loss = 0.6616759300231934\n",
      "Epoch 74, Step 8 Loss = 0.6262757778167725\n",
      "Epoch 74, Step 9 Loss = 0.2525531053543091\n",
      "Epoch 74, Step 10 Loss = 0.5783632397651672\n",
      "Epoch 74, Step 11 Loss = 0.5206953883171082\n",
      "Epoch 74, Step 12 Loss = 0.6170588731765747\n",
      "Epoch 74, Step 13 Loss = 0.3963455855846405\n",
      "Epoch 74, Step 14 Loss = 0.5218517780303955\n",
      "Epoch 74, Step 15 Loss = 0.7871118783950806\n",
      "Epoch 74, Step 16 Loss = 0.5572382211685181\n",
      "Epoch 74, Step 17 Loss = 0.7533082962036133\n",
      "Epoch 74, Step 18 Loss = 0.18069657683372498\n",
      "Epoch 74, Step 19 Loss = 0.48747581243515015\n",
      "Epoch 74, Step 20 Loss = 0.6666936874389648\n",
      "Epoch 74, Step 21 Loss = 0.4282078146934509\n",
      "Epoch 74, Step 22 Loss = 0.7121245861053467\n",
      "Epoch 74, Step 23 Loss = 0.5385685563087463\n",
      "Epoch 74, Step 24 Loss = 0.5568130612373352\n",
      "Epoch 74, Step 25 Loss = 0.8146451115608215\n",
      "Epoch 74, Step 26 Loss = 0.8105039596557617\n",
      "Epoch 74, Step 27 Loss = 0.26952069997787476\n",
      "Epoch 74, Step 28 Loss = 0.4178474545478821\n",
      "Epoch 74, Step 29 Loss = 0.6410656571388245\n",
      "Epoch 74, Step 30 Loss = 0.5332566499710083\n",
      "Epoch 74, Step 31 Loss = 0.9379041194915771\n",
      "Epoch 74, Step 32 Loss = 0.9796069860458374\n",
      "Epoch 74, Step 33 Loss = 0.5981119871139526\n",
      "Epoch 74, Step 34 Loss = 0.4791027903556824\n",
      "Epoch 74, Step 35 Loss = 0.8767270445823669\n",
      "Epoch 74, Step 36 Loss = 0.8594369888305664\n",
      "Epoch 74, Step 37 Loss = 0.5359380841255188\n",
      "Epoch 74, Step 38 Loss = 0.6806332468986511\n",
      "Epoch 74, Step 39 Loss = 0.5889593362808228\n",
      "Epoch 74, Step 40 Loss = 0.5242998600006104\n",
      "Epoch 74, Step 41 Loss = 0.9576601386070251\n",
      "Training loss: 0.6035\n",
      "Epoch 75/500\n",
      "Epoch 75, Step 1 Loss = 0.25994637608528137\n",
      "Epoch 75, Step 2 Loss = 0.3896503150463104\n",
      "Epoch 75, Step 3 Loss = 0.3754070997238159\n",
      "Epoch 75, Step 4 Loss = 0.6121211051940918\n",
      "Epoch 75, Step 5 Loss = 0.5437734127044678\n",
      "Epoch 75, Step 6 Loss = 0.8408834338188171\n",
      "Epoch 75, Step 7 Loss = 0.7557288408279419\n",
      "Epoch 75, Step 8 Loss = 0.7443070411682129\n",
      "Epoch 75, Step 9 Loss = 0.5451102256774902\n",
      "Epoch 75, Step 10 Loss = 0.5393849611282349\n",
      "Epoch 75, Step 11 Loss = 0.4181934595108032\n",
      "Epoch 75, Step 12 Loss = 0.6660069227218628\n",
      "Epoch 75, Step 13 Loss = 0.8334745168685913\n",
      "Epoch 75, Step 14 Loss = 0.694839358329773\n",
      "Epoch 75, Step 15 Loss = 0.8248071074485779\n",
      "Epoch 75, Step 16 Loss = 0.5703491568565369\n",
      "Epoch 75, Step 17 Loss = 0.3577008843421936\n",
      "Epoch 75, Step 18 Loss = 0.6636831164360046\n",
      "Epoch 75, Step 19 Loss = 0.4585886299610138\n",
      "Epoch 75, Step 20 Loss = 0.6004534959793091\n",
      "Epoch 75, Step 21 Loss = 0.5800975561141968\n",
      "Epoch 75, Step 22 Loss = 0.4918258786201477\n",
      "Epoch 75, Step 23 Loss = 0.6873224973678589\n",
      "Epoch 75, Step 24 Loss = 0.5244548320770264\n",
      "Epoch 75, Step 25 Loss = 0.5945621728897095\n",
      "Epoch 75, Step 26 Loss = 0.30098026990890503\n",
      "Epoch 75, Step 27 Loss = 0.604449987411499\n",
      "Epoch 75, Step 28 Loss = 0.7108808755874634\n",
      "Epoch 75, Step 29 Loss = 0.5870295763015747\n",
      "Epoch 75, Step 30 Loss = 0.6147369742393494\n",
      "Epoch 75, Step 31 Loss = 0.5135494470596313\n",
      "Epoch 75, Step 32 Loss = 0.5280264616012573\n",
      "Epoch 75, Step 33 Loss = 0.5540065765380859\n",
      "Epoch 75, Step 34 Loss = 0.6796820163726807\n",
      "Epoch 75, Step 35 Loss = 0.3660919666290283\n",
      "Epoch 75, Step 36 Loss = 0.6928057670593262\n",
      "Epoch 75, Step 37 Loss = 0.691449761390686\n",
      "Epoch 75, Step 38 Loss = 0.9365414977073669\n",
      "Epoch 75, Step 39 Loss = 0.6842318773269653\n",
      "Epoch 75, Step 40 Loss = 0.5889471769332886\n",
      "Epoch 75, Step 41 Loss = 1.1318690776824951\n",
      "Training loss: 0.6039\n",
      "Epoch 76/500\n",
      "Epoch 76, Step 1 Loss = 0.3241313099861145\n",
      "Epoch 76, Step 2 Loss = 0.22593161463737488\n",
      "Epoch 76, Step 3 Loss = 0.749198317527771\n",
      "Epoch 76, Step 4 Loss = 0.6457093954086304\n",
      "Epoch 76, Step 5 Loss = 0.6423987150192261\n",
      "Epoch 76, Step 6 Loss = 0.8257428407669067\n",
      "Epoch 76, Step 7 Loss = 0.3980880379676819\n",
      "Epoch 76, Step 8 Loss = 0.7038142085075378\n",
      "Epoch 76, Step 9 Loss = 0.363537073135376\n",
      "Epoch 76, Step 10 Loss = 0.5364397764205933\n",
      "Epoch 76, Step 11 Loss = 0.5914649963378906\n",
      "Epoch 76, Step 12 Loss = 0.5684723854064941\n",
      "Epoch 76, Step 13 Loss = 0.6475445032119751\n",
      "Epoch 76, Step 14 Loss = 0.6346196532249451\n",
      "Epoch 76, Step 15 Loss = 0.725295901298523\n",
      "Epoch 76, Step 16 Loss = 0.25857415795326233\n",
      "Epoch 76, Step 17 Loss = 0.6022183299064636\n",
      "Epoch 76, Step 18 Loss = 0.573642373085022\n",
      "Epoch 76, Step 19 Loss = 0.6821917295455933\n",
      "Epoch 76, Step 20 Loss = 0.4037644863128662\n",
      "Epoch 76, Step 21 Loss = 0.7009783983230591\n",
      "Epoch 76, Step 22 Loss = 0.7773211598396301\n",
      "Epoch 76, Step 23 Loss = 0.6864448189735413\n",
      "Epoch 76, Step 24 Loss = 0.39274540543556213\n",
      "Epoch 76, Step 25 Loss = 0.7148447036743164\n",
      "Epoch 76, Step 26 Loss = 0.6150360107421875\n",
      "Epoch 76, Step 27 Loss = 0.6630674004554749\n",
      "Epoch 76, Step 28 Loss = 0.8747043609619141\n",
      "Epoch 76, Step 29 Loss = 0.9981082677841187\n",
      "Epoch 76, Step 30 Loss = 0.5133090019226074\n",
      "Epoch 76, Step 31 Loss = 0.5856317281723022\n",
      "Epoch 76, Step 32 Loss = 0.3418106734752655\n",
      "Epoch 76, Step 33 Loss = 0.6594234108924866\n",
      "Epoch 76, Step 34 Loss = 0.6714239120483398\n",
      "Epoch 76, Step 35 Loss = 0.4101666212081909\n",
      "Epoch 76, Step 36 Loss = 0.6387514472007751\n",
      "Epoch 76, Step 37 Loss = 0.36190858483314514\n",
      "Epoch 76, Step 38 Loss = 0.7217082381248474\n",
      "Epoch 76, Step 39 Loss = 0.6698046326637268\n",
      "Epoch 76, Step 40 Loss = 0.6366083025932312\n",
      "Epoch 76, Step 41 Loss = 0.5356432199478149\n",
      "Training loss: 0.5920\n",
      "Epoch 77/500\n",
      "Epoch 77, Step 1 Loss = 0.47517240047454834\n",
      "Epoch 77, Step 2 Loss = 0.6530381441116333\n",
      "Epoch 77, Step 3 Loss = 0.488892525434494\n",
      "Epoch 77, Step 4 Loss = 0.6243947744369507\n",
      "Epoch 77, Step 5 Loss = 0.5888638496398926\n",
      "Epoch 77, Step 6 Loss = 0.6624477505683899\n",
      "Epoch 77, Step 7 Loss = 0.6505343914031982\n",
      "Epoch 77, Step 8 Loss = 0.2556159198284149\n",
      "Epoch 77, Step 9 Loss = 0.694445013999939\n",
      "Epoch 77, Step 10 Loss = 0.4314592182636261\n",
      "Epoch 77, Step 11 Loss = 0.6445872783660889\n",
      "Epoch 77, Step 12 Loss = 0.5527226328849792\n",
      "Epoch 77, Step 13 Loss = 0.2826598286628723\n",
      "Epoch 77, Step 14 Loss = 0.3911789655685425\n",
      "Epoch 77, Step 15 Loss = 0.5747171640396118\n",
      "Epoch 77, Step 16 Loss = 0.5525704622268677\n",
      "Epoch 77, Step 17 Loss = 0.3482854962348938\n",
      "Epoch 77, Step 18 Loss = 0.7201143503189087\n",
      "Epoch 77, Step 19 Loss = 0.4670100808143616\n",
      "Epoch 77, Step 20 Loss = 0.21718576550483704\n",
      "Epoch 77, Step 21 Loss = 0.727279543876648\n",
      "Epoch 77, Step 22 Loss = 0.2280965894460678\n",
      "Epoch 77, Step 23 Loss = 0.5637903809547424\n",
      "Epoch 77, Step 24 Loss = 0.6112195253372192\n",
      "Epoch 77, Step 25 Loss = 0.8866972923278809\n",
      "Epoch 77, Step 26 Loss = 0.3936763107776642\n",
      "Epoch 77, Step 27 Loss = 0.9090371131896973\n",
      "Epoch 77, Step 28 Loss = 1.1128480434417725\n",
      "Epoch 77, Step 29 Loss = 0.8845043778419495\n",
      "Epoch 77, Step 30 Loss = 1.160355567932129\n",
      "Epoch 77, Step 31 Loss = 0.5282210111618042\n",
      "Epoch 77, Step 32 Loss = 0.8831856846809387\n",
      "Epoch 77, Step 33 Loss = 0.797836184501648\n",
      "Epoch 77, Step 34 Loss = 0.856947660446167\n",
      "Epoch 77, Step 35 Loss = 0.5197574496269226\n",
      "Epoch 77, Step 36 Loss = 0.5573825836181641\n",
      "Epoch 77, Step 37 Loss = 0.61203932762146\n",
      "Epoch 77, Step 38 Loss = 0.6972115635871887\n",
      "Epoch 77, Step 39 Loss = 0.3983779847621918\n",
      "Epoch 77, Step 40 Loss = 0.49561187624931335\n",
      "Epoch 77, Step 41 Loss = 0.9724301695823669\n",
      "Training loss: 0.6115\n",
      "Epoch 78/500\n",
      "Epoch 78, Step 1 Loss = 0.43963584303855896\n",
      "Epoch 78, Step 2 Loss = 0.4592585861682892\n",
      "Epoch 78, Step 3 Loss = 0.5817057490348816\n",
      "Epoch 78, Step 4 Loss = 0.41858023405075073\n",
      "Epoch 78, Step 5 Loss = 0.42997586727142334\n",
      "Epoch 78, Step 6 Loss = 0.4412223994731903\n",
      "Epoch 78, Step 7 Loss = 0.6027467250823975\n",
      "Epoch 78, Step 8 Loss = 0.6252074837684631\n",
      "Epoch 78, Step 9 Loss = 0.34958598017692566\n",
      "Epoch 78, Step 10 Loss = 0.6025031208992004\n",
      "Epoch 78, Step 11 Loss = 0.5035362839698792\n",
      "Epoch 78, Step 12 Loss = 0.6229159832000732\n",
      "Epoch 78, Step 13 Loss = 1.0154281854629517\n",
      "Epoch 78, Step 14 Loss = 0.560800313949585\n",
      "Epoch 78, Step 15 Loss = 0.7917829751968384\n",
      "Epoch 78, Step 16 Loss = 0.36550095677375793\n",
      "Epoch 78, Step 17 Loss = 0.5198536515235901\n",
      "Epoch 78, Step 18 Loss = 0.6509934067726135\n",
      "Epoch 78, Step 19 Loss = 0.4379212260246277\n",
      "Epoch 78, Step 20 Loss = 0.40295520424842834\n",
      "Epoch 78, Step 21 Loss = 0.647325336933136\n",
      "Epoch 78, Step 22 Loss = 0.5863789319992065\n",
      "Epoch 78, Step 23 Loss = 0.5984053611755371\n",
      "Epoch 78, Step 24 Loss = 0.7215762138366699\n",
      "Epoch 78, Step 25 Loss = 0.5016171932220459\n",
      "Epoch 78, Step 26 Loss = 0.5091739892959595\n",
      "Epoch 78, Step 27 Loss = 0.6106317043304443\n",
      "Epoch 78, Step 28 Loss = 0.7405478358268738\n",
      "Epoch 78, Step 29 Loss = 0.4467812776565552\n",
      "Epoch 78, Step 30 Loss = 0.8517420291900635\n",
      "Epoch 78, Step 31 Loss = 0.9648938179016113\n",
      "Epoch 78, Step 32 Loss = 0.5171959400177002\n",
      "Epoch 78, Step 33 Loss = 0.5993022918701172\n",
      "Epoch 78, Step 34 Loss = 0.5003601312637329\n",
      "Epoch 78, Step 35 Loss = 0.7121838927268982\n",
      "Epoch 78, Step 36 Loss = 0.85929936170578\n",
      "Epoch 78, Step 37 Loss = 1.0317727327346802\n",
      "Epoch 78, Step 38 Loss = 0.8225408792495728\n",
      "Epoch 78, Step 39 Loss = 0.20888939499855042\n",
      "Epoch 78, Step 40 Loss = 0.4738042950630188\n",
      "Epoch 78, Step 41 Loss = 0.45666542649269104\n",
      "Training loss: 0.5898\n",
      "Epoch 79/500\n",
      "Epoch 79, Step 1 Loss = 0.5379304885864258\n",
      "Epoch 79, Step 2 Loss = 0.6313412189483643\n",
      "Epoch 79, Step 3 Loss = 0.35273247957229614\n",
      "Epoch 79, Step 4 Loss = 0.36550772190093994\n",
      "Epoch 79, Step 5 Loss = 0.7794700860977173\n",
      "Epoch 79, Step 6 Loss = 0.5907748937606812\n",
      "Epoch 79, Step 7 Loss = 0.5257319808006287\n",
      "Epoch 79, Step 8 Loss = 0.49593716859817505\n",
      "Epoch 79, Step 9 Loss = 0.49422889947891235\n",
      "Epoch 79, Step 10 Loss = 0.7209784984588623\n",
      "Epoch 79, Step 11 Loss = 0.7592155933380127\n",
      "Epoch 79, Step 12 Loss = 0.5955945253372192\n",
      "Epoch 79, Step 13 Loss = 0.581215500831604\n",
      "Epoch 79, Step 14 Loss = 0.44164708256721497\n",
      "Epoch 79, Step 15 Loss = 0.4874556064605713\n",
      "Epoch 79, Step 16 Loss = 0.5172299146652222\n",
      "Epoch 79, Step 17 Loss = 0.16023121774196625\n",
      "Epoch 79, Step 18 Loss = 0.6267416477203369\n",
      "Epoch 79, Step 19 Loss = 0.28610873222351074\n",
      "Epoch 79, Step 20 Loss = 0.48459625244140625\n",
      "Epoch 79, Step 21 Loss = 0.6919757127761841\n",
      "Epoch 79, Step 22 Loss = 0.6070479154586792\n",
      "Epoch 79, Step 23 Loss = 0.5537596940994263\n",
      "Epoch 79, Step 24 Loss = 0.6310527920722961\n",
      "Epoch 79, Step 25 Loss = 0.33152514696121216\n",
      "Epoch 79, Step 26 Loss = 0.5098164677619934\n",
      "Epoch 79, Step 27 Loss = 0.3751797676086426\n",
      "Epoch 79, Step 28 Loss = 0.7568831443786621\n",
      "Epoch 79, Step 29 Loss = 0.5576431751251221\n",
      "Epoch 79, Step 30 Loss = 0.6300444602966309\n",
      "Epoch 79, Step 31 Loss = 0.6534292101860046\n",
      "Epoch 79, Step 32 Loss = 0.7569761276245117\n",
      "Epoch 79, Step 33 Loss = 0.5707473754882812\n",
      "Epoch 79, Step 34 Loss = 0.6248229742050171\n",
      "Epoch 79, Step 35 Loss = 0.6536710262298584\n",
      "Epoch 79, Step 36 Loss = 0.6993967890739441\n",
      "Epoch 79, Step 37 Loss = 0.8804932832717896\n",
      "Epoch 79, Step 38 Loss = 0.7933273911476135\n",
      "Epoch 79, Step 39 Loss = 0.6163730621337891\n",
      "Epoch 79, Step 40 Loss = 0.8597462177276611\n",
      "Epoch 79, Step 41 Loss = 1.0927114486694336\n",
      "Training loss: 0.5922\n",
      "Epoch 80/500\n",
      "Epoch 80, Step 1 Loss = 0.6102241277694702\n",
      "Epoch 80, Step 2 Loss = 0.23610633611679077\n",
      "Epoch 80, Step 3 Loss = 0.5453566312789917\n",
      "Epoch 80, Step 4 Loss = 0.7911902666091919\n",
      "Epoch 80, Step 5 Loss = 0.35521024465560913\n",
      "Epoch 80, Step 6 Loss = 0.5629952549934387\n",
      "Epoch 80, Step 7 Loss = 0.22280248999595642\n",
      "Epoch 80, Step 8 Loss = 0.7196811437606812\n",
      "Epoch 80, Step 9 Loss = 0.6024031639099121\n",
      "Epoch 80, Step 10 Loss = 0.5138585567474365\n",
      "Epoch 80, Step 11 Loss = 0.6282955408096313\n",
      "Epoch 80, Step 12 Loss = 0.3888865113258362\n",
      "Epoch 80, Step 13 Loss = 0.34720557928085327\n",
      "Epoch 80, Step 14 Loss = 0.5608766078948975\n",
      "Epoch 80, Step 15 Loss = 0.5591973662376404\n",
      "Epoch 80, Step 16 Loss = 0.5838567018508911\n",
      "Epoch 80, Step 17 Loss = 0.5184858441352844\n",
      "Epoch 80, Step 18 Loss = 0.6306743025779724\n",
      "Epoch 80, Step 19 Loss = 0.5745037794113159\n",
      "Epoch 80, Step 20 Loss = 0.7964504957199097\n",
      "Epoch 80, Step 21 Loss = 0.6311911344528198\n",
      "Epoch 80, Step 22 Loss = 0.5889607071876526\n",
      "Epoch 80, Step 23 Loss = 0.45432624220848083\n",
      "Epoch 80, Step 24 Loss = 0.7791208028793335\n",
      "Epoch 80, Step 25 Loss = 0.5641573667526245\n",
      "Epoch 80, Step 26 Loss = 0.43130502104759216\n",
      "Epoch 80, Step 27 Loss = 0.5975606441497803\n",
      "Epoch 80, Step 28 Loss = 0.4125971794128418\n",
      "Epoch 80, Step 29 Loss = 0.6058743000030518\n",
      "Epoch 80, Step 30 Loss = 0.46394258737564087\n",
      "Epoch 80, Step 31 Loss = 0.515895664691925\n",
      "Epoch 80, Step 32 Loss = 0.9926846623420715\n",
      "Epoch 80, Step 33 Loss = 0.7492573261260986\n",
      "Epoch 80, Step 34 Loss = 0.23271146416664124\n",
      "Epoch 80, Step 35 Loss = 0.44079920649528503\n",
      "Epoch 80, Step 36 Loss = 0.6170308589935303\n",
      "Epoch 80, Step 37 Loss = 0.9091747999191284\n",
      "Epoch 80, Step 38 Loss = 1.296330213546753\n",
      "Epoch 80, Step 39 Loss = 0.7288689613342285\n",
      "Epoch 80, Step 40 Loss = 0.9187554717063904\n",
      "Epoch 80, Step 41 Loss = 0.6006083488464355\n",
      "Training loss: 0.5922\n",
      "Epoch 81/500\n",
      "Epoch 81, Step 1 Loss = 0.63677978515625\n",
      "Epoch 81, Step 2 Loss = 0.4658178389072418\n",
      "Epoch 81, Step 3 Loss = 0.6122196912765503\n",
      "Epoch 81, Step 4 Loss = 0.3998284935951233\n",
      "Epoch 81, Step 5 Loss = 0.5931282639503479\n",
      "Epoch 81, Step 6 Loss = 0.6068716049194336\n",
      "Epoch 81, Step 7 Loss = 0.5187587738037109\n",
      "Epoch 81, Step 8 Loss = 0.2515755295753479\n",
      "Epoch 81, Step 9 Loss = 0.38784706592559814\n",
      "Epoch 81, Step 10 Loss = 0.3847815990447998\n",
      "Epoch 81, Step 11 Loss = 0.8286799788475037\n",
      "Epoch 81, Step 12 Loss = 0.3875613808631897\n",
      "Epoch 81, Step 13 Loss = 0.4988744854927063\n",
      "Epoch 81, Step 14 Loss = 0.30151909589767456\n",
      "Epoch 81, Step 15 Loss = 0.6036773920059204\n",
      "Epoch 81, Step 16 Loss = 0.40432536602020264\n",
      "Epoch 81, Step 17 Loss = 0.8132824897766113\n",
      "Epoch 81, Step 18 Loss = 0.5460374355316162\n",
      "Epoch 81, Step 19 Loss = 0.6430591344833374\n",
      "Epoch 81, Step 20 Loss = 0.9905228614807129\n",
      "Epoch 81, Step 21 Loss = 1.0369606018066406\n",
      "Epoch 81, Step 22 Loss = 0.7063406705856323\n",
      "Epoch 81, Step 23 Loss = 0.4853985011577606\n",
      "Epoch 81, Step 24 Loss = 0.6014528274536133\n",
      "Epoch 81, Step 25 Loss = 0.505516529083252\n",
      "Epoch 81, Step 26 Loss = 0.794011116027832\n",
      "Epoch 81, Step 27 Loss = 0.7835123538970947\n",
      "Epoch 81, Step 28 Loss = 0.6228141188621521\n",
      "Epoch 81, Step 29 Loss = 0.6817708611488342\n",
      "Epoch 81, Step 30 Loss = 0.9846767783164978\n",
      "Epoch 81, Step 31 Loss = 0.40672987699508667\n",
      "Epoch 81, Step 32 Loss = 0.45135435461997986\n",
      "Epoch 81, Step 33 Loss = 0.5913065671920776\n",
      "Epoch 81, Step 34 Loss = 0.6260986924171448\n",
      "Epoch 81, Step 35 Loss = 1.0509376525878906\n",
      "Epoch 81, Step 36 Loss = 0.6378605961799622\n",
      "Epoch 81, Step 37 Loss = 0.6103602647781372\n",
      "Epoch 81, Step 38 Loss = 0.4810982644557953\n",
      "Epoch 81, Step 39 Loss = 0.34355470538139343\n",
      "Epoch 81, Step 40 Loss = 0.5314090847969055\n",
      "Epoch 81, Step 41 Loss = 1.0567107200622559\n",
      "Training loss: 0.6065\n",
      "Epoch 82/500\n",
      "Epoch 82, Step 1 Loss = 0.5759252309799194\n",
      "Epoch 82, Step 2 Loss = 0.3498237133026123\n",
      "Epoch 82, Step 3 Loss = 0.6200118064880371\n",
      "Epoch 82, Step 4 Loss = 0.622097909450531\n",
      "Epoch 82, Step 5 Loss = 0.47117406129837036\n",
      "Epoch 82, Step 6 Loss = 0.3769557476043701\n",
      "Epoch 82, Step 7 Loss = 0.4591779112815857\n",
      "Epoch 82, Step 8 Loss = 0.528039813041687\n",
      "Epoch 82, Step 9 Loss = 0.5604611039161682\n",
      "Epoch 82, Step 10 Loss = 0.6319162845611572\n",
      "Epoch 82, Step 11 Loss = 0.5593542456626892\n",
      "Epoch 82, Step 12 Loss = 0.46278324723243713\n",
      "Epoch 82, Step 13 Loss = 0.3437584638595581\n",
      "Epoch 82, Step 14 Loss = 0.6828345656394958\n",
      "Epoch 82, Step 15 Loss = 0.4662000834941864\n",
      "Epoch 82, Step 16 Loss = 0.4410609304904938\n",
      "Epoch 82, Step 17 Loss = 0.29637056589126587\n",
      "Epoch 82, Step 18 Loss = 0.3516743779182434\n",
      "Epoch 82, Step 19 Loss = 0.4240656793117523\n",
      "Epoch 82, Step 20 Loss = 0.6805696487426758\n",
      "Epoch 82, Step 21 Loss = 0.6161907315254211\n",
      "Epoch 82, Step 22 Loss = 0.7411080598831177\n",
      "Epoch 82, Step 23 Loss = 0.6721761226654053\n",
      "Epoch 82, Step 24 Loss = 0.5429542660713196\n",
      "Epoch 82, Step 25 Loss = 0.3540170192718506\n",
      "Epoch 82, Step 26 Loss = 0.7435060739517212\n",
      "Epoch 82, Step 27 Loss = 0.747428834438324\n",
      "Epoch 82, Step 28 Loss = 0.4597679376602173\n",
      "Epoch 82, Step 29 Loss = 0.620768666267395\n",
      "Epoch 82, Step 30 Loss = 0.6027258634567261\n",
      "Epoch 82, Step 31 Loss = 0.7694474458694458\n",
      "Epoch 82, Step 32 Loss = 0.5890824794769287\n",
      "Epoch 82, Step 33 Loss = 1.0524096488952637\n",
      "Epoch 82, Step 34 Loss = 0.6334726810455322\n",
      "Epoch 82, Step 35 Loss = 0.8429367542266846\n",
      "Epoch 82, Step 36 Loss = 0.8971660137176514\n",
      "Epoch 82, Step 37 Loss = 0.31465381383895874\n",
      "Epoch 82, Step 38 Loss = 0.4275340139865875\n",
      "Epoch 82, Step 39 Loss = 0.8188776969909668\n",
      "Epoch 82, Step 40 Loss = 0.7351616621017456\n",
      "Epoch 82, Step 41 Loss = 0.8713503479957581\n",
      "Training loss: 0.5843\n",
      "Epoch 83/500\n",
      "Epoch 83, Step 1 Loss = 0.5821336507797241\n",
      "Epoch 83, Step 2 Loss = 0.43956929445266724\n",
      "Epoch 83, Step 3 Loss = 0.5110116600990295\n",
      "Epoch 83, Step 4 Loss = 0.4458509683609009\n",
      "Epoch 83, Step 5 Loss = 0.3250144422054291\n",
      "Epoch 83, Step 6 Loss = 0.5758224129676819\n",
      "Epoch 83, Step 7 Loss = 0.5195007920265198\n",
      "Epoch 83, Step 8 Loss = 0.38839200139045715\n",
      "Epoch 83, Step 9 Loss = 0.4520692825317383\n",
      "Epoch 83, Step 10 Loss = 0.4601176381111145\n",
      "Epoch 83, Step 11 Loss = 0.6373928785324097\n",
      "Epoch 83, Step 12 Loss = 0.367573082447052\n",
      "Epoch 83, Step 13 Loss = 0.8640564680099487\n",
      "Epoch 83, Step 14 Loss = 0.42111051082611084\n",
      "Epoch 83, Step 15 Loss = 0.6104973554611206\n",
      "Epoch 83, Step 16 Loss = 0.35647422075271606\n",
      "Epoch 83, Step 17 Loss = 0.6446182727813721\n",
      "Epoch 83, Step 18 Loss = 0.6185588240623474\n",
      "Epoch 83, Step 19 Loss = 0.6884693503379822\n",
      "Epoch 83, Step 20 Loss = 0.7002039551734924\n",
      "Epoch 83, Step 21 Loss = 0.37808847427368164\n",
      "Epoch 83, Step 22 Loss = 0.5057440996170044\n",
      "Epoch 83, Step 23 Loss = 0.7065355181694031\n",
      "Epoch 83, Step 24 Loss = 0.6133444309234619\n",
      "Epoch 83, Step 25 Loss = 0.6296484470367432\n",
      "Epoch 83, Step 26 Loss = 0.33049067854881287\n",
      "Epoch 83, Step 27 Loss = 0.7948899269104004\n",
      "Epoch 83, Step 28 Loss = 0.8608043193817139\n",
      "Epoch 83, Step 29 Loss = 0.8838191628456116\n",
      "Epoch 83, Step 30 Loss = 0.9440294504165649\n",
      "Epoch 83, Step 31 Loss = 0.9659909009933472\n",
      "Epoch 83, Step 32 Loss = 0.6843247413635254\n",
      "Epoch 83, Step 33 Loss = 0.4871968626976013\n",
      "Epoch 83, Step 34 Loss = 0.7938647866249084\n",
      "Epoch 83, Step 35 Loss = 0.9487748146057129\n",
      "Epoch 83, Step 36 Loss = 0.476941853761673\n",
      "Epoch 83, Step 37 Loss = 0.5153388977050781\n",
      "Epoch 83, Step 38 Loss = 0.7770461440086365\n",
      "Epoch 83, Step 39 Loss = 0.9203569889068604\n",
      "Epoch 83, Step 40 Loss = 0.6110445261001587\n",
      "Epoch 83, Step 41 Loss = 0.47394225001335144\n",
      "Training loss: 0.6076\n",
      "Epoch 84/500\n",
      "Epoch 84, Step 1 Loss = 0.8560488224029541\n",
      "Epoch 84, Step 2 Loss = 0.38925692439079285\n",
      "Epoch 84, Step 3 Loss = 0.5859415531158447\n",
      "Epoch 84, Step 4 Loss = 0.6524495482444763\n",
      "Epoch 84, Step 5 Loss = 0.1602276712656021\n",
      "Epoch 84, Step 6 Loss = 0.44513779878616333\n",
      "Epoch 84, Step 7 Loss = 0.5177227258682251\n",
      "Epoch 84, Step 8 Loss = 0.7142367362976074\n",
      "Epoch 84, Step 9 Loss = 0.5945464968681335\n",
      "Epoch 84, Step 10 Loss = 0.2226903736591339\n",
      "Epoch 84, Step 11 Loss = 0.5782766342163086\n",
      "Epoch 84, Step 12 Loss = 0.6404663920402527\n",
      "Epoch 84, Step 13 Loss = 0.4669496715068817\n",
      "Epoch 84, Step 14 Loss = 0.5506783127784729\n",
      "Epoch 84, Step 15 Loss = 0.1742624044418335\n",
      "Epoch 84, Step 16 Loss = 0.6677672266960144\n",
      "Epoch 84, Step 17 Loss = 0.43812304735183716\n",
      "Epoch 84, Step 18 Loss = 0.7915524244308472\n",
      "Epoch 84, Step 19 Loss = 0.4775265157222748\n",
      "Epoch 84, Step 20 Loss = 0.25401735305786133\n",
      "Epoch 84, Step 21 Loss = 0.5394461750984192\n",
      "Epoch 84, Step 22 Loss = 0.7452805042266846\n",
      "Epoch 84, Step 23 Loss = 0.6982138156890869\n",
      "Epoch 84, Step 24 Loss = 0.37806078791618347\n",
      "Epoch 84, Step 25 Loss = 1.0233020782470703\n",
      "Epoch 84, Step 26 Loss = 0.44328710436820984\n",
      "Epoch 84, Step 27 Loss = 0.4325341284275055\n",
      "Epoch 84, Step 28 Loss = 0.707633376121521\n",
      "Epoch 84, Step 29 Loss = 0.43401432037353516\n",
      "Epoch 84, Step 30 Loss = 0.7514866590499878\n",
      "Epoch 84, Step 31 Loss = 0.7395786046981812\n",
      "Epoch 84, Step 32 Loss = 0.7413066029548645\n",
      "Epoch 84, Step 33 Loss = 0.5679466724395752\n",
      "Epoch 84, Step 34 Loss = 0.8460811376571655\n",
      "Epoch 84, Step 35 Loss = 0.867885947227478\n",
      "Epoch 84, Step 36 Loss = 0.6406589150428772\n",
      "Epoch 84, Step 37 Loss = 1.054850697517395\n",
      "Epoch 84, Step 38 Loss = 0.6895902752876282\n",
      "Epoch 84, Step 39 Loss = 0.4657825231552124\n",
      "Epoch 84, Step 40 Loss = 0.7805135250091553\n",
      "Epoch 84, Step 41 Loss = 1.004556655883789\n",
      "Training loss: 0.6032\n",
      "Epoch 85/500\n",
      "Epoch 85, Step 1 Loss = 0.4114486277103424\n",
      "Epoch 85, Step 2 Loss = 0.4557648301124573\n",
      "Epoch 85, Step 3 Loss = 0.4993250370025635\n",
      "Epoch 85, Step 4 Loss = 0.3803156018257141\n",
      "Epoch 85, Step 5 Loss = 0.6069797277450562\n",
      "Epoch 85, Step 6 Loss = 0.46216636896133423\n",
      "Epoch 85, Step 7 Loss = 0.7012158632278442\n",
      "Epoch 85, Step 8 Loss = 0.4667293429374695\n",
      "Epoch 85, Step 9 Loss = 0.7579481601715088\n",
      "Epoch 85, Step 10 Loss = 0.39077121019363403\n",
      "Epoch 85, Step 11 Loss = 0.6105297803878784\n",
      "Epoch 85, Step 12 Loss = 0.7446456551551819\n",
      "Epoch 85, Step 13 Loss = 0.6510627865791321\n",
      "Epoch 85, Step 14 Loss = 0.41394394636154175\n",
      "Epoch 85, Step 15 Loss = 0.31409841775894165\n",
      "Epoch 85, Step 16 Loss = 0.6019538044929504\n",
      "Epoch 85, Step 17 Loss = 0.5852123498916626\n",
      "Epoch 85, Step 18 Loss = 0.5932377576828003\n",
      "Epoch 85, Step 19 Loss = 0.5629227161407471\n",
      "Epoch 85, Step 20 Loss = 0.4749501347541809\n",
      "Epoch 85, Step 21 Loss = 0.4826837182044983\n",
      "Epoch 85, Step 22 Loss = 0.4324786961078644\n",
      "Epoch 85, Step 23 Loss = 0.4611361026763916\n",
      "Epoch 85, Step 24 Loss = 0.6498669981956482\n",
      "Epoch 85, Step 25 Loss = 0.3899758458137512\n",
      "Epoch 85, Step 26 Loss = 0.7604665756225586\n",
      "Epoch 85, Step 27 Loss = 0.3449117839336395\n",
      "Epoch 85, Step 28 Loss = 0.4247220456600189\n",
      "Epoch 85, Step 29 Loss = 0.5916892886161804\n",
      "Epoch 85, Step 30 Loss = 0.7278838157653809\n",
      "Epoch 85, Step 31 Loss = 0.9713547229766846\n",
      "Epoch 85, Step 32 Loss = 0.6322470903396606\n",
      "Epoch 85, Step 33 Loss = 1.0691193342208862\n",
      "Epoch 85, Step 34 Loss = 0.5342910289764404\n",
      "Epoch 85, Step 35 Loss = 0.6946042776107788\n",
      "Epoch 85, Step 36 Loss = 0.24544697999954224\n",
      "Epoch 85, Step 37 Loss = 0.7258294224739075\n",
      "Epoch 85, Step 38 Loss = 0.922083854675293\n",
      "Epoch 85, Step 39 Loss = 0.5966761112213135\n",
      "Epoch 85, Step 40 Loss = 0.4924374222755432\n",
      "Epoch 85, Step 41 Loss = 0.7406351566314697\n",
      "Training loss: 0.5750\n",
      "Epoch 86/500\n",
      "Epoch 86, Step 1 Loss = 0.1529417186975479\n",
      "Epoch 86, Step 2 Loss = 0.6620794534683228\n",
      "Epoch 86, Step 3 Loss = 0.6796537041664124\n",
      "Epoch 86, Step 4 Loss = 0.45556026697158813\n",
      "Epoch 86, Step 5 Loss = 0.42498406767845154\n",
      "Epoch 86, Step 6 Loss = 0.5765045881271362\n",
      "Epoch 86, Step 7 Loss = 0.8004269599914551\n",
      "Epoch 86, Step 8 Loss = 0.4869815707206726\n",
      "Epoch 86, Step 9 Loss = 0.41060054302215576\n",
      "Epoch 86, Step 10 Loss = 0.3871682286262512\n",
      "Epoch 86, Step 11 Loss = 0.47870808839797974\n",
      "Epoch 86, Step 12 Loss = 0.8006988167762756\n",
      "Epoch 86, Step 13 Loss = 0.32011571526527405\n",
      "Epoch 86, Step 14 Loss = 0.3801266551017761\n",
      "Epoch 86, Step 15 Loss = 0.518357515335083\n",
      "Epoch 86, Step 16 Loss = 0.6026103496551514\n",
      "Epoch 86, Step 17 Loss = 1.0483392477035522\n",
      "Epoch 86, Step 18 Loss = 0.6015692949295044\n",
      "Epoch 86, Step 19 Loss = 0.3796292543411255\n",
      "Epoch 86, Step 20 Loss = 0.6050623655319214\n",
      "Epoch 86, Step 21 Loss = 0.4334096908569336\n",
      "Epoch 86, Step 22 Loss = 0.6152338981628418\n",
      "Epoch 86, Step 23 Loss = 0.3748927712440491\n",
      "Epoch 86, Step 24 Loss = 0.23882116377353668\n",
      "Epoch 86, Step 25 Loss = 0.16413095593452454\n",
      "Epoch 86, Step 26 Loss = 0.7693189382553101\n",
      "Epoch 86, Step 27 Loss = 0.5152500867843628\n",
      "Epoch 86, Step 28 Loss = 1.0766515731811523\n",
      "Epoch 86, Step 29 Loss = 0.29909855127334595\n",
      "Epoch 86, Step 30 Loss = 0.6045651435852051\n",
      "Epoch 86, Step 31 Loss = 0.4824419617652893\n",
      "Epoch 86, Step 32 Loss = 0.8982229232788086\n",
      "Epoch 86, Step 33 Loss = 0.6384342312812805\n",
      "Epoch 86, Step 34 Loss = 0.754732608795166\n",
      "Epoch 86, Step 35 Loss = 0.9592065811157227\n",
      "Epoch 86, Step 36 Loss = 0.5416290760040283\n",
      "Epoch 86, Step 37 Loss = 0.875083863735199\n",
      "Epoch 86, Step 38 Loss = 0.8633793592453003\n",
      "Epoch 86, Step 39 Loss = 0.8347941637039185\n",
      "Epoch 86, Step 40 Loss = 0.9579902291297913\n",
      "Epoch 86, Step 41 Loss = 1.131112813949585\n",
      "Training loss: 0.6049\n",
      "Epoch 87/500\n",
      "Epoch 87, Step 1 Loss = 0.3801555633544922\n",
      "Epoch 87, Step 2 Loss = 0.3974859118461609\n",
      "Epoch 87, Step 3 Loss = 0.6478211879730225\n",
      "Epoch 87, Step 4 Loss = 0.42053601145744324\n",
      "Epoch 87, Step 5 Loss = 0.24981677532196045\n",
      "Epoch 87, Step 6 Loss = 0.7396440505981445\n",
      "Epoch 87, Step 7 Loss = 0.5741644501686096\n",
      "Epoch 87, Step 8 Loss = 0.6650402545928955\n",
      "Epoch 87, Step 9 Loss = 0.5154128074645996\n",
      "Epoch 87, Step 10 Loss = 0.5971747040748596\n",
      "Epoch 87, Step 11 Loss = 0.5821987390518188\n",
      "Epoch 87, Step 12 Loss = 0.7394862771034241\n",
      "Epoch 87, Step 13 Loss = 0.5851110219955444\n",
      "Epoch 87, Step 14 Loss = 0.26039403676986694\n",
      "Epoch 87, Step 15 Loss = 0.5760893821716309\n",
      "Epoch 87, Step 16 Loss = 0.35320812463760376\n",
      "Epoch 87, Step 17 Loss = 0.7421867847442627\n",
      "Epoch 87, Step 18 Loss = 0.4168013334274292\n",
      "Epoch 87, Step 19 Loss = 0.7091696262359619\n",
      "Epoch 87, Step 20 Loss = 0.6504720449447632\n",
      "Epoch 87, Step 21 Loss = 0.5847156047821045\n",
      "Epoch 87, Step 22 Loss = 0.5884665846824646\n",
      "Epoch 87, Step 23 Loss = 0.8174612522125244\n",
      "Epoch 87, Step 24 Loss = 0.48319193720817566\n",
      "Epoch 87, Step 25 Loss = 0.6222295165061951\n",
      "Epoch 87, Step 26 Loss = 0.4390478730201721\n",
      "Epoch 87, Step 27 Loss = 1.1347041130065918\n",
      "Epoch 87, Step 28 Loss = 0.7387104034423828\n",
      "Epoch 87, Step 29 Loss = 0.5833288431167603\n",
      "Epoch 87, Step 30 Loss = 0.6214635372161865\n",
      "Epoch 87, Step 31 Loss = 0.20567823946475983\n",
      "Epoch 87, Step 32 Loss = 0.7082784175872803\n",
      "Epoch 87, Step 33 Loss = 0.7608305215835571\n",
      "Epoch 87, Step 34 Loss = 0.5761340856552124\n",
      "Epoch 87, Step 35 Loss = 0.68300461769104\n",
      "Epoch 87, Step 36 Loss = 0.6290768384933472\n",
      "Epoch 87, Step 37 Loss = 0.8519145846366882\n",
      "Epoch 87, Step 38 Loss = 0.7822010517120361\n",
      "Epoch 87, Step 39 Loss = 0.3255316913127899\n",
      "Epoch 87, Step 40 Loss = 0.37737172842025757\n",
      "Epoch 87, Step 41 Loss = 0.7500118017196655\n",
      "Training loss: 0.5870\n",
      "Epoch 88/500\n",
      "Epoch 88, Step 1 Loss = 0.575339674949646\n",
      "Epoch 88, Step 2 Loss = 0.2982306480407715\n",
      "Epoch 88, Step 3 Loss = 0.3663039207458496\n",
      "Epoch 88, Step 4 Loss = 0.526477575302124\n",
      "Epoch 88, Step 5 Loss = 0.7192948460578918\n",
      "Epoch 88, Step 6 Loss = 0.35809651017189026\n",
      "Epoch 88, Step 7 Loss = 0.5875657796859741\n",
      "Epoch 88, Step 8 Loss = 0.38706067204475403\n",
      "Epoch 88, Step 9 Loss = 0.15540502965450287\n",
      "Epoch 88, Step 10 Loss = 0.5684400796890259\n",
      "Epoch 88, Step 11 Loss = 0.4938855767250061\n",
      "Epoch 88, Step 12 Loss = 0.8845714330673218\n",
      "Epoch 88, Step 13 Loss = 0.5868233442306519\n",
      "Epoch 88, Step 14 Loss = 0.5158312320709229\n",
      "Epoch 88, Step 15 Loss = 0.5090734958648682\n",
      "Epoch 88, Step 16 Loss = 0.6387397646903992\n",
      "Epoch 88, Step 17 Loss = 0.8576717376708984\n",
      "Epoch 88, Step 18 Loss = 0.6209125518798828\n",
      "Epoch 88, Step 19 Loss = 0.5082893371582031\n",
      "Epoch 88, Step 20 Loss = 0.5141232013702393\n",
      "Epoch 88, Step 21 Loss = 0.22238492965698242\n",
      "Epoch 88, Step 22 Loss = 0.6754491329193115\n",
      "Epoch 88, Step 23 Loss = 0.43276122212409973\n",
      "Epoch 88, Step 24 Loss = 0.6300601363182068\n",
      "Epoch 88, Step 25 Loss = 1.0479099750518799\n",
      "Epoch 88, Step 26 Loss = 0.5468512773513794\n",
      "Epoch 88, Step 27 Loss = 0.44415363669395447\n",
      "Epoch 88, Step 28 Loss = 0.7542285919189453\n",
      "Epoch 88, Step 29 Loss = 0.4703294336795807\n",
      "Epoch 88, Step 30 Loss = 0.49990540742874146\n",
      "Epoch 88, Step 31 Loss = 0.443899542093277\n",
      "Epoch 88, Step 32 Loss = 0.5255222320556641\n",
      "Epoch 88, Step 33 Loss = 0.9965749979019165\n",
      "Epoch 88, Step 34 Loss = 0.43868860602378845\n",
      "Epoch 88, Step 35 Loss = 0.5929960012435913\n",
      "Epoch 88, Step 36 Loss = 0.9238429069519043\n",
      "Epoch 88, Step 37 Loss = 0.5212427377700806\n",
      "Epoch 88, Step 38 Loss = 0.6751630306243896\n",
      "Epoch 88, Step 39 Loss = 0.7410266399383545\n",
      "Epoch 88, Step 40 Loss = 0.7760993242263794\n",
      "Epoch 88, Step 41 Loss = 0.982235312461853\n",
      "Training loss: 0.5857\n",
      "Epoch 89/500\n",
      "Epoch 89, Step 1 Loss = 0.3425552248954773\n",
      "Epoch 89, Step 2 Loss = 0.2871459126472473\n",
      "Epoch 89, Step 3 Loss = 0.5439636707305908\n",
      "Epoch 89, Step 4 Loss = 0.5683984756469727\n",
      "Epoch 89, Step 5 Loss = 0.5962816476821899\n",
      "Epoch 89, Step 6 Loss = 0.7616035342216492\n",
      "Epoch 89, Step 7 Loss = 0.3137888014316559\n",
      "Epoch 89, Step 8 Loss = 0.5545322895050049\n",
      "Epoch 89, Step 9 Loss = 0.6110019087791443\n",
      "Epoch 89, Step 10 Loss = 0.6371921300888062\n",
      "Epoch 89, Step 11 Loss = 0.27151572704315186\n",
      "Epoch 89, Step 12 Loss = 0.45272719860076904\n",
      "Epoch 89, Step 13 Loss = 0.41517066955566406\n",
      "Epoch 89, Step 14 Loss = 0.6301052570343018\n",
      "Epoch 89, Step 15 Loss = 0.6066858768463135\n",
      "Epoch 89, Step 16 Loss = 0.9238706827163696\n",
      "Epoch 89, Step 17 Loss = 1.17509925365448\n",
      "Epoch 89, Step 18 Loss = 0.42328083515167236\n",
      "Epoch 89, Step 19 Loss = 0.28385108709335327\n",
      "Epoch 89, Step 20 Loss = 0.3839734196662903\n",
      "Epoch 89, Step 21 Loss = 0.4071582555770874\n",
      "Epoch 89, Step 22 Loss = 0.8956613540649414\n",
      "Epoch 89, Step 23 Loss = 0.555579662322998\n",
      "Epoch 89, Step 24 Loss = 0.8095029592514038\n",
      "Epoch 89, Step 25 Loss = 0.7017934322357178\n",
      "Epoch 89, Step 26 Loss = 0.4660210609436035\n",
      "Epoch 89, Step 27 Loss = 0.9535968899726868\n",
      "Epoch 89, Step 28 Loss = 0.4825206995010376\n",
      "Epoch 89, Step 29 Loss = 0.4957112967967987\n",
      "Epoch 89, Step 30 Loss = 0.6569703817367554\n",
      "Epoch 89, Step 31 Loss = 0.23663045465946198\n",
      "Epoch 89, Step 32 Loss = 0.5998494029045105\n",
      "Epoch 89, Step 33 Loss = 1.1946525573730469\n",
      "Epoch 89, Step 34 Loss = 0.3593021631240845\n",
      "Epoch 89, Step 35 Loss = 0.8148748874664307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26316\\31189905.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_gru4rec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26316\\1377463807.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, train_dataset, optimizer, loss_fn, epochs, k, val_dataset)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# Compute gradients and apply them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Update Recall@k metric using logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# recall_at_k.update_state(targets, logits)  # Use logits for metric calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;31m# Apply gradient updates.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend_apply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clip_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;31m# Run udpate step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             self._backend_update_step(\n\u001b[0m\u001b[0;32m    473\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_reduce_sum_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_tf_update_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mThe\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \"\"\"\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n\u001b[0;32m     54\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             distribution.extended.update(\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3001\u001b[0m         \u001b[0m_get_default_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3002\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   3003\u001b[0m           \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_by_default\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3004\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3005\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3006\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3007\u001b[0m       return self._replica_ctx_update(\n\u001b[0;32m   3008\u001b[0m           \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4072\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4073\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4074\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4075\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4077\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4078\u001b[0m     \u001b[1;31m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4079\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4080\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4081\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4082\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4083\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4084\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(var, grad, learning_rate)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_momentums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_variable_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_velocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_variable_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta_2_power\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta_1_power\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         self.assign_add(\n\u001b[0;32m    133\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m       \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m       \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subtract_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"math.subtract\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subtract\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_binary_elementwise_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Haidar\\bangkit\\capstone\\ML-TuneHive\\model-dev\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  13675\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Sub\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13676\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13677\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13678\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 13679\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  13680\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13681\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13682\u001b[0m       return sub_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Train the model\n",
    "train_gru4rec(model=model, train_dataset=train_dataset,optimizer=optimizer, loss_fn=loss_fn, epochs=500, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 recall: 0.7500\n",
      "Batch 2 recall: 0.7500\n",
      "Batch 3 recall: 0.8750\n",
      "Batch 4 recall: 0.7500\n",
      "Batch 5 recall: 0.7500\n",
      "Batch 6 recall: 0.7500\n",
      "Batch 7 recall: 0.8125\n",
      "Batch 8 recall: 0.7500\n",
      "Batch 9 recall: 0.7500\n",
      "Batch 10 recall: 0.6875\n",
      "Batch 11 recall: 0.8750\n",
      "Batch 12 recall: 0.9375\n",
      "Batch 13 recall: 0.6875\n",
      "Batch 14 recall: 0.9375\n",
      "Batch 15 recall: 0.8125\n",
      "Batch 16 recall: 0.6250\n",
      "Batch 17 recall: 0.8125\n",
      "Batch 18 recall: 0.6875\n",
      "Batch 19 recall: 0.8125\n",
      "Batch 20 recall: 1.0000\n",
      "Batch 21 recall: 0.6875\n",
      "Batch 22 recall: 0.7500\n",
      "Batch 23 recall: 0.7500\n",
      "Batch 24 recall: 0.6875\n",
      "Batch 25 recall: 0.6875\n",
      "Batch 26 recall: 0.7500\n",
      "Batch 27 recall: 0.8125\n",
      "Batch 28 recall: 0.8125\n",
      "Batch 29 recall: 0.5000\n",
      "Batch 30 recall: 0.8750\n",
      "Batch 31 recall: 0.8125\n",
      "Batch 32 recall: 0.6875\n",
      "Batch 33 recall: 0.8125\n",
      "Batch 34 recall: 0.8125\n",
      "Batch 35 recall: 0.8125\n",
      "Batch 36 recall: 0.7500\n",
      "Batch 37 recall: 0.8750\n",
      "Batch 38 recall: 0.6250\n",
      "Batch 39 recall: 0.8750\n",
      "Batch 40 recall: 0.5000\n",
      "Batch 41 recall: 0.7500\n",
      "Overall recall: 0.7668\n"
     ]
    }
   ],
   "source": [
    "def predict(model, item_sequences, item_features, item_genres):\n",
    "    \"\"\"\n",
    "    Predict the item with the highest probability for the given input sequences using argmax of softmax.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
    "    - item_features: Input item features (batch_size, feature_length).\n",
    "    - item_genres: Input item genres (batch_size, genre_length).\n",
    "\n",
    "    Returns:\n",
    "    - predicted_items: A list of predicted items with the highest probability for each input sequence.\n",
    "    \"\"\"\n",
    "    # Run the model in inference mode (not training)\n",
    "    predicted_items, logits = model((item_sequences, item_features, item_genres), training=False)\n",
    "    \n",
    "    # Apply softmax to the logits to get probabilities\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Get the item with the highest probability by finding the index of the maximum probability\n",
    "    predicted_items = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    # Convert to numpy array for easier handling\n",
    "    predicted_items = predicted_items.numpy()\n",
    "\n",
    "    return predicted_items\n",
    "\n",
    "def compute_recall(predicted_items, targets):\n",
    "    \"\"\"\n",
    "    Compute the recall for the given predictions and targets.\n",
    "\n",
    "    Args:\n",
    "    - predicted_items: The predicted items (batch_size,).\n",
    "    - targets: The actual next items (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "    - recall: The recall metric.\n",
    "    \"\"\"\n",
    "    # True positives: Predicted item matches the target\n",
    "    true_positives = np.sum(predicted_items == targets)\n",
    "    \n",
    "    # Total relevant items (in this case, it is the number of items in the batch)\n",
    "    total_items = len(targets)\n",
    "    \n",
    "    # Recall calculation\n",
    "    recall = true_positives / total_items if total_items > 0 else 0\n",
    "    return recall\n",
    "\n",
    "# Initialize variables to calculate overall recall\n",
    "total_true_positives = 0\n",
    "total_items = 0\n",
    "\n",
    "# Loop through training dataset and predict the most probable item\n",
    "for step, batch in enumerate(train_dataset):\n",
    "    item_sequences = batch['item']\n",
    "    item_genres = batch['genre']\n",
    "    item_features = batch['features']\n",
    "    targets = batch['next_item']\n",
    "    \n",
    "    # Get the predicted item with the highest probability for each sequence in the batch\n",
    "    predicted_items = predict(model, item_sequences, item_features, item_genres)\n",
    "    \n",
    "    # Compute recall for the current batch\n",
    "    batch_recall = compute_recall(predicted_items, targets)\n",
    "    print(f\"Batch {step + 1} recall: {batch_recall:.4f}\")\n",
    "    \n",
    "    # Accumulate for overall recall\n",
    "    total_true_positives += np.sum(predicted_items == targets)\n",
    "    total_items += len(targets)\n",
    "\n",
    "# Calculate overall recall\n",
    "overall_recall = total_true_positives / total_items if total_items > 0 else 0\n",
    "print(f\"Overall recall: {overall_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
