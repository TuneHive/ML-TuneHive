{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeev-haydar/ML-TuneHive/blob/model-development%2Fhaidar/model-dev/preprocessing-modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1TZ3huHSIkB",
        "outputId": "3a68ad79-241c-445a-a509-0e1b6b870fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 11 10:59:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7iFBw8Iau9f"
      },
      "source": [
        "# Preprocessing and Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7tdLXmCau9h"
      },
      "source": [
        "## Import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ni2TKE2oau9i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.api.layers import Dense, Embedding, GRU, LeakyReLU, Concatenate, Masking, Layer, StringLookup, Normalization, BatchNormalization, Attention, Dropout\n",
        "from keras.api import Input\n",
        "from keras.api.models import Model\n",
        "from keras.api.losses import SparseCategoricalCrossentropy\n",
        "from keras.api.metrics import SparseCategoricalAccuracy, Mean, TopKCategoricalAccuracy\n",
        "# from transformers.models.bert import TFBertTokenizer, TFBertEmbeddings  # embedding and tokenizer for description/nlp related stufff\n",
        "from keras.api.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtBlEo2au9j"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkeNofysau9k"
      },
      "source": [
        "### Load CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhWy8OaOaxMX",
        "outputId": "4868560b-25dd-46bb-969a-7bddf1508b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 11:16:36--  https://raw.githubusercontent.com/zeev-haydar/ML-TuneHive/main/model-dev/data/session-data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21952709 (21M) [text/plain]\n",
            "Failed to rename session-data.csv to session-data.csv.1: (2) No such file or directory\n",
            "Saving to: ‘session-data.csv’\n",
            "\n",
            "session-data.csv    100%[===================>]  20.94M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-12-11 11:16:37 (232 MB/s) - ‘session-data.csv’ saved [21952709/21952709]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Running in Google Colab\n",
        "# \"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{file}\"\n",
        "url = \"https://raw.githubusercontent.com/zeev-haydar/ML-TuneHive/main/model-dev/data/session-data.csv\"\n",
        "!wget --no-cache --backups=1 {url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "hFAqYSf1au9k",
        "outputId": "c024e548-6dd9-48ee-8490-b72022e84286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       index_x                                       SongID  ...       session_3_hour session_id\n",
              "0            0                 Twenty Five MilesEdwin Starr  ...  2021-05-25 21:00:00       4332\n",
              "1            1                       Devil's EyesGreyhounds  ...  2021-05-25 21:00:00       4332\n",
              "2            2                          Pussy and PizzaMurs  ...  2021-05-25 21:00:00       4332\n",
              "3            8                   Our Special PlaceThe Heavy  ...  2021-05-25 21:00:00       4332\n",
              "4           10      Make Peace and be FreePerfect Confusion  ...  2021-05-25 21:00:00       4332\n",
              "...        ...                                          ...  ...                  ...        ...\n",
              "50013    62902  From Me To You - Remastered 2009The Beatles  ...  2017-01-01 15:00:00          0\n",
              "50014    62903  And I Love Her - Remastered 2009The Beatles  ...  2017-01-01 15:00:00          0\n",
              "50015    62904  Ticket To Ride - Remastered 2009The Beatles  ...  2017-01-01 15:00:00          0\n",
              "50016    62905   Come Together - Remastered 2009The Beatles  ...  2017-01-01 15:00:00          0\n",
              "50017    62906      Penny Lane - Remastered 2009The Beatles  ...  2017-01-01 15:00:00          0\n",
              "\n",
              "[50018 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c221de5-beb1-46ec-a74a-7abd63b131d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index_x</th>\n",
              "      <th>SongID</th>\n",
              "      <th>TimeStamp_Central</th>\n",
              "      <th>Performer_x</th>\n",
              "      <th>Album</th>\n",
              "      <th>Song_x</th>\n",
              "      <th>TimeStamp_UTC</th>\n",
              "      <th>index_y</th>\n",
              "      <th>Performer_y</th>\n",
              "      <th>Song_y</th>\n",
              "      <th>spotify_genre</th>\n",
              "      <th>spotify_track_id</th>\n",
              "      <th>spotify_track_preview_url</th>\n",
              "      <th>spotify_track_duration_ms</th>\n",
              "      <th>spotify_track_popularity</th>\n",
              "      <th>spotify_track_explicit</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>key</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>liveness</th>\n",
              "      <th>valence</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>session_3_hour</th>\n",
              "      <th>session_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Twenty Five MilesEdwin Starr</td>\n",
              "      <td>5/25/2021 5:18:00 PM</td>\n",
              "      <td>Edwin Starr</td>\n",
              "      <td>25 Miles</td>\n",
              "      <td>Twenty Five Miles</td>\n",
              "      <td>2021-05-25 23:18:00</td>\n",
              "      <td>9761</td>\n",
              "      <td>Edwin Starr</td>\n",
              "      <td>Twenty Five Miles</td>\n",
              "      <td>['classic soul', 'motown', 'northern soul', 'q...</td>\n",
              "      <td>0vstoapfzoOlqD8zurG2uJ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>198213.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.814</td>\n",
              "      <td>0.836</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-7.376</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>0.0595</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.2240</td>\n",
              "      <td>0.964</td>\n",
              "      <td>124.567</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2021-05-25 21:00:00</td>\n",
              "      <td>4332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Devil's EyesGreyhounds</td>\n",
              "      <td>5/25/2021 5:15:00 PM</td>\n",
              "      <td>Greyhounds</td>\n",
              "      <td>Change of Pace</td>\n",
              "      <td>Devil's Eyes</td>\n",
              "      <td>2021-05-25 23:15:00</td>\n",
              "      <td>206</td>\n",
              "      <td>Greyhounds</td>\n",
              "      <td>Devil's Eyes</td>\n",
              "      <td>['deep new americana', 'funk']</td>\n",
              "      <td>3CkOUiI69MLiPWbwnhuLqy</td>\n",
              "      <td>https://p.scdn.co/mp3-preview/63277cc513a316c5...</td>\n",
              "      <td>202849.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.890</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-7.913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0456</td>\n",
              "      <td>0.3540</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.858</td>\n",
              "      <td>113.236</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2021-05-25 21:00:00</td>\n",
              "      <td>4332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Pussy and PizzaMurs</td>\n",
              "      <td>5/25/2021 5:12:00 PM</td>\n",
              "      <td>Murs</td>\n",
              "      <td>Have a Nice Life</td>\n",
              "      <td>Pussy and Pizza</td>\n",
              "      <td>2021-05-25 23:12:00</td>\n",
              "      <td>6404</td>\n",
              "      <td>Murs</td>\n",
              "      <td>Pussy and Pizza</td>\n",
              "      <td>['dance pop', 'europop', 'neo mellow', 'pop', ...</td>\n",
              "      <td>2FF8eCwJktWb5sX261j8ew</td>\n",
              "      <td>https://p.scdn.co/mp3-preview/6acd37ab2288d7b5...</td>\n",
              "      <td>197306.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.697</td>\n",
              "      <td>0.697</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-7.123</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0659</td>\n",
              "      <td>0.0708</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0780</td>\n",
              "      <td>0.381</td>\n",
              "      <td>93.991</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2021-05-25 21:00:00</td>\n",
              "      <td>4332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>Our Special PlaceThe Heavy</td>\n",
              "      <td>5/25/2021 4:46:00 PM</td>\n",
              "      <td>The Heavy</td>\n",
              "      <td>Great Vengeance and Furious Fire</td>\n",
              "      <td>Our Special Place</td>\n",
              "      <td>2021-05-25 22:46:00</td>\n",
              "      <td>6205</td>\n",
              "      <td>The Heavy</td>\n",
              "      <td>Our Special Place</td>\n",
              "      <td>['bath indie', 'garage rock', 'modern blues ro...</td>\n",
              "      <td>5bICqYTxNBwSq1QM0WjvDj</td>\n",
              "      <td>https://p.scdn.co/mp3-preview/3b709da0c0c90487...</td>\n",
              "      <td>217506.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.697</td>\n",
              "      <td>0.798</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.173</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0386</td>\n",
              "      <td>0.2720</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.0991</td>\n",
              "      <td>0.939</td>\n",
              "      <td>193.996</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2021-05-25 21:00:00</td>\n",
              "      <td>4332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>Make Peace and be FreePerfect Confusion</td>\n",
              "      <td>5/25/2021 4:39:00 PM</td>\n",
              "      <td>Perfect Confusion</td>\n",
              "      <td>Perfect Confusion</td>\n",
              "      <td>Make Peace and be Free</td>\n",
              "      <td>2021-05-25 22:39:00</td>\n",
              "      <td>6051</td>\n",
              "      <td>Perfect Confusion</td>\n",
              "      <td>Make Peace and be Free</td>\n",
              "      <td>[]</td>\n",
              "      <td>6MB9kNDosalOSJjyVWAs3F</td>\n",
              "      <td>https://p.scdn.co/mp3-preview/02ff8d331ebff3bd...</td>\n",
              "      <td>230880.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.0</td>\n",
              "      <td>-6.943</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.431</td>\n",
              "      <td>78.037</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2021-05-25 21:00:00</td>\n",
              "      <td>4332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50013</th>\n",
              "      <td>62902</td>\n",
              "      <td>From Me To You - Remastered 2009The Beatles</td>\n",
              "      <td>1/1/2017 10:04:00 AM</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Past Masters (Vols. 1 &amp; 2 / Remastered)</td>\n",
              "      <td>From Me To You - Remastered 2009</td>\n",
              "      <td>2017-01-01 16:04:00</td>\n",
              "      <td>5693</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>From Me To You - Remastered 2009</td>\n",
              "      <td>['british invasion', 'merseybeat', 'psychedeli...</td>\n",
              "      <td>7nBGBbzfnPiowCvXKJnV9J</td>\n",
              "      <td>NaN</td>\n",
              "      <td>117106.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.660</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-8.513</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0309</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.2690</td>\n",
              "      <td>0.966</td>\n",
              "      <td>136.125</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2017-01-01 15:00:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50014</th>\n",
              "      <td>62903</td>\n",
              "      <td>And I Love Her - Remastered 2009The Beatles</td>\n",
              "      <td>1/1/2017 10:01:00 AM</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>A Hard Day's Night (Remastered)</td>\n",
              "      <td>And I Love Her - Remastered 2009</td>\n",
              "      <td>2017-01-01 16:01:00</td>\n",
              "      <td>360</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>And I Love Her - Remastered 2009</td>\n",
              "      <td>['british invasion', 'merseybeat', 'psychedeli...</td>\n",
              "      <td>65vdMBskhx3akkG9vQlSH1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>149693.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.331</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-10.777</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0337</td>\n",
              "      <td>0.6400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.636</td>\n",
              "      <td>113.312</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2017-01-01 15:00:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50015</th>\n",
              "      <td>62904</td>\n",
              "      <td>Ticket To Ride - Remastered 2009The Beatles</td>\n",
              "      <td>1/1/2017 9:58:00 AM</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Help! (Remastered)</td>\n",
              "      <td>Ticket To Ride - Remastered 2009</td>\n",
              "      <td>2017-01-01 15:58:00</td>\n",
              "      <td>9715</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Ticket To Ride - Remastered 2009</td>\n",
              "      <td>['british invasion', 'merseybeat', 'psychedeli...</td>\n",
              "      <td>7CZiDzGVjUssMSOXrDNYHL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>189680.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.850</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-6.777</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.2330</td>\n",
              "      <td>0.749</td>\n",
              "      <td>123.419</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2017-01-01 15:00:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50016</th>\n",
              "      <td>62905</td>\n",
              "      <td>Come Together - Remastered 2009The Beatles</td>\n",
              "      <td>1/1/2017 9:54:00 AM</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Abbey Road (Remastered)</td>\n",
              "      <td>Come Together - Remastered 2009</td>\n",
              "      <td>2017-01-01 15:54:00</td>\n",
              "      <td>7425</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Come Together - Remastered 2009</td>\n",
              "      <td>['british invasion', 'merseybeat', 'psychedeli...</td>\n",
              "      <td>2EqlS6tkEnglzr7tkKAAYD</td>\n",
              "      <td>NaN</td>\n",
              "      <td>259946.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.376</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-11.913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0393</td>\n",
              "      <td>0.0302</td>\n",
              "      <td>0.248000</td>\n",
              "      <td>0.0926</td>\n",
              "      <td>0.187</td>\n",
              "      <td>165.007</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2017-01-01 15:00:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50017</th>\n",
              "      <td>62906</td>\n",
              "      <td>Penny Lane - Remastered 2009The Beatles</td>\n",
              "      <td>1/1/2017 9:51:00 AM</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Magical Mystery Tour (Remastered)</td>\n",
              "      <td>Penny Lane - Remastered 2009</td>\n",
              "      <td>2017-01-01 15:51:00</td>\n",
              "      <td>4401</td>\n",
              "      <td>The Beatles</td>\n",
              "      <td>Penny Lane - Remastered 2009</td>\n",
              "      <td>['british invasion', 'merseybeat', 'psychedeli...</td>\n",
              "      <td>1h04XMpzGzmAudoI6VHBgA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>180893.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.488</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-8.220</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.2120</td>\n",
              "      <td>0.026000</td>\n",
              "      <td>0.1360</td>\n",
              "      <td>0.490</td>\n",
              "      <td>113.038</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2017-01-01 15:00:00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50018 rows × 30 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c221de5-beb1-46ec-a74a-7abd63b131d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c221de5-beb1-46ec-a74a-7abd63b131d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c221de5-beb1-46ec-a74a-7abd63b131d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7608db27-b642-4906-95e6-b59b2245ad08\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7608db27-b642-4906-95e6-b59b2245ad08')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7608db27-b642-4906-95e6-b59b2245ad08 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a7a85bff-c004-412b-9269-6aa6bd986e5c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a7a85bff-c004-412b-9269-6aa6bd986e5c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import keras\n",
        "import os\n",
        "print(keras.__version__)\n",
        "\n",
        "# root_path = \"data\"\n",
        "root_path = \"\" # if using colab\n",
        "df = pd.read_csv(os.path.join(root_path, \"session-data.csv\"))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkOi7VaWau9m",
        "outputId": "885e777a-404e-4294-de1f-08d2497b2b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50018 entries, 0 to 50017\n",
            "Data columns (total 30 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   index_x                    50018 non-null  int64  \n",
            " 1   SongID                     50018 non-null  object \n",
            " 2   TimeStamp_Central          50018 non-null  object \n",
            " 3   Performer_x                50018 non-null  object \n",
            " 4   Album                      47890 non-null  object \n",
            " 5   Song_x                     50018 non-null  object \n",
            " 6   TimeStamp_UTC              50018 non-null  object \n",
            " 7   index_y                    50018 non-null  int64  \n",
            " 8   Performer_y                50018 non-null  object \n",
            " 9   Song_y                     50018 non-null  object \n",
            " 10  spotify_genre              50018 non-null  object \n",
            " 11  spotify_track_id           50018 non-null  object \n",
            " 12  spotify_track_preview_url  36001 non-null  object \n",
            " 13  spotify_track_duration_ms  50018 non-null  float64\n",
            " 14  spotify_track_popularity   50018 non-null  float64\n",
            " 15  spotify_track_explicit     50018 non-null  bool   \n",
            " 16  danceability               50018 non-null  float64\n",
            " 17  energy                     50018 non-null  float64\n",
            " 18  key                        50018 non-null  float64\n",
            " 19  loudness                   50018 non-null  float64\n",
            " 20  mode                       50018 non-null  float64\n",
            " 21  speechiness                50018 non-null  float64\n",
            " 22  acousticness               50018 non-null  float64\n",
            " 23  instrumentalness           50018 non-null  float64\n",
            " 24  liveness                   50018 non-null  float64\n",
            " 25  valence                    50018 non-null  float64\n",
            " 26  tempo                      50018 non-null  float64\n",
            " 27  time_signature             50018 non-null  float64\n",
            " 28  session_3_hour             50018 non-null  object \n",
            " 29  session_id                 50018 non-null  int64  \n",
            "dtypes: bool(1), float64(14), int64(3), object(12)\n",
            "memory usage: 11.1+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUBP-uxUau9m",
        "outputId": "7b495386-3f88-4c64-cca4-2d26ecae8f88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        1.0\n",
              "1        0.0\n",
              "2        1.0\n",
              "3        1.0\n",
              "4        1.0\n",
              "        ... \n",
              "50013    1.0\n",
              "50014    0.0\n",
              "50015    1.0\n",
              "50016    0.0\n",
              "50017    1.0\n",
              "Name: mode, Length: 50018, dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_col_name = 'mode'\n",
        "df.loc[:, test_col_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdbzTAEau9n"
      },
      "source": [
        "## Remove N.A.N data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxvNRVFeau9n"
      },
      "outputs": [],
      "source": [
        "# df_filtered = df[~df['danceability'].isna()]\n",
        "# df_filtered.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ3vqtcvau9o"
      },
      "source": [
        "### Prepare Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "RP5P9y7yau9o"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.api.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feature columns (as provided)\n",
        "feature_columns = [\n",
        "    'spotify_genre',\n",
        "]\n",
        "\n",
        "# Define the DataPreprocessor class\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, df, feature_columns, batch_size=16, fixed_genre_size=10, train_size=0.8):\n",
        "        \"\"\"\n",
        "        Initializes the data preprocessor with necessary parameters and preprocessing layers.\n",
        "\n",
        "        Args:\n",
        "            df (DataFrame): The input DataFrame containing session data.\n",
        "            feature_columns (list): List of feature column names.\n",
        "            batch_size (int): The batch size for dataset creation.\n",
        "            fixed_genre_size (int): The fixed size for genre vectorization.\n",
        "            train_size (float): Proportion of the data to use for training (between 0 and 1).\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.feature_columns = feature_columns\n",
        "        self.batch_size = batch_size\n",
        "        self.fixed_genre_size = fixed_genre_size\n",
        "        self.train_size = train_size\n",
        "\n",
        "        # Split the dataset into training and testing datasets\n",
        "        self.train_df, self.test_df = train_test_split(self.df, train_size=self.train_size, random_state=42)\n",
        "\n",
        "        # Numeric feature preprocessing\n",
        "        self.numeric_data = self.df[feature_columns[1:]].apply(pd.to_numeric, errors='coerce')\n",
        "        self.mean_values = self.numeric_data.mean()\n",
        "        self.std_values = self.numeric_data.std()\n",
        "\n",
        "        # Initialize LabelEncoder for SongID and spotify_genre\n",
        "        self.song_id_encoder = LabelEncoder()\n",
        "        self.genre_encoder = LabelEncoder()\n",
        "\n",
        "        # Extract unique SongIDs and genres\n",
        "        unique_song_ids = self.df['SongID'].unique()\n",
        "        all_genres = []\n",
        "        for genre_str in self.df['spotify_genre']:\n",
        "            try:\n",
        "                genre_list = ast.literal_eval(genre_str)  # Safely parse the string into a list\n",
        "                if isinstance(genre_list, list):\n",
        "                    all_genres.extend(genre_list)\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing genre: {e}\")\n",
        "\n",
        "        unique_genres = list(set(all_genres))\n",
        "\n",
        "        # Fit the LabelEncoders on the data\n",
        "        self.song_id_encoder.fit(unique_song_ids)\n",
        "        self.genre_encoder.fit(unique_genres)\n",
        "\n",
        "        self.items_size = len(self.song_id_encoder.classes_)  # Number of unique SongIDs\n",
        "        self.genres_size = len(self.genre_encoder.classes_)\n",
        "\n",
        "        self.dataset = None\n",
        "\n",
        "    def preprocess_song_id(self, song_id):\n",
        "        \"\"\"\n",
        "        Encode the SongID using LabelEncoder.\n",
        "        \"\"\"\n",
        "        return self.song_id_encoder.transform([song_id])[0]\n",
        "\n",
        "    def clean_genre(self, value, default_value=0, dtype=tf.int32):\n",
        "        \"\"\"\n",
        "        Clean and process the 'spotify_genre' feature.\n",
        "        \"\"\"\n",
        "        if value is None or (isinstance(value, str) and not value.strip()):\n",
        "            return np.full((self.fixed_genre_size,), default_value, dtype=dtype.as_numpy_dtype)\n",
        "\n",
        "        try:\n",
        "            genre_list = eval(value) if isinstance(value, str) else value\n",
        "            if isinstance(genre_list, list):\n",
        "                genre_encoded = self.genre_encoder.transform(genre_list)\n",
        "            else:\n",
        "                genre_encoded = self.genre_encoder.transform([value])\n",
        "        except Exception:\n",
        "            genre_encoded = self.genre_encoder.transform([value])\n",
        "\n",
        "        # Pad or truncate to fixed size\n",
        "        return np.pad(genre_encoded, (0, max(0, self.fixed_genre_size - len(genre_encoded))),\n",
        "                      mode='constant')[:self.fixed_genre_size].astype(dtype.as_numpy_dtype)\n",
        "\n",
        "    def clean_numeric_feature(self, value, default_value=0.0, feature_name=\"feature\", mean=None, std=None):\n",
        "        \"\"\"\n",
        "        Clean, process, and normalize numerical features using Z-score normalization.\n",
        "        \"\"\"\n",
        "        if value is None or (isinstance(value, float) and np.isnan(value)):\n",
        "            return default_value\n",
        "\n",
        "        try:\n",
        "            value = float(value)\n",
        "            # Apply Z-score normalization if mean and std are provided\n",
        "            if mean is not None and std is not None and std != 0:\n",
        "                z_score_value = (value - mean) / std\n",
        "                return z_score_value\n",
        "            return value  # Return raw value if no normalization\n",
        "        except ValueError:\n",
        "            return default_value\n",
        "\n",
        "    def create_session_dataset(self, session_df):\n",
        "        \"\"\"\n",
        "        Create session dataset as a list of dictionaries for each session.\n",
        "        \"\"\"\n",
        "        session_df = session_df.sort_values(by=['session_id', 'TimeStamp_UTC'])\n",
        "        grouped = session_df.groupby('session_id')\n",
        "        sessions_data = []\n",
        "        for session_id, group in grouped:\n",
        "            session_data = group.to_dict(orient='records')\n",
        "            sessions_data.append(session_data)\n",
        "        return sessions_data\n",
        "\n",
        "    def preprocess_data(self, sessions, k=1):\n",
        "        \"\"\"\n",
        "        Preprocess session data into TensorFlow dataset with split genre and features,\n",
        "        filtering out sequences where the next item sequence length is not greater than 10.\n",
        "        \"\"\"\n",
        "        item_sequences = []\n",
        "        next_item_sequences = []\n",
        "        genre_sequences = []\n",
        "        next_genre_sequences = []\n",
        "        feature_sequences = []\n",
        "        processed_item_count = 0\n",
        "\n",
        "        for idx, session in enumerate(sessions):\n",
        "            # Filter the session that has length less than k\n",
        "            if len(session) < k:\n",
        "                continue\n",
        "            session_item_sequences = []\n",
        "            session_next_item_sequences = []\n",
        "            session_genre_sequences = []\n",
        "            session_next_genre_sequences= []\n",
        "            session_feature_sequences = []\n",
        "            session_id = session[0]['session_id']\n",
        "\n",
        "            for i in range(len(session) - 1):\n",
        "                # Process items\n",
        "                session_item_encoded = self.preprocess_song_id(session[i]['SongID'])\n",
        "                next_session_item_encoded = self.preprocess_song_id(session[i + 1]['SongID'])\n",
        "                session_item_sequences.append(session_item_encoded)\n",
        "                session_next_item_sequences.append(next_session_item_encoded)\n",
        "\n",
        "                # Process genre\n",
        "                genre_cleaned = self.clean_genre(session[i].get('spotify_genre', None))\n",
        "                next_genre_cleaned = self.clean_genre(session[i+1].get('spotify_genre', None))\n",
        "                session_genre_sequences.append(genre_cleaned)\n",
        "                session_next_genre_sequences.append(next_genre_cleaned)\n",
        "\n",
        "                # Process numerical features\n",
        "                numeric_features = []\n",
        "                for col in self.feature_columns:\n",
        "                    if col != 'spotify_genre':\n",
        "                        mean = self.mean_values.get(col, None)\n",
        "                        std = self.std_values.get(col, None)\n",
        "                        cleaned_feature = self.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n",
        "                        numeric_features.append(cleaned_feature)\n",
        "\n",
        "                session_feature_sequences.append(numeric_features)\n",
        "\n",
        "            # Filter out sessions where the next item sequence length is not greater than 10\n",
        "            # Extend sequences only if the next item sequence length is greater than 10\n",
        "            print(\"session item sequences:\", session_item_sequences)\n",
        "            print(\"session next item sequences:\", session_next_item_sequences)\n",
        "\n",
        "            # Filter the item that have session length less than k\n",
        "            item_sequences.append(session_item_sequences)\n",
        "            next_item_sequences.append(session_next_item_sequences)\n",
        "            genre_sequences.append(session_genre_sequences)\n",
        "            next_genre_sequences.append(session_next_genre_sequences)\n",
        "            feature_sequences.append(session_feature_sequences)\n",
        "            processed_item_count += len(session_item_sequences)\n",
        "\n",
        "            #     # print(f\"Session {idx + 1} processed with {len(session_item_sequences)} items.\")\n",
        "            # else:\n",
        "            #     print(f\"Session {idx + 1} skipped because next item sequence length is {len(session_next_item_sequences)}.\")\n",
        "\n",
        "        print(f\"Total processed items: {processed_item_count}\")\n",
        "\n",
        "        # Pad sequences\n",
        "        item_sequences = pad_sequences(item_sequences, padding='pre', value=0)\n",
        "        next_item_sequences = pad_sequences(next_item_sequences, padding='pre', value=0)\n",
        "        genre_sequences = pad_sequences(genre_sequences, padding='pre', value=0)\n",
        "        next_genre_sequences = pad_sequences(next_genre_sequences, padding='pre', value=0)\n",
        "        feature_sequences = pad_sequences(feature_sequences, padding='pre', dtype='float32', value=0.0)\n",
        "        # print(f\"item_sequences shape: {item_sequences.shape}\")\n",
        "        # print(f\"next_item_sequences shape: {next_item_sequences.shape}\")\n",
        "        # print(f\"genre_sequences shape: {genre_sequences.shape}\")\n",
        "        # print(f\"next_genre_sequences shape: {next_genre_sequences.shape}\")\n",
        "        # print(f\"feature_sequences shape: {feature_sequences.shape}\")\n",
        "        # print(\"item sequence padded:\", item_sequences)\n",
        "        # print(\"next item sequence padded:\", next_item_sequences)\n",
        "        # print(\"genre sequence padded:\", genre_sequences)\n",
        "        # print(\"next genre sequence padded:\", next_genre_sequences)\n",
        "        # print(\"feature sequence padded:\", feature_sequences)\n",
        "\n",
        "        # Create TensorFlow dataset\n",
        "        dataset = tf.data.Dataset.from_tensor_slices({\n",
        "            'item': item_sequences,\n",
        "            'genre': genre_sequences,\n",
        "            'features': feature_sequences,\n",
        "            'next_item': next_item_sequences,\n",
        "            'next_genre': next_genre_sequences\n",
        "        })\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def create_session_dataset_tensor(self, k=1):\n",
        "        \"\"\"\n",
        "        Main function to create session dataset as tensors and return the dataset.\n",
        "        \"\"\"\n",
        "        if self.dataset is not None:\n",
        "            print(\"Dataset already created\")\n",
        "            return\n",
        "\n",
        "        print(\"Creating session dataset\")\n",
        "        sessions_data = self.create_session_dataset(self.df)\n",
        "        dataset = self.preprocess_data(sessions_data, k=k)\n",
        "\n",
        "        # Shuffle and batch the training data\n",
        "        dataset = (\n",
        "            dataset.batch(self.batch_size, drop_remainder=True)\n",
        "                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "        return dataset\n",
        "\n",
        "    def create_train_dataset(self, k=1):\n",
        "        \"\"\"\n",
        "        Main function to create session dataset as tensors and return the dataset.\n",
        "        \"\"\"\n",
        "        if self.dataset is not None:\n",
        "            print(\"Dataset already created\")\n",
        "            return\n",
        "\n",
        "        print(\"Creating session dataset\")\n",
        "        sessions_data = self.create_session_dataset(self.train_df)  # Use train data for training\n",
        "        print(\"Creating tensor dataset\")\n",
        "        dataset = self.preprocess_data(sessions_data, k=k)\n",
        "\n",
        "        # Shuffle and batch the training data\n",
        "        dataset = (\n",
        "            dataset.shuffle(buffer_size=1024)\n",
        "                   .batch(self.batch_size, drop_remainder=True)\n",
        "                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        )\n",
        "        return dataset\n",
        "\n",
        "    def get_test_data(self, k):\n",
        "        \"\"\"\n",
        "        Return preprocessed test dataset without shuffling.\n",
        "        \"\"\"\n",
        "        sessions_data = self.create_session_dataset(self.test_df)\n",
        "        dataset = self.preprocess_data(sessions_data, k)\n",
        "\n",
        "        # Batch the test data without shuffling\n",
        "        dataset = (\n",
        "            dataset.batch(self.batch_size, drop_remainder=True)\n",
        "                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        )\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def batch_timer(self, dataset):\n",
        "        \"\"\"\n",
        "        Timer function to track the time taken for batch processing.\n",
        "        \"\"\"\n",
        "        for batch in dataset:\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Simulate processing (e.g., model training or data transformation)\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "            print(f\"Batch processing time: {batch_time:.4f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-size:36px;\">PENTING: Baca Notes Comment di setiap Sel!!!<p>"
      ],
      "metadata": {
        "id": "5Ug642Mp4nKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "vqP4prfnau9q",
        "outputId": "db4e8a66-b078-460a-ff06-050e67d7cf91"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DataPreprocessor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a5e94003498>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preprocessor = DataPreprocessor(df, feature_columns)  # <---- pakai ini untuk bener-bener ngetrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <------ pakai ini buat ngetes apakah bisa ditrain, bisa ekspor model, bisa import modelnya dengan lancar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create the session dataset tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session_dataset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# k = 4 artinya mengambil sesi dengan panjang sesi item selanjutnya lebih dari 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataPreprocessor' is not defined"
          ]
        }
      ],
      "source": [
        "# preprocessor = DataPreprocessor(df, feature_columns)  # <---- pakai ini untuk bener-bener ngetrain\n",
        "preprocessor = DataPreprocessor(df[:3000], feature_columns) # <------ pakai ini buat ngetes apakah bisa ditrain, bisa ekspor model, bisa import modelnya dengan lancar\n",
        "\n",
        "# Create the session dataset tensor\n",
        "dataset = preprocessor.create_session_dataset_tensor(k=10)  # k = 4 artinya mengambil sesi dengan panjang sesi item selanjutnya lebih dari 4\n",
        "# for batch in dataset.take(1):\n",
        "#     print(\"Items (SongID):\", batch['item'].numpy())\n",
        "#     print(\"Genre:\", batch['genre'].numpy())\n",
        "#     print(\"Features:\", batch['features'].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nPLPszcfaHv",
        "outputId": "d0688b74-7cfa-461b-f2b0-62253af0b9c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session 1 skipped because next item sequence length is 0.\n",
            "Session 2 skipped because next item sequence length is 0.\n",
            "Session 3 skipped because next item sequence length is 0.\n",
            "Session 4 skipped because next item sequence length is 0.\n",
            "Session 5 skipped because next item sequence length is 0.\n",
            "Session 6 skipped because next item sequence length is 0.\n",
            "Session 7 processed with 2 items.\n",
            "Session 8 skipped because next item sequence length is 1.\n",
            "Session 9 processed with 2 items.\n",
            "Session 10 skipped because next item sequence length is 0.\n",
            "Session 11 processed with 2 items.\n",
            "Session 12 skipped because next item sequence length is 1.\n",
            "Session 13 skipped because next item sequence length is 0.\n",
            "Session 14 skipped because next item sequence length is 0.\n",
            "Session 15 skipped because next item sequence length is 1.\n",
            "Session 16 skipped because next item sequence length is 0.\n",
            "Session 17 skipped because next item sequence length is 1.\n",
            "Session 18 processed with 2 items.\n",
            "Session 19 processed with 3 items.\n",
            "Session 20 skipped because next item sequence length is 0.\n",
            "Session 21 skipped because next item sequence length is 0.\n",
            "Session 22 skipped because next item sequence length is 0.\n",
            "Session 23 skipped because next item sequence length is 0.\n",
            "Session 24 processed with 3 items.\n",
            "Session 25 skipped because next item sequence length is 0.\n",
            "Session 26 processed with 3 items.\n",
            "Session 27 skipped because next item sequence length is 0.\n",
            "Session 28 skipped because next item sequence length is 0.\n",
            "Session 29 skipped because next item sequence length is 1.\n",
            "Session 30 skipped because next item sequence length is 0.\n",
            "Session 31 skipped because next item sequence length is 0.\n",
            "Session 32 skipped because next item sequence length is 1.\n",
            "Session 33 processed with 4 items.\n",
            "Session 34 skipped because next item sequence length is 1.\n",
            "Session 35 skipped because next item sequence length is 1.\n",
            "Session 36 processed with 2 items.\n",
            "Session 37 skipped because next item sequence length is 0.\n",
            "Session 38 skipped because next item sequence length is 0.\n",
            "Session 39 skipped because next item sequence length is 0.\n",
            "Session 40 skipped because next item sequence length is 0.\n",
            "Session 41 processed with 5 items.\n",
            "Session 42 processed with 2 items.\n",
            "Session 43 skipped because next item sequence length is 0.\n",
            "Session 44 skipped because next item sequence length is 0.\n",
            "Session 45 skipped because next item sequence length is 0.\n",
            "Session 46 processed with 2 items.\n",
            "Session 47 processed with 3 items.\n",
            "Session 48 skipped because next item sequence length is 1.\n",
            "Session 49 skipped because next item sequence length is 0.\n",
            "Session 50 skipped because next item sequence length is 0.\n",
            "Session 51 skipped because next item sequence length is 0.\n",
            "Session 52 skipped because next item sequence length is 1.\n",
            "Session 53 processed with 7 items.\n",
            "Session 54 processed with 2 items.\n",
            "Session 55 skipped because next item sequence length is 1.\n",
            "Session 56 skipped because next item sequence length is 0.\n",
            "Session 57 processed with 2 items.\n",
            "Session 58 skipped because next item sequence length is 0.\n",
            "Session 59 skipped because next item sequence length is 0.\n",
            "Session 60 skipped because next item sequence length is 1.\n",
            "Session 61 skipped because next item sequence length is 0.\n",
            "Session 62 skipped because next item sequence length is 1.\n",
            "Session 63 processed with 2 items.\n",
            "Session 64 processed with 2 items.\n",
            "Session 65 processed with 3 items.\n",
            "Session 66 processed with 3 items.\n",
            "Session 67 skipped because next item sequence length is 0.\n",
            "Session 68 processed with 3 items.\n",
            "Session 69 skipped because next item sequence length is 0.\n",
            "Session 70 processed with 2 items.\n",
            "Session 71 processed with 5 items.\n",
            "Session 72 skipped because next item sequence length is 0.\n",
            "Session 73 skipped because next item sequence length is 0.\n",
            "Session 74 processed with 3 items.\n",
            "Session 75 skipped because next item sequence length is 0.\n",
            "Session 76 processed with 3 items.\n",
            "Session 77 processed with 3 items.\n",
            "Session 78 processed with 5 items.\n",
            "Session 79 skipped because next item sequence length is 0.\n",
            "Session 80 skipped because next item sequence length is 1.\n",
            "Session 81 skipped because next item sequence length is 0.\n",
            "Session 82 skipped because next item sequence length is 0.\n",
            "Session 83 skipped because next item sequence length is 1.\n",
            "Session 84 processed with 2 items.\n",
            "Session 85 processed with 7 items.\n",
            "Session 86 processed with 2 items.\n",
            "Session 87 processed with 3 items.\n",
            "Session 88 skipped because next item sequence length is 1.\n",
            "Session 89 skipped because next item sequence length is 1.\n",
            "Session 90 skipped because next item sequence length is 0.\n",
            "Session 91 skipped because next item sequence length is 0.\n",
            "Session 92 skipped because next item sequence length is 1.\n",
            "Session 93 skipped because next item sequence length is 1.\n",
            "Session 94 processed with 2 items.\n",
            "Session 95 processed with 2 items.\n",
            "Session 96 processed with 2 items.\n",
            "Session 97 skipped because next item sequence length is 1.\n",
            "Session 98 processed with 2 items.\n",
            "Session 99 skipped because next item sequence length is 0.\n",
            "Session 100 processed with 2 items.\n",
            "Session 101 skipped because next item sequence length is 1.\n",
            "Session 102 skipped because next item sequence length is 1.\n",
            "Session 103 processed with 3 items.\n",
            "Session 104 processed with 5 items.\n",
            "Session 105 processed with 2 items.\n",
            "Session 106 skipped because next item sequence length is 0.\n",
            "Session 107 skipped because next item sequence length is 1.\n",
            "Session 108 processed with 7 items.\n",
            "Session 109 skipped because next item sequence length is 1.\n",
            "Session 110 skipped because next item sequence length is 0.\n",
            "Session 111 skipped because next item sequence length is 1.\n",
            "Session 112 processed with 5 items.\n",
            "Session 113 processed with 4 items.\n",
            "Session 114 skipped because next item sequence length is 1.\n",
            "Session 115 processed with 4 items.\n",
            "Session 116 skipped because next item sequence length is 1.\n",
            "Session 117 processed with 2 items.\n",
            "Session 118 skipped because next item sequence length is 0.\n",
            "Session 119 processed with 3 items.\n",
            "Session 120 skipped because next item sequence length is 1.\n",
            "Session 121 skipped because next item sequence length is 0.\n",
            "Session 122 skipped because next item sequence length is 0.\n",
            "Session 123 skipped because next item sequence length is 1.\n",
            "Session 124 skipped because next item sequence length is 1.\n",
            "Session 125 processed with 3 items.\n",
            "Session 126 skipped because next item sequence length is 0.\n",
            "Session 127 skipped because next item sequence length is 0.\n",
            "Session 128 skipped because next item sequence length is 1.\n",
            "Session 129 skipped because next item sequence length is 1.\n",
            "Session 130 processed with 5 items.\n",
            "Session 131 skipped because next item sequence length is 1.\n",
            "Session 132 skipped because next item sequence length is 1.\n",
            "Session 133 skipped because next item sequence length is 0.\n",
            "Session 134 skipped because next item sequence length is 0.\n",
            "Session 135 processed with 2 items.\n",
            "Session 136 skipped because next item sequence length is 0.\n",
            "Session 137 processed with 5 items.\n",
            "Session 138 processed with 3 items.\n",
            "Session 139 processed with 2 items.\n",
            "Session 140 skipped because next item sequence length is 1.\n",
            "Session 141 processed with 5 items.\n",
            "Session 142 processed with 4 items.\n",
            "Session 143 skipped because next item sequence length is 0.\n",
            "Session 144 processed with 6 items.\n",
            "Session 145 skipped because next item sequence length is 1.\n",
            "Session 146 skipped because next item sequence length is 0.\n",
            "Session 147 processed with 3 items.\n",
            "Session 148 processed with 4 items.\n",
            "Session 149 processed with 3 items.\n",
            "Session 150 processed with 2 items.\n",
            "Session 151 processed with 2 items.\n",
            "Session 152 skipped because next item sequence length is 1.\n",
            "Session 153 skipped because next item sequence length is 1.\n",
            "Session 154 processed with 2 items.\n",
            "Session 155 skipped because next item sequence length is 1.\n",
            "Session 156 skipped because next item sequence length is 0.\n",
            "Session 157 skipped because next item sequence length is 1.\n",
            "Session 158 skipped because next item sequence length is 1.\n",
            "Session 159 skipped because next item sequence length is 0.\n",
            "Session 160 processed with 2 items.\n",
            "Session 161 skipped because next item sequence length is 0.\n",
            "Session 162 processed with 2 items.\n",
            "Session 163 skipped because next item sequence length is 0.\n",
            "Session 164 skipped because next item sequence length is 1.\n",
            "Session 165 skipped because next item sequence length is 1.\n",
            "Session 166 processed with 3 items.\n",
            "Session 167 processed with 2 items.\n",
            "Session 168 skipped because next item sequence length is 1.\n",
            "Session 169 processed with 6 items.\n",
            "Session 170 processed with 4 items.\n",
            "Session 171 processed with 5 items.\n",
            "Session 172 skipped because next item sequence length is 0.\n",
            "Session 173 processed with 4 items.\n",
            "Session 174 skipped because next item sequence length is 1.\n",
            "Session 175 skipped because next item sequence length is 0.\n",
            "Session 176 processed with 2 items.\n",
            "Session 177 processed with 6 items.\n",
            "Session 178 skipped because next item sequence length is 1.\n",
            "Session 179 skipped because next item sequence length is 0.\n",
            "Session 180 skipped because next item sequence length is 0.\n",
            "Session 181 skipped because next item sequence length is 0.\n",
            "Session 182 processed with 5 items.\n",
            "Session 183 skipped because next item sequence length is 1.\n",
            "Session 184 skipped because next item sequence length is 1.\n",
            "Session 185 skipped because next item sequence length is 0.\n",
            "Session 186 skipped because next item sequence length is 0.\n",
            "Session 187 skipped because next item sequence length is 1.\n",
            "Session 188 skipped because next item sequence length is 1.\n",
            "Session 189 processed with 7 items.\n",
            "Session 190 skipped because next item sequence length is 1.\n",
            "Session 191 processed with 2 items.\n",
            "Session 192 processed with 3 items.\n",
            "Session 193 processed with 3 items.\n",
            "Session 194 skipped because next item sequence length is 0.\n",
            "Session 195 processed with 2 items.\n",
            "Session 196 processed with 5 items.\n",
            "Session 197 processed with 2 items.\n",
            "Session 198 processed with 2 items.\n",
            "Session 199 processed with 2 items.\n",
            "Session 200 processed with 5 items.\n",
            "Session 201 processed with 3 items.\n",
            "Session 202 processed with 2 items.\n",
            "Session 203 skipped because next item sequence length is 0.\n",
            "Session 204 skipped because next item sequence length is 0.\n",
            "Session 205 skipped because next item sequence length is 0.\n",
            "Session 206 skipped because next item sequence length is 1.\n",
            "Session 207 skipped because next item sequence length is 0.\n",
            "Session 208 skipped because next item sequence length is 0.\n",
            "Session 209 processed with 4 items.\n",
            "Session 210 skipped because next item sequence length is 0.\n",
            "Session 211 processed with 3 items.\n",
            "Session 212 skipped because next item sequence length is 0.\n",
            "Session 213 skipped because next item sequence length is 0.\n",
            "Session 214 skipped because next item sequence length is 0.\n",
            "Session 215 skipped because next item sequence length is 0.\n",
            "Session 216 processed with 5 items.\n",
            "Session 217 processed with 3 items.\n",
            "Session 218 skipped because next item sequence length is 0.\n",
            "Session 219 skipped because next item sequence length is 0.\n",
            "Session 220 processed with 4 items.\n",
            "Session 221 skipped because next item sequence length is 1.\n",
            "Session 222 processed with 3 items.\n",
            "Session 223 processed with 2 items.\n",
            "Session 224 skipped because next item sequence length is 0.\n",
            "Session 225 skipped because next item sequence length is 1.\n",
            "Session 226 processed with 6 items.\n",
            "Session 227 skipped because next item sequence length is 1.\n",
            "Session 228 skipped because next item sequence length is 0.\n",
            "Session 229 skipped because next item sequence length is 1.\n",
            "Session 230 skipped because next item sequence length is 1.\n",
            "Session 231 skipped because next item sequence length is 0.\n",
            "Session 232 processed with 3 items.\n",
            "Session 233 skipped because next item sequence length is 0.\n",
            "Session 234 skipped because next item sequence length is 1.\n",
            "Session 235 processed with 8 items.\n",
            "Session 236 processed with 3 items.\n",
            "Session 237 skipped because next item sequence length is 0.\n",
            "Session 238 processed with 4 items.\n",
            "Session 239 skipped because next item sequence length is 0.\n",
            "Session 240 processed with 2 items.\n",
            "Session 241 processed with 3 items.\n",
            "Session 242 skipped because next item sequence length is 1.\n",
            "Session 243 processed with 6 items.\n",
            "Session 244 skipped because next item sequence length is 0.\n",
            "Session 245 processed with 2 items.\n",
            "Session 246 processed with 2 items.\n",
            "Session 247 processed with 2 items.\n",
            "Session 248 skipped because next item sequence length is 0.\n",
            "Session 249 skipped because next item sequence length is 1.\n",
            "Session 250 processed with 3 items.\n",
            "Session 251 skipped because next item sequence length is 1.\n",
            "Session 252 skipped because next item sequence length is 1.\n",
            "Session 253 skipped because next item sequence length is 1.\n",
            "Session 254 skipped because next item sequence length is 0.\n",
            "Session 255 processed with 3 items.\n",
            "Session 256 skipped because next item sequence length is 1.\n",
            "Session 257 skipped because next item sequence length is 1.\n",
            "Session 258 skipped because next item sequence length is 1.\n",
            "Session 259 skipped because next item sequence length is 0.\n",
            "Session 260 skipped because next item sequence length is 0.\n",
            "Session 261 processed with 2 items.\n",
            "Session 262 skipped because next item sequence length is 0.\n",
            "Session 263 processed with 2 items.\n",
            "Session 264 processed with 3 items.\n",
            "Session 265 skipped because next item sequence length is 0.\n",
            "Session 266 skipped because next item sequence length is 1.\n",
            "Session 267 skipped because next item sequence length is 1.\n",
            "Session 268 processed with 3 items.\n",
            "Session 269 processed with 3 items.\n",
            "Session 270 skipped because next item sequence length is 0.\n",
            "Session 271 skipped because next item sequence length is 1.\n",
            "Session 272 skipped because next item sequence length is 0.\n",
            "Session 273 skipped because next item sequence length is 1.\n",
            "Session 274 skipped because next item sequence length is 0.\n",
            "Session 275 skipped because next item sequence length is 0.\n",
            "Session 276 processed with 2 items.\n",
            "Session 277 skipped because next item sequence length is 0.\n",
            "Session 278 skipped because next item sequence length is 0.\n",
            "Session 279 skipped because next item sequence length is 0.\n",
            "Session 280 skipped because next item sequence length is 0.\n",
            "Session 281 skipped because next item sequence length is 0.\n",
            "Session 282 skipped because next item sequence length is 0.\n",
            "Session 283 processed with 2 items.\n",
            "Session 284 skipped because next item sequence length is 1.\n",
            "Session 285 skipped because next item sequence length is 1.\n",
            "Session 286 skipped because next item sequence length is 1.\n",
            "Session 287 skipped because next item sequence length is 1.\n",
            "Session 288 skipped because next item sequence length is 0.\n",
            "Session 289 skipped because next item sequence length is 0.\n",
            "Session 290 skipped because next item sequence length is 1.\n",
            "Session 291 skipped because next item sequence length is 1.\n",
            "Session 292 processed with 2 items.\n",
            "Session 293 skipped because next item sequence length is 1.\n",
            "Session 294 skipped because next item sequence length is 0.\n",
            "Session 295 skipped because next item sequence length is 1.\n",
            "Session 296 skipped because next item sequence length is 0.\n",
            "Session 297 processed with 2 items.\n",
            "Session 298 processed with 4 items.\n",
            "Session 299 skipped because next item sequence length is 1.\n",
            "Session 300 skipped because next item sequence length is 0.\n",
            "Session 301 skipped because next item sequence length is 0.\n",
            "Session 302 processed with 5 items.\n",
            "Session 303 skipped because next item sequence length is 0.\n",
            "Session 304 skipped because next item sequence length is 1.\n",
            "Session 305 processed with 2 items.\n",
            "Session 306 processed with 7 items.\n",
            "Session 307 skipped because next item sequence length is 1.\n",
            "Session 308 processed with 3 items.\n",
            "Session 309 skipped because next item sequence length is 1.\n",
            "Session 310 skipped because next item sequence length is 0.\n",
            "Session 311 skipped because next item sequence length is 0.\n",
            "Session 312 processed with 2 items.\n",
            "Session 313 skipped because next item sequence length is 1.\n",
            "Session 314 skipped because next item sequence length is 0.\n",
            "Session 315 skipped because next item sequence length is 0.\n",
            "Session 316 skipped because next item sequence length is 1.\n",
            "Session 317 processed with 3 items.\n",
            "Session 318 processed with 2 items.\n",
            "Session 319 processed with 2 items.\n",
            "Session 320 skipped because next item sequence length is 1.\n",
            "Session 321 skipped because next item sequence length is 1.\n",
            "Session 322 processed with 3 items.\n",
            "Session 323 processed with 2 items.\n",
            "Session 324 skipped because next item sequence length is 0.\n",
            "Session 325 skipped because next item sequence length is 0.\n",
            "Session 326 skipped because next item sequence length is 1.\n",
            "Session 327 skipped because next item sequence length is 0.\n",
            "Session 328 skipped because next item sequence length is 0.\n",
            "Session 329 skipped because next item sequence length is 1.\n",
            "Session 330 skipped because next item sequence length is 0.\n",
            "Session 331 skipped because next item sequence length is 1.\n",
            "Session 332 skipped because next item sequence length is 0.\n",
            "Session 333 skipped because next item sequence length is 1.\n",
            "Session 334 skipped because next item sequence length is 1.\n",
            "Session 335 skipped because next item sequence length is 0.\n",
            "Session 336 skipped because next item sequence length is 0.\n",
            "Session 337 processed with 2 items.\n",
            "Session 338 processed with 2 items.\n",
            "Session 339 processed with 2 items.\n",
            "Session 340 processed with 6 items.\n",
            "Session 341 skipped because next item sequence length is 1.\n",
            "Session 342 processed with 4 items.\n",
            "Session 343 processed with 7 items.\n",
            "Session 344 processed with 3 items.\n",
            "Session 345 skipped because next item sequence length is 1.\n",
            "Session 346 skipped because next item sequence length is 1.\n",
            "Session 347 processed with 2 items.\n",
            "Session 348 processed with 2 items.\n",
            "Session 349 processed with 2 items.\n",
            "Session 350 skipped because next item sequence length is 0.\n",
            "Session 351 skipped because next item sequence length is 1.\n",
            "Session 352 skipped because next item sequence length is 0.\n",
            "Session 353 processed with 3 items.\n",
            "Session 354 skipped because next item sequence length is 0.\n",
            "Session 355 skipped because next item sequence length is 0.\n",
            "Session 356 skipped because next item sequence length is 0.\n",
            "Session 357 skipped because next item sequence length is 1.\n",
            "Session 358 skipped because next item sequence length is 0.\n",
            "Session 359 processed with 3 items.\n",
            "Session 360 skipped because next item sequence length is 1.\n",
            "Session 361 skipped because next item sequence length is 1.\n",
            "Session 362 skipped because next item sequence length is 1.\n",
            "Session 363 processed with 8 items.\n",
            "Session 364 skipped because next item sequence length is 0.\n",
            "Session 365 skipped because next item sequence length is 0.\n",
            "Session 366 processed with 2 items.\n",
            "Session 367 skipped because next item sequence length is 0.\n",
            "Session 368 skipped because next item sequence length is 1.\n",
            "Session 369 skipped because next item sequence length is 0.\n",
            "Session 370 skipped because next item sequence length is 1.\n",
            "Session 371 processed with 5 items.\n",
            "Session 372 skipped because next item sequence length is 0.\n",
            "Session 373 skipped because next item sequence length is 0.\n",
            "Session 374 skipped because next item sequence length is 1.\n",
            "Session 375 skipped because next item sequence length is 0.\n",
            "Session 376 processed with 4 items.\n",
            "Session 377 skipped because next item sequence length is 1.\n",
            "Session 378 processed with 4 items.\n",
            "Session 379 processed with 3 items.\n",
            "Session 380 skipped because next item sequence length is 1.\n",
            "Session 381 processed with 2 items.\n",
            "Session 382 processed with 2 items.\n",
            "Session 383 processed with 4 items.\n",
            "Session 384 skipped because next item sequence length is 1.\n",
            "Session 385 skipped because next item sequence length is 1.\n",
            "Session 386 processed with 4 items.\n",
            "Session 387 skipped because next item sequence length is 0.\n",
            "Session 388 processed with 2 items.\n",
            "Session 389 processed with 2 items.\n",
            "Session 390 skipped because next item sequence length is 0.\n",
            "Session 391 skipped because next item sequence length is 1.\n",
            "Session 392 skipped because next item sequence length is 1.\n",
            "Session 393 skipped because next item sequence length is 0.\n",
            "Session 394 skipped because next item sequence length is 0.\n",
            "Session 395 skipped because next item sequence length is 0.\n",
            "Session 396 skipped because next item sequence length is 0.\n",
            "Session 397 processed with 2 items.\n",
            "Session 398 skipped because next item sequence length is 0.\n",
            "Session 399 processed with 2 items.\n",
            "Session 400 skipped because next item sequence length is 0.\n",
            "Session 401 skipped because next item sequence length is 1.\n",
            "Session 402 processed with 4 items.\n",
            "Session 403 processed with 2 items.\n",
            "Session 404 skipped because next item sequence length is 0.\n",
            "Session 405 skipped because next item sequence length is 1.\n",
            "Session 406 skipped because next item sequence length is 1.\n",
            "Session 407 skipped because next item sequence length is 1.\n",
            "Session 408 skipped because next item sequence length is 0.\n",
            "Session 409 skipped because next item sequence length is 0.\n",
            "Session 410 processed with 6 items.\n",
            "Session 411 skipped because next item sequence length is 0.\n",
            "Session 412 processed with 5 items.\n",
            "Session 413 skipped because next item sequence length is 0.\n",
            "Session 414 skipped because next item sequence length is 0.\n",
            "Session 415 skipped because next item sequence length is 0.\n",
            "Session 416 processed with 4 items.\n",
            "Session 417 processed with 9 items.\n",
            "Session 418 skipped because next item sequence length is 0.\n",
            "Session 419 processed with 4 items.\n",
            "Session 420 processed with 2 items.\n",
            "Session 421 processed with 3 items.\n",
            "Session 422 processed with 2 items.\n",
            "Session 423 skipped because next item sequence length is 1.\n",
            "Session 424 skipped because next item sequence length is 0.\n",
            "Session 425 skipped because next item sequence length is 1.\n",
            "Session 426 skipped because next item sequence length is 0.\n",
            "Session 427 skipped because next item sequence length is 1.\n",
            "Session 428 skipped because next item sequence length is 0.\n",
            "Session 429 processed with 2 items.\n",
            "Session 430 skipped because next item sequence length is 0.\n",
            "Session 431 processed with 4 items.\n",
            "Session 432 processed with 3 items.\n",
            "Session 433 skipped because next item sequence length is 1.\n",
            "Session 434 skipped because next item sequence length is 0.\n",
            "Session 435 skipped because next item sequence length is 0.\n",
            "Session 436 skipped because next item sequence length is 0.\n",
            "Session 437 skipped because next item sequence length is 1.\n",
            "Session 438 processed with 2 items.\n",
            "Session 439 skipped because next item sequence length is 0.\n",
            "Session 440 processed with 4 items.\n",
            "Session 441 skipped because next item sequence length is 1.\n",
            "Session 442 skipped because next item sequence length is 1.\n",
            "Session 443 skipped because next item sequence length is 0.\n",
            "Session 444 skipped because next item sequence length is 0.\n",
            "Session 445 processed with 4 items.\n",
            "Session 446 processed with 2 items.\n",
            "Session 447 skipped because next item sequence length is 1.\n",
            "Session 448 processed with 2 items.\n",
            "Session 449 processed with 3 items.\n",
            "Session 450 processed with 3 items.\n",
            "Session 451 skipped because next item sequence length is 0.\n",
            "Session 452 skipped because next item sequence length is 0.\n",
            "Session 453 skipped because next item sequence length is 0.\n",
            "Session 454 processed with 9 items.\n",
            "Session 455 skipped because next item sequence length is 0.\n",
            "Session 456 skipped because next item sequence length is 1.\n",
            "Session 457 processed with 3 items.\n",
            "Session 458 skipped because next item sequence length is 1.\n",
            "Session 459 skipped because next item sequence length is 1.\n",
            "Session 460 skipped because next item sequence length is 0.\n",
            "Session 461 skipped because next item sequence length is 0.\n",
            "Session 462 processed with 2 items.\n",
            "Session 463 skipped because next item sequence length is 0.\n",
            "Session 464 processed with 4 items.\n",
            "Session 465 processed with 2 items.\n",
            "Session 466 processed with 2 items.\n",
            "Session 467 skipped because next item sequence length is 0.\n",
            "Session 468 processed with 5 items.\n",
            "Session 469 skipped because next item sequence length is 1.\n",
            "Session 470 skipped because next item sequence length is 0.\n",
            "Session 471 skipped because next item sequence length is 1.\n",
            "Session 472 processed with 2 items.\n",
            "Session 473 processed with 2 items.\n",
            "Session 474 skipped because next item sequence length is 1.\n",
            "Session 475 skipped because next item sequence length is 1.\n",
            "Session 476 skipped because next item sequence length is 1.\n",
            "Session 477 skipped because next item sequence length is 0.\n",
            "Session 478 processed with 4 items.\n",
            "Session 479 processed with 2 items.\n",
            "Session 480 skipped because next item sequence length is 1.\n",
            "Session 481 skipped because next item sequence length is 1.\n",
            "Session 482 skipped because next item sequence length is 1.\n",
            "Session 483 skipped because next item sequence length is 1.\n",
            "Session 484 skipped because next item sequence length is 0.\n",
            "Session 485 skipped because next item sequence length is 1.\n",
            "Session 486 skipped because next item sequence length is 0.\n",
            "Session 487 skipped because next item sequence length is 1.\n",
            "Session 488 skipped because next item sequence length is 0.\n",
            "Session 489 skipped because next item sequence length is 0.\n",
            "Session 490 processed with 4 items.\n",
            "Session 491 processed with 4 items.\n",
            "Session 492 skipped because next item sequence length is 1.\n",
            "Session 493 skipped because next item sequence length is 1.\n",
            "Session 494 skipped because next item sequence length is 0.\n",
            "Session 495 processed with 2 items.\n",
            "Session 496 skipped because next item sequence length is 1.\n",
            "Session 497 skipped because next item sequence length is 0.\n",
            "Session 498 processed with 3 items.\n",
            "Session 499 skipped because next item sequence length is 1.\n",
            "Session 500 skipped because next item sequence length is 1.\n",
            "Session 501 skipped because next item sequence length is 0.\n",
            "Session 502 processed with 3 items.\n",
            "Session 503 skipped because next item sequence length is 1.\n",
            "Session 504 skipped because next item sequence length is 1.\n",
            "Session 505 processed with 4 items.\n",
            "Session 506 skipped because next item sequence length is 0.\n",
            "Session 507 skipped because next item sequence length is 0.\n",
            "Session 508 skipped because next item sequence length is 1.\n",
            "Session 509 skipped because next item sequence length is 1.\n",
            "Session 510 skipped because next item sequence length is 0.\n",
            "Session 511 skipped because next item sequence length is 1.\n",
            "Session 512 skipped because next item sequence length is 0.\n",
            "Session 513 skipped because next item sequence length is 1.\n",
            "Session 514 skipped because next item sequence length is 0.\n",
            "Session 515 skipped because next item sequence length is 1.\n",
            "Session 516 skipped because next item sequence length is 1.\n",
            "Session 517 processed with 2 items.\n",
            "Session 518 processed with 2 items.\n",
            "Session 519 processed with 2 items.\n",
            "Session 520 processed with 3 items.\n",
            "Session 521 processed with 4 items.\n",
            "Session 522 processed with 4 items.\n",
            "Session 523 skipped because next item sequence length is 0.\n",
            "Session 524 skipped because next item sequence length is 0.\n",
            "Session 525 processed with 3 items.\n",
            "Session 526 skipped because next item sequence length is 0.\n",
            "Session 527 skipped because next item sequence length is 1.\n",
            "Session 528 skipped because next item sequence length is 1.\n",
            "Session 529 skipped because next item sequence length is 0.\n",
            "Session 530 processed with 3 items.\n",
            "Session 531 skipped because next item sequence length is 0.\n",
            "Session 532 processed with 2 items.\n",
            "Session 533 skipped because next item sequence length is 1.\n",
            "Session 534 processed with 2 items.\n",
            "Session 535 skipped because next item sequence length is 0.\n",
            "Session 536 skipped because next item sequence length is 1.\n",
            "Session 537 skipped because next item sequence length is 1.\n",
            "Session 538 skipped because next item sequence length is 0.\n",
            "Session 539 skipped because next item sequence length is 0.\n",
            "Session 540 skipped because next item sequence length is 0.\n",
            "Session 541 processed with 2 items.\n",
            "Session 542 skipped because next item sequence length is 0.\n",
            "Session 543 skipped because next item sequence length is 1.\n",
            "Session 544 skipped because next item sequence length is 1.\n",
            "Session 545 skipped because next item sequence length is 0.\n",
            "Session 546 processed with 4 items.\n",
            "Session 547 skipped because next item sequence length is 0.\n",
            "Session 548 skipped because next item sequence length is 1.\n",
            "Session 549 skipped because next item sequence length is 0.\n",
            "Session 550 processed with 5 items.\n",
            "Session 551 skipped because next item sequence length is 1.\n",
            "Session 552 processed with 2 items.\n",
            "Session 553 skipped because next item sequence length is 1.\n",
            "Session 554 skipped because next item sequence length is 1.\n",
            "Session 555 processed with 4 items.\n",
            "Session 556 skipped because next item sequence length is 0.\n",
            "Session 557 processed with 5 items.\n",
            "Session 558 processed with 3 items.\n",
            "Session 559 processed with 5 items.\n",
            "Session 560 processed with 2 items.\n",
            "Session 561 skipped because next item sequence length is 1.\n",
            "Session 562 skipped because next item sequence length is 1.\n",
            "Session 563 processed with 8 items.\n",
            "Session 564 skipped because next item sequence length is 0.\n",
            "Session 565 skipped because next item sequence length is 1.\n",
            "Session 566 skipped because next item sequence length is 1.\n",
            "Session 567 skipped because next item sequence length is 0.\n",
            "Session 568 processed with 2 items.\n",
            "Session 569 processed with 6 items.\n",
            "Session 570 processed with 3 items.\n",
            "Session 571 skipped because next item sequence length is 1.\n",
            "Session 572 skipped because next item sequence length is 0.\n",
            "Session 573 processed with 3 items.\n",
            "Session 574 skipped because next item sequence length is 0.\n",
            "Session 575 processed with 2 items.\n",
            "Session 576 skipped because next item sequence length is 1.\n",
            "Session 577 skipped because next item sequence length is 0.\n",
            "Session 578 processed with 2 items.\n",
            "Session 579 skipped because next item sequence length is 1.\n",
            "Session 580 skipped because next item sequence length is 0.\n",
            "Session 581 skipped because next item sequence length is 0.\n",
            "Session 582 skipped because next item sequence length is 0.\n",
            "Session 583 skipped because next item sequence length is 0.\n",
            "Session 584 processed with 3 items.\n",
            "Session 585 processed with 2 items.\n",
            "Session 586 processed with 2 items.\n",
            "Session 587 skipped because next item sequence length is 0.\n",
            "Session 588 skipped because next item sequence length is 0.\n",
            "Session 589 processed with 5 items.\n",
            "Session 590 processed with 2 items.\n",
            "Session 591 skipped because next item sequence length is 0.\n",
            "Session 592 skipped because next item sequence length is 0.\n",
            "Session 593 skipped because next item sequence length is 0.\n",
            "Session 594 processed with 5 items.\n",
            "Session 595 skipped because next item sequence length is 1.\n",
            "Session 596 skipped because next item sequence length is 1.\n",
            "Session 597 skipped because next item sequence length is 0.\n",
            "Session 598 skipped because next item sequence length is 0.\n",
            "Session 599 processed with 4 items.\n",
            "Session 600 skipped because next item sequence length is 1.\n",
            "Session 601 skipped because next item sequence length is 0.\n",
            "Session 602 processed with 3 items.\n",
            "Session 603 skipped because next item sequence length is 1.\n",
            "Session 604 processed with 4 items.\n",
            "Session 605 processed with 2 items.\n",
            "Session 606 skipped because next item sequence length is 0.\n",
            "Session 607 processed with 3 items.\n",
            "Session 608 skipped because next item sequence length is 0.\n",
            "Session 609 skipped because next item sequence length is 0.\n",
            "Session 610 skipped because next item sequence length is 0.\n",
            "Session 611 skipped because next item sequence length is 0.\n",
            "Session 612 skipped because next item sequence length is 1.\n",
            "Session 613 skipped because next item sequence length is 0.\n",
            "Session 614 skipped because next item sequence length is 1.\n",
            "Session 615 processed with 2 items.\n",
            "Session 616 processed with 5 items.\n",
            "Session 617 skipped because next item sequence length is 0.\n",
            "Session 618 skipped because next item sequence length is 1.\n",
            "Session 619 skipped because next item sequence length is 1.\n",
            "Session 620 skipped because next item sequence length is 0.\n",
            "Session 621 processed with 2 items.\n",
            "Session 622 processed with 2 items.\n",
            "Session 623 processed with 4 items.\n",
            "Session 624 skipped because next item sequence length is 0.\n",
            "Session 625 skipped because next item sequence length is 1.\n",
            "Session 626 processed with 3 items.\n",
            "Session 627 skipped because next item sequence length is 1.\n",
            "Session 628 processed with 5 items.\n",
            "Session 629 processed with 3 items.\n",
            "Session 630 skipped because next item sequence length is 0.\n",
            "Session 631 processed with 4 items.\n",
            "Session 632 processed with 5 items.\n",
            "Session 633 skipped because next item sequence length is 0.\n",
            "Session 634 processed with 2 items.\n",
            "Session 635 skipped because next item sequence length is 0.\n",
            "Session 636 skipped because next item sequence length is 1.\n",
            "Session 637 skipped because next item sequence length is 0.\n",
            "Session 638 processed with 3 items.\n",
            "Session 639 skipped because next item sequence length is 0.\n",
            "Session 640 skipped because next item sequence length is 1.\n",
            "Session 641 processed with 2 items.\n",
            "Session 642 skipped because next item sequence length is 1.\n",
            "Session 643 skipped because next item sequence length is 0.\n",
            "Session 644 skipped because next item sequence length is 0.\n",
            "Session 645 skipped because next item sequence length is 1.\n",
            "Session 646 skipped because next item sequence length is 1.\n",
            "Session 647 skipped because next item sequence length is 1.\n",
            "Session 648 processed with 2 items.\n",
            "Session 649 skipped because next item sequence length is 1.\n",
            "Session 650 skipped because next item sequence length is 1.\n",
            "Session 651 skipped because next item sequence length is 0.\n",
            "Session 652 skipped because next item sequence length is 1.\n",
            "Session 653 skipped because next item sequence length is 1.\n",
            "Session 654 skipped because next item sequence length is 0.\n",
            "Session 655 skipped because next item sequence length is 1.\n",
            "Session 656 skipped because next item sequence length is 1.\n",
            "Session 657 processed with 2 items.\n",
            "Session 658 processed with 4 items.\n",
            "Session 659 skipped because next item sequence length is 0.\n",
            "Session 660 skipped because next item sequence length is 0.\n",
            "Session 661 skipped because next item sequence length is 0.\n",
            "Session 662 processed with 3 items.\n",
            "Session 663 skipped because next item sequence length is 1.\n",
            "Session 664 skipped because next item sequence length is 0.\n",
            "Session 665 processed with 3 items.\n",
            "Session 666 processed with 3 items.\n",
            "Session 667 skipped because next item sequence length is 1.\n",
            "Session 668 skipped because next item sequence length is 0.\n",
            "Session 669 processed with 4 items.\n",
            "Session 670 skipped because next item sequence length is 1.\n",
            "Session 671 processed with 2 items.\n",
            "Session 672 skipped because next item sequence length is 1.\n",
            "Session 673 skipped because next item sequence length is 0.\n",
            "Session 674 skipped because next item sequence length is 1.\n",
            "Session 675 processed with 3 items.\n",
            "Session 676 skipped because next item sequence length is 1.\n",
            "Session 677 processed with 2 items.\n",
            "Session 678 skipped because next item sequence length is 0.\n",
            "Session 679 processed with 2 items.\n",
            "Session 680 skipped because next item sequence length is 1.\n",
            "Session 681 skipped because next item sequence length is 1.\n",
            "Session 682 processed with 2 items.\n",
            "Session 683 processed with 2 items.\n",
            "Session 684 processed with 3 items.\n",
            "Session 685 skipped because next item sequence length is 1.\n",
            "Session 686 skipped because next item sequence length is 0.\n",
            "Session 687 skipped because next item sequence length is 0.\n",
            "Session 688 processed with 3 items.\n",
            "Session 689 skipped because next item sequence length is 1.\n",
            "Session 690 skipped because next item sequence length is 0.\n",
            "Session 691 skipped because next item sequence length is 0.\n",
            "Session 692 processed with 2 items.\n",
            "Session 693 skipped because next item sequence length is 1.\n",
            "Session 694 processed with 2 items.\n",
            "Session 695 processed with 2 items.\n",
            "Session 696 processed with 2 items.\n",
            "Session 697 skipped because next item sequence length is 0.\n",
            "Session 698 processed with 3 items.\n",
            "Session 699 processed with 3 items.\n",
            "Session 700 skipped because next item sequence length is 0.\n",
            "Session 701 skipped because next item sequence length is 1.\n",
            "Session 702 skipped because next item sequence length is 1.\n",
            "Session 703 skipped because next item sequence length is 1.\n",
            "Session 704 skipped because next item sequence length is 0.\n",
            "Session 705 skipped because next item sequence length is 1.\n",
            "Session 706 skipped because next item sequence length is 0.\n",
            "Session 707 skipped because next item sequence length is 0.\n",
            "Session 708 processed with 3 items.\n",
            "Session 709 skipped because next item sequence length is 0.\n",
            "Session 710 skipped because next item sequence length is 1.\n",
            "Session 711 processed with 5 items.\n",
            "Session 712 processed with 6 items.\n",
            "Session 713 skipped because next item sequence length is 0.\n",
            "Session 714 processed with 2 items.\n",
            "Session 715 processed with 3 items.\n",
            "Session 716 processed with 2 items.\n",
            "Session 717 skipped because next item sequence length is 0.\n",
            "Session 718 skipped because next item sequence length is 1.\n",
            "Session 719 skipped because next item sequence length is 0.\n",
            "Session 720 processed with 4 items.\n",
            "Session 721 skipped because next item sequence length is 0.\n",
            "Session 722 processed with 2 items.\n",
            "Session 723 processed with 2 items.\n",
            "Session 724 skipped because next item sequence length is 0.\n",
            "Session 725 skipped because next item sequence length is 0.\n",
            "Session 726 skipped because next item sequence length is 0.\n",
            "Session 727 processed with 2 items.\n",
            "Session 728 skipped because next item sequence length is 0.\n",
            "Session 729 processed with 2 items.\n",
            "Session 730 skipped because next item sequence length is 1.\n",
            "Session 731 processed with 2 items.\n",
            "Session 732 processed with 5 items.\n",
            "Session 733 skipped because next item sequence length is 0.\n",
            "Session 734 skipped because next item sequence length is 1.\n",
            "Session 735 processed with 4 items.\n",
            "Session 736 skipped because next item sequence length is 1.\n",
            "Session 737 skipped because next item sequence length is 0.\n",
            "Session 738 processed with 2 items.\n",
            "Session 739 processed with 2 items.\n",
            "Session 740 processed with 2 items.\n",
            "Session 741 processed with 2 items.\n",
            "Session 742 skipped because next item sequence length is 0.\n",
            "Session 743 skipped because next item sequence length is 0.\n",
            "Session 744 skipped because next item sequence length is 1.\n",
            "Session 745 skipped because next item sequence length is 1.\n",
            "Session 746 skipped because next item sequence length is 0.\n",
            "Session 747 processed with 4 items.\n",
            "Session 748 skipped because next item sequence length is 0.\n",
            "Session 749 processed with 8 items.\n",
            "Session 750 skipped because next item sequence length is 0.\n",
            "Session 751 processed with 2 items.\n",
            "Session 752 processed with 2 items.\n",
            "Session 753 skipped because next item sequence length is 0.\n",
            "Session 754 skipped because next item sequence length is 1.\n",
            "Session 755 skipped because next item sequence length is 0.\n",
            "Session 756 skipped because next item sequence length is 1.\n",
            "Session 757 processed with 3 items.\n",
            "Session 758 processed with 2 items.\n",
            "Session 759 processed with 2 items.\n",
            "Session 760 skipped because next item sequence length is 0.\n",
            "Session 761 skipped because next item sequence length is 0.\n",
            "Session 762 skipped because next item sequence length is 1.\n",
            "Session 763 processed with 3 items.\n",
            "Session 764 skipped because next item sequence length is 0.\n",
            "Session 765 processed with 2 items.\n",
            "Session 766 processed with 2 items.\n",
            "Session 767 processed with 2 items.\n",
            "Session 768 skipped because next item sequence length is 0.\n",
            "Session 769 skipped because next item sequence length is 1.\n",
            "Session 770 processed with 5 items.\n",
            "Session 771 skipped because next item sequence length is 1.\n",
            "Session 772 skipped because next item sequence length is 0.\n",
            "Session 773 skipped because next item sequence length is 0.\n",
            "Session 774 processed with 2 items.\n",
            "Session 775 skipped because next item sequence length is 0.\n",
            "Session 776 skipped because next item sequence length is 0.\n",
            "Session 777 skipped because next item sequence length is 1.\n",
            "Session 778 skipped because next item sequence length is 0.\n",
            "Session 779 skipped because next item sequence length is 1.\n",
            "Session 780 skipped because next item sequence length is 0.\n",
            "Session 781 skipped because next item sequence length is 0.\n",
            "Session 782 skipped because next item sequence length is 1.\n",
            "Session 783 skipped because next item sequence length is 0.\n",
            "Session 784 skipped because next item sequence length is 1.\n",
            "Session 785 processed with 2 items.\n",
            "Session 786 skipped because next item sequence length is 0.\n",
            "Session 787 processed with 2 items.\n",
            "Session 788 processed with 2 items.\n",
            "Session 789 processed with 3 items.\n",
            "Session 790 skipped because next item sequence length is 1.\n",
            "Session 791 processed with 3 items.\n",
            "Session 792 processed with 5 items.\n",
            "Session 793 skipped because next item sequence length is 0.\n",
            "Session 794 processed with 2 items.\n",
            "Session 795 skipped because next item sequence length is 0.\n",
            "Session 796 skipped because next item sequence length is 1.\n",
            "Session 797 processed with 3 items.\n",
            "Session 798 skipped because next item sequence length is 1.\n",
            "Session 799 processed with 2 items.\n",
            "Session 800 skipped because next item sequence length is 0.\n",
            "Session 801 skipped because next item sequence length is 0.\n",
            "Session 802 skipped because next item sequence length is 1.\n",
            "Session 803 processed with 3 items.\n",
            "Session 804 skipped because next item sequence length is 0.\n",
            "Session 805 processed with 3 items.\n",
            "Session 806 processed with 2 items.\n",
            "Session 807 skipped because next item sequence length is 1.\n",
            "Session 808 skipped because next item sequence length is 0.\n",
            "Session 809 skipped because next item sequence length is 1.\n",
            "Session 810 processed with 2 items.\n",
            "Session 811 skipped because next item sequence length is 0.\n",
            "Session 812 processed with 4 items.\n",
            "Session 813 processed with 5 items.\n",
            "Session 814 processed with 3 items.\n",
            "Session 815 skipped because next item sequence length is 1.\n",
            "Total processed items: 960\n"
          ]
        }
      ],
      "source": [
        "test_dataset = preprocessor.get_test_data(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaFqUvobau9r",
        "outputId": "737d98d4-0530-411a-c356-35e294c71045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "Items (SongID): [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 1492  187 1355 2435 2786    5   71 2615  560\n",
            "   986]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0  790  678 1864 1394  290  720 1585 2547 1587 1038 1045 1307\n",
            "  2227]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 1085 2167 2182   43 1567 1604 1317  379 2281\n",
            "  2321]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 1774 2686 1585 1303 1497 1033  293 2084 2239\n",
            "  2250]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2245 2768  226  721  717  144  487 1354 1262 2503\n",
            "  1770]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0  538 2300 1077 1645 2084 2114  206 2547 2135\n",
            "  2239]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 2297 1860  743  794 2385 1656 2186 2747  856 2146 2551 2553\n",
            "  1001]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0 1521 1886 1392 1553  386 1492 2225 2296\n",
            "  1995]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0   89  336  679 1651 2345 1888 1428  434 1409\n",
            "  1799]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0  313 1388 1543 1774  678  916  216 2758 1573 1853   77  712 1422\n",
            "  1301]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 1062 1120 2552  394 1933 1615 2339 2259  845\n",
            "  1230]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 1052 2079 2450 2387 2206 2092  518  545  705  332  154 2081  678\n",
            "   645]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0  660 1567 2339 2297 1656 1436  969  157 1687 1591  846  626 1052\n",
            "  2079]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0 1436 1733  885\n",
            "  1663 1670 1026  210  941   57 2106 2135  206   99 2480 2106 2546 1225\n",
            "  1154]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0  926 2480   99 2546  297 2160   52 1219 2311 1267  733 1555\n",
            "  1770]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2229  645  366 2150   88  403 1541 2738   95 2745\n",
            "  2559]]\n",
            "Genre: [[[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  5  78  91 ... 547 606   0]\n",
            "  [262 375 486 ...   0   0   0]\n",
            "  [  8  30  69 ... 234 274 520]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  8  30 135 ...   0   0   0]\n",
            "  [ 14  17 148 ... 531 532 545]\n",
            "  [314 546 547 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [252 521 547 ...   0   0   0]\n",
            "  [ 10  18 171 ... 525 548 550]\n",
            "  [264   0   0 ...   0   0   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [126   0   0 ...   0   0   0]\n",
            "  [165 218 491 ...   0   0   0]\n",
            "  [ 14 267 274 ... 671   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [ 36   0   0 ...   0   0   0]\n",
            "  [ 11 167 440 ...   0   0   0]\n",
            "  [536 537   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [491 496   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [566   0   0 ...   0   0   0]]]\n",
            "Features: []\n",
            "Next Items (Next SongID): [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0  187 1355 2435 2786    5   71 2615  560  986\n",
            "    93]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0  678 1864 1394  290  720 1585 2547 1587 1038 1045 1307 2227\n",
            "  1850]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2167 2182   43 1567 1604 1317  379 2281 2321\n",
            "  1449]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2686 1585 1303 1497 1033  293 2084 2239 2250\n",
            "  2114]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0 2768  226  721  717  144  487 1354 1262 2503 1770\n",
            "  2486]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 2300 1077 1645 2084 2114  206 2547 2135 2239\n",
            "  1539]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 1860  743  794 2385 1656 2186 2747  856 2146 2551 2553 1001\n",
            "  2434]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0 1886 1392 1553  386 1492 2225 2296 1995\n",
            "   820]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0  336  679 1651 2345 1888 1428  434 1409 1799\n",
            "  1220]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 1388 1543 1774  678  916  216 2758 1573 1853   77  712 1422 1301\n",
            "  1661]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0 1120 2552  394 1933 1615 2339 2259  845 1230\n",
            "  2214]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 2079 2450 2387 2206 2092  518  545  705  332  154 2081  678  645\n",
            "  1381]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 1567 2339 2297 1656 1436  969  157 1687 1591  846  626 1052 2079\n",
            "  2450]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0 1733  885 1663\n",
            "  1670 1026  210  941   57 2106 2135  206   99 2480 2106 2546 1225 1154\n",
            "  1397]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 2480   99 2546  297 2160   52 1219 2311 1267  733 1555 1770\n",
            "  2383]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0  645  366 2150   88  403 1541 2738   95 2745 2559\n",
            "  2824]]\n",
            "Next Genre: [[[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [262 375 486 ...   0   0   0]\n",
            "  [  8  30  69 ... 234 274 520]\n",
            "  [116 175 295 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [ 14  17 148 ... 531 532 545]\n",
            "  [314 546 547 ...   0   0   0]\n",
            "  [  8  68  69 ... 545 618   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [ 10  18 171 ... 525 548 550]\n",
            "  [264   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [165 218 491 ...   0   0   0]\n",
            "  [ 14 267 274 ... 671   0   0]\n",
            "  [149 159   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [ 11 167 440 ...   0   0   0]\n",
            "  [536 537   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [566   0   0 ...   0   0   0]\n",
            "  [349   0   0 ...   0   0   0]]]\n",
            "<_PrefetchDataset element_spec={'item': TensorSpec(shape=(16, 43), dtype=tf.int32, name=None), 'genre': TensorSpec(shape=(16, 43, 10), dtype=tf.int32, name=None), 'features': TensorSpec(shape=(16, 43, 0), dtype=tf.float32, name=None), 'next_item': TensorSpec(shape=(16, 43), dtype=tf.int32, name=None), 'next_genre': TensorSpec(shape=(16, 43, 10), dtype=tf.int32, name=None)}>\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset))\n",
        "for batch in dataset.take(1):\n",
        "    print(\"Items (SongID):\", batch['item'].numpy())\n",
        "    print(\"Genre:\", batch['genre'].numpy())\n",
        "    print(\"Features:\", batch['features'].numpy())\n",
        "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())\n",
        "    print(\"Next Genre:\", batch['next_genre'].numpy())\n",
        "    # print(batch.type())\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "MalfdduKbjlB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "462296f9-acfe-441d-ba46-a4e7f6e13d3a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-0cdb1c1c81a3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Items (SongID):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Genre:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Features:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Next Items (Next SongID):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "for batch in test_dataset.take(1):\n",
        "    print(\"Items (SongID):\", batch['item'].numpy())\n",
        "    print(\"Genre:\", batch['genre'].numpy())\n",
        "    print(\"Features:\", batch['features'].numpy())\n",
        "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1s8zV6au9r"
      },
      "source": [
        "## Define Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "urRmM_jCau9r"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\n",
        "class ItemEmbedding(Layer):\n",
        "    def __init__(self, num_items, item_embed_dim):\n",
        "        super(ItemEmbedding, self).__init__()\n",
        "\n",
        "        self.item_embedding = Embedding(input_dim=num_items, output_dim=item_embed_dim, mask_zero=True)\n",
        "\n",
        "    def call(self, items):\n",
        "        # Embed items\n",
        "        items_embedded = self.item_embedding(items)\n",
        "        return items_embedded\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\n",
        "class GRU4REC(Model):\n",
        "    def __init__(self, rnn_params, genre_embed_dim, item_embed_dim, ffn1_units, feature_dense_units,  preprocessed_data:DataPreprocessor):\n",
        "        super(GRU4REC, self).__init__()\n",
        "        print(f\"items size: {preprocessed_data.items_size}\")\n",
        "        print(f\"genres size: {preprocessed_data.genres_size}\")\n",
        "        self.embedding = ItemEmbedding(preprocessed_data.items_size, item_embed_dim)\n",
        "\n",
        "        # Genre embedding (only for genre, which is categorical and a string)\n",
        "        self.genre_embedding = Embedding(input_dim=preprocessed_data.genres_size, output_dim=genre_embed_dim, mask_zero=True, name='genre_embedding')\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn_layers = []\n",
        "        self.rnn_layers.append(GRU(**rnn_params[0], return_sequences=True))\n",
        "        for i in range(1, len(rnn_params) - 1):\n",
        "            self.rnn_layers.append(GRU(**rnn_params[i], return_sequences=True))\n",
        "        self.rnn_layers.append(GRU(**rnn_params[-1], return_sequences=True))\n",
        "\n",
        "        self.concat = Concatenate(axis=-1, name='concat_1')\n",
        "        self.batch_norm = BatchNormalization(name='batchnorm')\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = Dropout(0.2, name='dropout')\n",
        "\n",
        "        # Feed-forward layers\n",
        "        self.feature_dense = Dense(feature_dense_units, activation='relu', name='feature_dense')  # Dense layer for features (if required)\n",
        "        self.ffn1 = Dense(ffn1_units, name='ffn_1')\n",
        "        self.activation1 = LeakyReLU(negative_slope=0.2, name='freaky_relu')\n",
        "        self.item_output = Dense(preprocessed_data.items_size, activation='softmax', name='item_output')\n",
        "        # self.genre_output = Dense(preprocessed_data.genres_size, activation='softmax', name='genre_output')\n",
        "\n",
        "        self.attention = Attention(use_scale=False, dropout=0.2, name='attention')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward pass for the GRU4REC model.\n",
        "        :param inputs: Tuple (item_sequences, item_features, item_genres)\n",
        "        :param training: Boolean indicating if the model is in training mode\n",
        "        \"\"\"\n",
        "        item_sequences, item_features, item_genres = inputs\n",
        "        encoding_padding_mask = tf.math.logical_not(tf.math.equal(item_sequences, 0))\n",
        "\n",
        "        print(\"Item Sequence Shape:\", item_sequences.shape)\n",
        "        print(\"Item Genres Shape:\", item_genres.shape)\n",
        "        # Embed items\n",
        "        item_embedded = self.embedding(item_sequences)\n",
        "        item_embedded = tf.expand_dims(item_embedded, axis=2)\n",
        "        # Genre embedding\n",
        "        genre_embedded = self.genre_embedding(item_genres)\n",
        "\n",
        "        print(\"Item Embedded Shape:\", item_embedded.shape)\n",
        "        print(\"Genre Embedded Shape:\", genre_embedded.shape)\n",
        "        # genre_embedded = tf.reduce_mean(genre_embedded, axis=1)\n",
        "        # genre_embedded = tf.expand_dims(genre_embedded, axis=1)\n",
        "\n",
        "        # Feature transformation (features are passed directly as floats, so no embedding is needed)\n",
        "        # feature_transformed = self.feature_dense(item_features)\n",
        "        # feature_transformed = tf.expand_dims(feature_transformed, axis=1)\n",
        "\n",
        "        # combined_input = tf.concat([item_embedded, feature_transformed, genre_embedded], axis=-1)\n",
        "        combined_input = tf.concat([item_embedded, genre_embedded], axis=-2)\n",
        "        combined_input = self.batch_norm(combined_input)\n",
        "        # Pass through RNN layers\n",
        "        x = combined_input\n",
        "        x = self.rnn_layers[0](x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(1, len(self.rnn_layers)):\n",
        "            x = self.concat([combined_input, x])  # Concatenate item embeddings with RNN outputs\n",
        "            x = self.rnn_layers[i](x)\n",
        "            x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.batch_norm(x)\n",
        "\n",
        "        # Give attention\n",
        "        expanded_mask = tf.expand_dims(encoding_padding_mask, axis=1)\n",
        "        # print(f\"Shape before attention: {x.shape}\")\n",
        "        x = self.attention(inputs=[x,x], mask=[expanded_mask, expanded_mask], use_causal_mask=True)\n",
        "\n",
        "        # Feed-forward layers\n",
        "        # print(f\"Shape before squeeze: {x.shape}\")\n",
        "        # x = tf.squeeze(x, axis=1)\n",
        "        # print(f\"Shape before softmax: {x.shape}\")\n",
        "        x = self.ffn1(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.activation1(x)\n",
        "        # print(f\"Shape after activation: {x.shape}\")\n",
        "        item_logits = self.item_output(x)  # Item prediction\n",
        "        # print(f\"Output shape: {item_logits.shape}\")\n",
        "\n",
        "        return item_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0qU_Hpsau9s"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "2_6LBqBlau9s"
      },
      "outputs": [],
      "source": [
        "import keras.api\n",
        "from keras.api.metrics import Recall\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\n",
        "class RecallAtK(tf.keras.metrics.Metric):\n",
        "    def __init__(self, k=10, name=\"recall_at_k\", **kwargs):\n",
        "        super(RecallAtK, self).__init__(name=name, **kwargs)\n",
        "        self.k = k\n",
        "        self.recall_at_k = Recall(top_k=self.k)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Update the state of the metric.\n",
        "        \"\"\"\n",
        "        # Since y_true is a list of true items and y_pred are the predicted scores,\n",
        "        # we need to calculate recall for top-k predicted items\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "\n",
        "        # Calculate the top-k predicted items\n",
        "        top_k_preds = tf.argsort(y_pred, axis=-1, direction='DESCENDING')[:, :self.k]\n",
        "\n",
        "        # Calculate recall by comparing true labels with the top-k predictions\n",
        "        recall = tf.reduce_mean(tf.cast(tf.equal(y_true, top_k_preds), tf.float32), axis=-1)\n",
        "        return recall\n",
        "\n",
        "    def result(self):\n",
        "        return self.recall_at_k.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.recall_at_k.reset_state()\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\n",
        "class GRU4RECLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, item_loss_weight=1.0, genre_loss_weight=1.0, name=\"gru4rec_loss\"):\n",
        "        super(GRU4RECLoss, self).__init__(name=name)\n",
        "        self.item_loss_weight = item_loss_weight\n",
        "        self.genre_loss_weight = genre_loss_weight\n",
        "        self.categorical_crossentropy = keras.api.losses.CategoricalCrossentropy()\n",
        "        self.sparse_categorical_crossentropy = keras.api.losses.SparseCategoricalCrossentropy()\n",
        "        self.binary_crossentropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    def sparse_to_multi_hot(self, true_genres, num_genres):\n",
        "        \"\"\"\n",
        "        Converts sparse label encoded genres into multi-hot encoded vectors.\n",
        "\n",
        "        :param true_genres: Tensor of shape (batch_size, num_labels) with sparse integer labels\n",
        "        :param num_genres: Total number of genres (the size of the multi-hot vector)\n",
        "        :return: Multi-hot encoded tensor of shape (batch_size, num_genres)\n",
        "        \"\"\"\n",
        "        ## Create a tensor of zeros with shape (batch_size, num_genres)\n",
        "        batch_size = tf.shape(true_genres)[0]\n",
        "        multi_hot = tf.zeros((batch_size, num_genres), dtype=tf.float32)\n",
        "\n",
        "        # Flatten the batch for indexing\n",
        "        indices = tf.reshape(true_genres, [-1])  # Flatten true_genres to a 1D tensor\n",
        "        updates = tf.ones_like(indices, dtype=tf.float32)  # Create a 1D tensor of ones\n",
        "        batch_indices = tf.repeat(tf.range(batch_size), tf.shape(true_genres)[1])  # Batch indices for each label\n",
        "\n",
        "        # Combine batch and label indices\n",
        "        scatter_indices = tf.stack([batch_indices, indices], axis=1)\n",
        "\n",
        "        # Update the multi-hot tensor\n",
        "        multi_hot = tf.tensor_scatter_nd_update(multi_hot, scatter_indices, updates)\n",
        "\n",
        "        return multi_hot\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute the total loss.\n",
        "        :param y_true: A tuple (true_items, true_genres)\n",
        "        :param y_pred: A tuple (predicted_items, predicted_genres)\n",
        "        \"\"\"\n",
        "        true_items, true_genres = y_true\n",
        "        pred_items, pred_genres = y_pred\n",
        "\n",
        "        # Calculate item loss\n",
        "        item_loss = self.sparse_categorical_crossentropy(true_items, pred_items)\n",
        "\n",
        "        true_genres = tf.cast(true_genres, tf.int32)\n",
        "        num_genres = pred_genres.shape[1]\n",
        "\n",
        "        true_genres_multi_hot = self.sparse_to_multi_hot(true_genres, num_genres)\n",
        "\n",
        "\n",
        "        # Calculate genre loss\n",
        "        genre_loss = self.binary_crossentropy(true_genres_multi_hot, pred_genres)\n",
        "\n",
        "        total_loss = self.item_loss_weight * item_loss + self.genre_loss_weight * genre_loss\n",
        "        return total_loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(batch, loss_fn, model, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        item_logits = model((batch['item'], batch['features'], batch['genre']), training=True)\n",
        "        loss = loss_fn(batch['next_item'], item_logits)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "def train_gru4rec(model, train_dataset, optimizer, epochs, k, val_dataset=None):\n",
        "    # metric = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n",
        "\n",
        "    loss_fn = keras.api.losses.SparseCategoricalCrossentropy()\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "    val_metric_history = []\n",
        "    metric_history = []\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for step, batch in enumerate(train_dataset):\n",
        "            loss = train_step(batch, loss_fn, model, optimizer)\n",
        "            if loss is None:\n",
        "              print(f\"Warning: train_step returned None at step {step}\")\n",
        "              continue\n",
        "            epoch_loss += loss.numpy()\n",
        "\n",
        "            # Update metric\n",
        "            # metric.update_state(batch['next_item'], logits)\n",
        "\n",
        "        print(f\"Training Loss: {epoch_loss / (step + 1):.4f}\")\n",
        "        loss_history.append(epoch_loss / (step + 1))\n",
        "        # metric_history.append(metric.result().numpy())\n",
        "        # metric.reset_state()\n",
        "\n",
        "        if val_dataset:\n",
        "            val_loss = 0.0\n",
        "            for step, batch in enumerate(val_dataset):\n",
        "                item_logits, genre_logits = model((batch['item'], batch['features'], batch['genre']), training=False)\n",
        "                val_loss += loss_fn((batch['next_item'], batch['next_genre']), (item_logits, genre_logits)).numpy()\n",
        "            print(f\"Validation Loss: {val_loss / (step + 1):.4f}\")\n",
        "            val_loss_history.append(val_loss / (step + 1))\n",
        "    return {\n",
        "        'loss_history': loss_history,\n",
        "        'metric_history': metric_history,\n",
        "        'val_loss_history': val_loss_history,\n",
        "        'val_metric_history': val_metric_history\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run with strategy"
      ],
      "metadata": {
        "id": "8vN2c_coYWs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    model = GRU4REC(\n",
        "        rnn_params=[\n",
        "          {\"units\": 128},\n",
        "          {\"units\": 128},\n",
        "          {\"units\": 64}\n",
        "        ],\n",
        "        item_embed_dim=32,\n",
        "        genre_embed_dim=32,\n",
        "        ffn1_units=256,\n",
        "        feature_dense_units=16,\n",
        "        preprocessed_data=preprocessor\n",
        "    )\n",
        "    optimizer = keras.api.optimizers.Adam(learning_rate=0.001)\n",
        "    loss_fn = keras.api.losses.SparseCategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "Qd0w08zYC9oJ",
        "outputId": "324f1cc5-8b00-49d9-b775-0d5612ebebb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "items size: 2864\n",
            "genres size: 675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "@tf.function\n",
        "def distributed_train_step(batch, loss_fn, model, optimizer):\n",
        "    def step_fn(batch):\n",
        "      with tf.GradientTape() as tape:\n",
        "          item_logits = model((batch['item'], batch['features'], batch['genre']), training=True)\n",
        "          loss = loss_fn(batch['next_item'], item_logits)\n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "      return loss\n",
        "\n",
        "    per_replica_losses = strategy.run(step_fn, args=(batch,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "@tf.function\n",
        "def distributed_val_step(batch, loss_fn, model):\n",
        "    def step_fn(batch):\n",
        "        item_logits, genre_logits = model((batch['item'], batch['features'], batch['genre']), training=False)\n",
        "        loss = loss_fn((batch['next_item'], batch['next_genre']), (item_logits, genre_logits))\n",
        "        return loss\n",
        "\n",
        "    per_replica_losses = strategy.run(step_fn, args=(batch,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "def train_gru4rec_with_strategy(model, train_dataset, optimizer, epochs, k, val_dataset=None, early_stopping=None):\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "    val_metric_history = []\n",
        "    metric_history = []\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(train_dataset):\n",
        "            loss = distributed_train_step(batch, loss_fn, model, optimizer)\n",
        "            if loss is None:\n",
        "              print(f\"Warning: train_step returned None at step {step}\")\n",
        "              continue\n",
        "            epoch_loss += loss.numpy()\n",
        "\n",
        "        print(f\"Training Loss: {epoch_loss / (step + 1):.4f}\")\n",
        "        loss_history.append(epoch_loss / (step + 1))\n",
        "        if val_dataset:\n",
        "            val_loss = 0.0\n",
        "            for step, batch in enumerate(val_dataset):\n",
        "                val_loss += distributed_val_step(batch, loss_fn, model).numpy()\n",
        "            print(f\"Validation Loss: {val_loss / (step + 1):.4f}\")\n",
        "            val_loss_history.append(val_loss / (step + 1))\n",
        "\n",
        "    return {\n",
        "        'loss_history': loss_history,\n",
        "        'metric_history': metric_history,\n",
        "        'val_loss_history': val_loss_history,\n",
        "        'val_metric_history': val_metric_history\n",
        "    }\n"
      ],
      "metadata": {
        "id": "mfG75HwPYYI8"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train with distributed Training"
      ],
      "metadata": {
        "id": "nfj1AZ1KaFxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api.callbacks import EarlyStopping\n",
        "# from keras.api.optimizers import Adam\n",
        "\n",
        "history = train_gru4rec_with_strategy(model, dataset, optimizer, epochs=100, k=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q_1BbWZIaLfQ",
        "outputId": "de3bebd4-74ea-4845-d6db-3c0bc2618964"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Item Sequence Shape: (16, 43)\n",
            "Item Genres Shape: (16, 43, 10)\n",
            "Item Embedded Shape: (16, 43, 1, 32)\n",
            "Genre Embedded Shape: (16, 43, 10, 32)\n",
            "Item Sequence Shape: (16, 43)\n",
            "Item Genres Shape: (16, 43, 10)\n",
            "Item Embedded Shape: (16, 43, 1, 32)\n",
            "Genre Embedded Shape: (16, 43, 10, 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1383: UserWarning: Layer 'gru4rec_15' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Input 0 of layer \"gru_45\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (16, 43, 11, 32)''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'gru4rec_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"<ipython-input-128-cecd1f390a67>\", line 5, in step_fn  *\n        item_logits = model((batch['item'], batch['features'], batch['genre']), training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"<ipython-input-126-fe0087d571ca>\", line 77, in call\n        x = self.rnn_layers[0](x)\n\n    ValueError: Exception encountered when calling GRU4REC.call().\n    \n    \u001b[1mInput 0 of layer \"gru_45\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (16, 43, 11, 32)\u001b[0m\n    \n    Arguments received by GRU4REC.call():\n      • inputs=('tf.Tensor(shape=(16, 43), dtype=int32)', 'tf.Tensor(shape=(16, 43, 0), dtype=float32)', 'tf.Tensor(shape=(16, 43, 10), dtype=int32)')\n      • training=True\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-3f0c060a9748>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from keras.api.optimizers import Adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gru4rec_with_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-128-cecd1f390a67>\u001b[0m in \u001b[0;36mtrain_gru4rec_with_strategy\u001b[0;34m(model, train_dataset, optimizer, epochs, k, val_dataset, early_stopping)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Warning: train_step returned None at step {step}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filek1a_3jkj.py\u001b[0m in \u001b[0;36mtf__distributed_train_step\u001b[0;34m(batch, loss_fn, model, optimizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m                             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval__1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mper_replica_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filek1a_3jkj.py\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mretval__1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                             \u001b[0mitem_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-fe0087d571ca>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Pass through RNN layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-128-cecd1f390a67>\", line 5, in step_fn  *\n        item_logits = model((batch['item'], batch['features'], batch['genre']), training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"<ipython-input-126-fe0087d571ca>\", line 77, in call\n        x = self.rnn_layers[0](x)\n\n    ValueError: Exception encountered when calling GRU4REC.call().\n    \n    \u001b[1mInput 0 of layer \"gru_45\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (16, 43, 11, 32)\u001b[0m\n    \n    Arguments received by GRU4REC.call():\n      • inputs=('tf.Tensor(shape=(16, 43), dtype=int32)', 'tf.Tensor(shape=(16, 43, 0), dtype=float32)', 'tf.Tensor(shape=(16, 43, 10), dtype=int32)')\n      • training=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RASvcPZbau9t"
      },
      "source": [
        "## Run the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Don't run this if you have run using the distributed training"
      ],
      "metadata": {
        "id": "Haly4qb9uuzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSkPj14Jau9u"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "# num_items = len()\n",
        "# feature_vocab_size = len(feature_columns)\n",
        "\n",
        "\n",
        "# model = GRU4REC(\n",
        "#     rnn_params=[\n",
        "#         {\"units\": 128},\n",
        "#         {\"units\": 128},\n",
        "#         {\"units\": 64}\n",
        "#     ],\n",
        "#     item_embed_dim=32,\n",
        "#     genre_embed_dim=32,\n",
        "#     ffn1_units=256,\n",
        "#     feature_dense_units=16,\n",
        "#     preprocessed_data=preprocessor\n",
        "# )\n",
        "\n",
        "# print(model.trainable_variables)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlczKLIHau9v"
      },
      "outputs": [],
      "source": [
        "# # Compile the model\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "# # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# # Train the model\n",
        "# train_gru4rec(model=model, train_dataset=dataset,optimizer=optimizer, epochs=500, k=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmy4kH_bau9v",
        "outputId": "a43ccf20-8f5c-458a-df54-8df1827b206f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 recall: 0.7500\n",
            "Batch 2 recall: 0.8750\n",
            "Batch 3 recall: 1.0000\n",
            "Batch 4 recall: 1.0000\n",
            "Overall recall: 0.9062\n"
          ]
        }
      ],
      "source": [
        "def predict(model, item_sequences, item_features, item_genres):\n",
        "    \"\"\"\n",
        "    Predict the item with the highest probability for the given input sequences using argmax of softmax.\n",
        "\n",
        "    Args:\n",
        "    - model: The trained model.\n",
        "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
        "    - item_features: Input item features (batch_size, feature_length).\n",
        "    - item_genres: Input item genres (batch_size, genre_length).\n",
        "\n",
        "    Returns:\n",
        "    - predicted_items: A list of predicted items with the highest probability for each input sequence.\n",
        "    \"\"\"\n",
        "    # Run the model in inference mode (not training)\n",
        "    logits = model((item_sequences, item_features, item_genres), training=False)\n",
        "\n",
        "    # Apply softmax to the logits to get probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Get the item with the highest probability by finding the index of the maximum probability\n",
        "    predicted_items = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n",
        "\n",
        "    # Convert to numpy array for easier handling\n",
        "    predicted_items = predicted_items.numpy()\n",
        "\n",
        "    return predicted_items\n",
        "\n",
        "def compute_recall(predicted_items, targets):\n",
        "    \"\"\"\n",
        "    Compute the recall for the given predictions and targets.\n",
        "\n",
        "    Args:\n",
        "    - predicted_items: The predicted items (batch_size,).\n",
        "    - targets: The actual next items (batch_size,).\n",
        "\n",
        "    Returns:\n",
        "    - recall: The recall metric.\n",
        "    \"\"\"\n",
        "    # True positives: Predicted item matches the target\n",
        "    true_positives = np.sum(predicted_items == targets)\n",
        "\n",
        "    # Total relevant items (in this case, it is the number of items in the batch)\n",
        "    total_items = len(targets)\n",
        "\n",
        "    # Recall calculation\n",
        "    recall = true_positives / total_items if total_items > 0 else 0\n",
        "    return recall\n",
        "\n",
        "# Initialize variables to calculate overall recall\n",
        "total_true_positives = 0\n",
        "total_items = 0\n",
        "\n",
        "# Loop through training dataset and predict the most probable item\n",
        "for step, batch in enumerate(dataset):\n",
        "    item_sequences = batch['item']\n",
        "    item_genres = batch['genre']\n",
        "    item_features = batch['features']\n",
        "    targets = batch['next_item']\n",
        "\n",
        "    # Get the predicted item with the highest probability for each sequence in the batch\n",
        "    predicted_items = predict(model, item_sequences, item_features, item_genres)\n",
        "\n",
        "    # Compute recall for the current batch\n",
        "    batch_recall = compute_recall(predicted_items, targets)\n",
        "    print(f\"Batch {step + 1} recall: {batch_recall:.4f}\")\n",
        "\n",
        "    # Accumulate for overall recall\n",
        "    total_true_positives += np.sum(predicted_items == targets)\n",
        "    total_items += len(targets)\n",
        "\n",
        "# Calculate overall recall\n",
        "overall_recall = total_true_positives / total_items if total_items > 0 else 0\n",
        "print(f\"Overall recall: {overall_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KyEfocm_yBRR"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_single_session(\n",
        "    session,\n",
        "    feature_columns,\n",
        "    k=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Preprocess a single session into TensorFlow dataset with split genre and features.\n",
        "\n",
        "    Args:\n",
        "    - session (list): A list of dictionaries containing session data.\n",
        "    - feature_columns (list): List of numerical feature column names.\n",
        "    - mean_values (dict): Mean values for numerical features for normalization.\n",
        "    - std_values (dict): Std values for numerical features for normalization.\n",
        "    - k (int): Minimum length of `next_item_sequences`.\n",
        "\n",
        "    Returns:\n",
        "    - tf.data.Dataset: TensorFlow dataset containing preprocessed data.\n",
        "    \"\"\"\n",
        "    item_sequences = []\n",
        "    next_item_sequences = []\n",
        "    genre_sequences = []\n",
        "    feature_sequences = []\n",
        "\n",
        "    for i in range(len(session) - 1):\n",
        "        # Process items\n",
        "        session_item_encoded = preprocessor.preprocess_song_id(session[i]['SongID'])\n",
        "        next_session_item_encoded = preprocessor.preprocess_song_id(session[i + 1]['SongID'])\n",
        "        item_sequences.append(session_item_encoded)\n",
        "        next_item_sequences.append(next_session_item_encoded)\n",
        "\n",
        "        # Process genre\n",
        "        genre_cleaned = preprocessor.clean_genre(session[i].get('spotify_genre', None))\n",
        "        genre_sequences.append(genre_cleaned)\n",
        "\n",
        "        # Process numerical features\n",
        "        numeric_features = []\n",
        "        for col in feature_columns:\n",
        "            if col != 'spotify_genre':\n",
        "                mean = preprocessor.mean_values.get(col, None)\n",
        "                std = preprocessor.std_values.get(col, None)\n",
        "                cleaned_feature = preprocessor.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n",
        "                numeric_features.append(cleaned_feature)\n",
        "\n",
        "        feature_sequences.append(numeric_features)\n",
        "\n",
        "    # Filter session if next_item_sequences length is not greater than k\n",
        "    if len(next_item_sequences) <= k:\n",
        "        print(f\"Session skipped because next item sequence length is {len(next_item_sequences)}.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processed session with {len(item_sequences)} items.\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    item_sequences = tf.stack(item_sequences, axis=0)\n",
        "    next_item_sequences = tf.stack(next_item_sequences, axis=0)\n",
        "    genre_sequences_tensor = tf.constant(genre_sequences, dtype=tf.int32)\n",
        "    feature_sequences_tensor = tf.constant(feature_sequences, dtype=tf.float32)\n",
        "\n",
        "    # Create TensorFlow dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        'item': item_sequences,\n",
        "        'genre': genre_sequences_tensor,\n",
        "        'features': feature_sequences_tensor,\n",
        "        'next_item': next_item_sequences\n",
        "    })\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note: Pakai Cara 3 (itu yang bener)"
      ],
      "metadata": {
        "id": "r37Q_7lRu5_O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzF4yZg47oey"
      },
      "source": [
        "## Cara 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZAs7Kq0au9w",
        "outputId": "d2c3c328-2726-4dde-ee9b-933c4fb89af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating session dataset\n",
            "Creating tensor dataset\n",
            "Session skipped because next item sequence length is 10.\n",
            "Session skipped because next item sequence length is 10.\n",
            "Processed session with 12 items.\n",
            "Predicted Sequence: ['Sirens (feat. Dirty Heads)Sublime with Rome' 'StonedSmash Mouth'\n",
            " 'Burn the House DownAJR' 'OmniverseGreydon Square'\n",
            " 'No Flex Zone (Remix)Karmin' 'BelieveEminem' 'LuxuryJon Bellion'\n",
            " 'Hope Dies (feat. Patricia Lynn)Zeale' 'Everything About YouUgly Kid Joe'\n",
            " 'World On fire (Feat. Slightly Stoopid)Stick Figure']\n",
            "True Sequence: ['Finer Things (feat. deM atlaS)Atmosphere', 'The Irony of It AllThe Streets', 'No Flex Zone (Remix)Karmin', 'Spill Me UpDoomtree', 'Windows (Feat. Prof)Atmosphere', 'GasolineProf', 'So Many PlacesVokab Kompany', 'Two Against OneK Theory', 'Two Different WorldsEpic Beard Men', 'HelloKiller Mike']\n",
            "Session recall: 0.0000\n",
            "Processed session with 13 items.\n",
            "Predicted Sequence: ['Born to DieSevendust' 'Mercury in RetrogradeSturgill Simpson'\n",
            " 'PainEarl St. Clair' 'PonyFar'\n",
            " 'Black Or White - Single VersionMichael Jackson' 'DenyDefault'\n",
            " 'My Love (Live)Humming House' 'MisunderstandingGenesis' 'Tokyo SunriseLP'\n",
            " 'Bad ReputationThin Lizzy']\n",
            "True Sequence: ['Dub Me To The River - Daniel Scholz Indaba RemixSon Little', 'Good IntentionsRobert Ellis', 'Beijing HoneyScoundrels', 'Without DesireThe Wood Brothers', 'MisunderstandingGenesis', 'Private InvestigationsDire Straits', 'Elvis Presley BluesTom Jones', \"Livin' on a PrayerBon Jovi\", 'Kickstart My HeartMötley Crüe', 'No MoreDirtwire']\n",
            "Session recall: 0.0000\n",
            "Processed session with 13 items.\n",
            "Predicted Sequence: ['ArtsyThe Grouch' 'Them ShoesPatrick Sweany' 'MindshiftTauk'\n",
            " \"Start Shootin' - Americana RemixLittle People\"\n",
            " \"ShudderMarty O'Reilly & the Old Soul Orchestra\" 'CreeperLincoln Durham'\n",
            " 'CypherLooPRaT' 'High SocietyLedinsky'\n",
            " 'Broke in Half (Donnie Dumphy cover) - liveJesse Stewart'\n",
            " 'Atlanta GirlsTrash Panda']\n",
            "True Sequence: [\"Start Shootin' - Americana RemixLittle People\", \"ShudderMarty O'Reilly & the Old Soul Orchestra\", 'CreeperLincoln Durham', 'CypherLooPRaT', 'ElationBass Physics', 'Broke in Half (Donnie Dumphy cover) - liveJesse Stewart', 'Atlanta GirlsTrash Panda', 'ShineStick Figure', 'Dub Me To The River - Daniel Scholz Indaba RemixSon Little', \"Let's Push Things ForwardThe Streets\"]\n",
            "Session recall: 0.0000\n",
            "Processed session with 16 items.\n",
            "Predicted Sequence: ['Long Way DownRobert DeLong' 'Make It Bun DemSkrillex'\n",
            " 'Love & War - DOLF RemixYellow Claw'\n",
            " 'No Sugar In My CoffeeCaught a Ghost' 'Bad DreamsLyrics Born'\n",
            " 'Boy Got it BadKail Baxley' 'IgnitionThe Wind and the Wave'\n",
            " 'Brutal PlanetAlice Cooper' 'Addicted To LoveAlex Clare'\n",
            " 'Feeling GoodMuse']\n",
            "True Sequence: ['GuardedDisturbed', 'Addicted to LoveSkylar Grey', 'Since I Lost YouGenesis', 'Smiles on Faces (with KBong)Stick Figure', 'Beautiful (feat. Camila Cabello)Bazzi', 'All for YouStick Figure', 'ThunderImagine Dragons', 'Since I Lost YouGenesis', 'IssuesJulia Michaels', 'Light of DayAubrie Sellers']\n",
            "Session recall: 0.0000\n",
            "Processed session with 12 items.\n",
            "Predicted Sequence: ['Edge of SeventeenStevie Nicks'\n",
            " 'Take What You Want (feat. Ozzy Osbourne & Travis Scott)Post Malone'\n",
            " 'Above the StormStick Figure' 'SunsetsAso' 'No Flex Zone (Remix)Karmin'\n",
            " 'The Void (feat. Sherry St. Germain)Cage'\n",
            " \"Whiskey Sun (feat. TJ O'Neill)Stick Figure\"\n",
            " \"I Don't Care AnymoreHellyeah\" 'BudapestGeorge Ezra' 'MagnoliaHazy Year']\n",
            "True Sequence: ['Twenty Past FourKognitif', 'BoomerangThe Uncluded', 'Something From Outer SpaceVibe Street', 'Above the StormStick Figure', 'Iron LungJon Kennedy', 'Tea for TwoThe Harpoonist & The Axe Murderer', 'Jesus Came to Tennessee - LiveWill Hoge', 'Every NightWalker Lukens', 'OxygenDirty Heads', 'The HustleA Night In The Box']\n",
            "Session recall: 0.0000\n",
            "Session skipped because next item sequence length is 10.\n",
            "Processed session with 11 items.\n",
            "Predicted Sequence: ['For Whom The Bell Tolls (Remastered)Metallica' 'My ApocalypseMetallica'\n",
            " \"Ain't My BitchMetallica\" '2 X 4Metallica' 'Until It SleepsMetallica'\n",
            " 'Hero of the DayMetallica' 'Chop Suey!System of a Down'\n",
            " 'Bleeding MeMetallica' '...And Justice For All (Remastered)Metallica'\n",
            " 'Wasting My HateMetallica']\n",
            "True Sequence: ['My ApocalypseMetallica', \"Ain't My BitchMetallica\", '2 X 4Metallica', 'Until It SleepsMetallica', 'King NothingMetallica', 'Hero of the DayMetallica', 'CureMetallica', 'Poor Twisted MeMetallica', 'Wasting My HateMetallica', 'Mama SaidMetallica']\n",
            "Session recall: 0.0000\n",
            "Processed session with 16 items.\n",
            "Predicted Sequence: ['The BonesMaren Morris' \"Ramblin' ManThe Allman Brothers Band\"\n",
            " 'TomorrowShakey Graves' 'Take It or Leave ItSublime with Rome'\n",
            " 'Fox on the RunSweet' 'Good Vibrations - RemasteredThe Beach Boys'\n",
            " 'No RainBlind Melon' 'No RainBlind Melon' 'Buddy HollyWeezer'\n",
            " 'Mr. Blue SkyElectric Light Orchestra']\n",
            "True Sequence: ['Jump in the LineHarry Belafonte', 'No RainBlind Melon', 'All Summer LongKid Rock', 'Mr. Blue SkyElectric Light Orchestra', 'ShotgunJr. Walker & The All Stars', 'Sober Up (feat. Rivers Cuomo)AJR', '1234Feist', 'How Sweet It Is (To Be Loved By You)Jr. Walker & The All Stars', \"Rockin' RobinBobby Day\", 'Are You Gonna Be My GirlJet']\n",
            "Session recall: 0.0000\n",
            "Processed session with 16 items.\n",
            "Predicted Sequence: ['Blackened (remastered)Metallica' 'Possum KingdomToadies'\n",
            " 'Rooms On FireStevie Nicks' 'Battery (Remastered)Metallica'\n",
            " 'Of Wolf and ManMetallica'\n",
            " 'The Thing That Should Not Be (Remastered)Metallica'\n",
            " 'Of Wolf and ManMetallica'\n",
            " 'The Thing That Should Not Be (Remastered)Metallica'\n",
            " 'The Frayed Ends Of Sanity (Remastered)Metallica'\n",
            " 'To Live Is To Die (Remastered)Metallica']\n",
            "True Sequence: ['Of Wolf and ManMetallica', 'The Thing That Should Not Be (Remastered)Metallica', 'The Frayed Ends Of Sanity (Remastered)Metallica', 'Moth Into FlameMetallica', 'HardwiredMetallica', 'King NothingMetallica', 'HardwiredMetallica', 'Space TreeMedium Troy', 'JellyOPIUO', 'Log CabinPhontaine']\n",
            "Session recall: 0.0000\n",
            "Processed session with 18 items.\n",
            "Predicted Sequence: ['Everything About YouUgly Kid Joe' 'Feeling GoodAvicii'\n",
            " 'Super Rich KidsFrank Ocean' 'Super Rich KidsFrank Ocean'\n",
            " 'No Flex Zone (Remix)Karmin' \"I Don't Care AnymoreHellyeah\"\n",
            " 'Under Arrest (feat. Stick Figure) - Original MixAlific'\n",
            " 'Joker and the ThiefWolfmother' 'Your Body Is a WonderlandJohn Mayer'\n",
            " 'ThunderImagine Dragons']\n",
            "True Sequence: ['Your Body Is a WonderlandJohn Mayer', 'Take Me Home Country RoadsJohn Denver', 'Safety DanceMen Without Hats', 'PeachesThe Presidents of the United States of America', 'LoserBeck', 'SanteriaSublime', 'LumpThe Presidents of the United States of America', 'Buddy HollyWeezer', 'Can I Kick It?A Tribe Called Quest', 'One WeekBarenaked Ladies']\n",
            "Session recall: 0.0000\n",
            "Processed session with 18 items.\n",
            "Predicted Sequence: ['Black Hole SunSoundgarden' 'Good So BadEric Tessmer'\n",
            " \"Walkin' On The SunSmash Mouth\"\n",
            " '1979 - Remastered 2012The Smashing Pumpkins' 'Purple HazeJimi Hendrix'\n",
            " 'Even FlowPearl Jam' 'Enter SandmanMetallica'\n",
            " 'Disarm - RemasteredThe Smashing Pumpkins'\n",
            " 'Only Wanna Be With YouHootie & The Blowfish' 'PushMatchbox Twenty']\n",
            "True Sequence: ['Only Wanna Be With YouHootie & The Blowfish', 'PushMatchbox Twenty', 'Closing TimeSemisonic', 'Fade Into YouMazzy Star', 'Mr. JonesCounting Crows', 'When I Come AroundGreen Day', 'Under the BridgeRed Hot Chili Peppers', \"She's So HighTal Bachman\", 'praise you - radio editFatboy Slim', 'AlivePearl Jam']\n",
            "Session recall: 0.0000\n",
            "Processed session with 15 items.\n",
            "Predicted Sequence: ['High EnoughSkydyed' 'Bag Full of MoneyLittle Stranger'\n",
            " 'Mind Runs DeepSaqi' \"Early Mornin' TonyFelt\"\n",
            " 'Are You Dumm?Flmmboiint Frdii' 'Hippie TrippieDruzu'\n",
            " 'Waiting On Your LoveEric Krasno' 'Fast LaneShrub'\n",
            " 'You KnowThe Boom Booms' 'CreeperLincoln Durham']\n",
            "True Sequence: ['Run Through the DesertThe Cunning', 'Waiting On Your LoveEric Krasno', 'Fast LaneShrub', 'Stray (feat. David Shaw)Naughty Professor', 'Morning EyesAtlas Road Crew', 'My Space BabyDJ Premier', 'SedonaHoundmouth', 'Hold OnAlabama Shakes', 'BadfishSublime', 'OxygenDirty Heads']\n",
            "Session recall: 0.0000\n",
            "Processed session with 19 items.\n",
            "Predicted Sequence: ['Home by the Sea - 2007 RemasterGenesis' \"Slammin'Huey Lewis & The News\"\n",
            " 'Put Your Money on MeThe Struts' 'The Glow (feat. Kimbra)Big Data'\n",
            " 'DaylightMatt & Kim' 'Land Of 1000 DancesWilson Pickett'\n",
            " 'The Way We MoveLanghorne Slim' 'Fool For LoveLord Huron'\n",
            " 'Shadows (feat. Grieves)Cydeways' 'Mountain SoundOf Monsters and Men']\n",
            "True Sequence: ['BelieverImagine Dragons', 'WeakAJR', 'Stormy WeatherGreat Dane', 'LOVE 3X - R3hab RemixZZ Ward', \"Winner's CircleBear Hands\", 'Cecilia And The SatelliteAndrew McMahon in the Wilderness', 'FanfareMagic City Hippies', 'Nico and the NinersTwenty One Pilots', 'Put Your Money on MeThe Struts', 'LuxuryJon Bellion']\n",
            "Session recall: 0.0000\n",
            "Session skipped because next item sequence length is 10.\n",
            "Session skipped because next item sequence length is 10.\n",
            "Processed session with 18 items.\n",
            "Predicted Sequence: ['No Flex Zone (Remix)Karmin' 'Hey Look Ma I Made ItPanic! at the Disco'\n",
            " 'Tainted LoveSoft Cell' 'Graveyard - AcousticHalsey'\n",
            " 'ConsequencesCamila Cabello' \"Live and Let DieGuns N' Roses\"\n",
            " 'Black Or White - Single VersionMichael Jackson'\n",
            " 'Angels Above MeStick Figure' 'Eye of the TigerSurvivor'\n",
            " 'No Flex Zone (Remix)Karmin']\n",
            "True Sequence: ['Burn the House DownAJR', 'WeakAJR', 'GhostbustersRay Parker Jr.', 'Sweet Cream In ItJel', 'Forgot About DreDr. Dre', 'Date With the NightYeah Yeah Yeahs', 'Memories - Dillon Francis RemixMaroon 5', 'Is This LoveBob Marley & The Wailers', 'Dub Me To The River - Daniel Scholz Indaba RemixSon Little', 'Shoot Me DeadHours Eastly']\n",
            "Session recall: 0.0000\n",
            "Processed session with 13 items.\n",
            "Predicted Sequence: [\"I Don't Care AnymoreHellyeah\" 'You or ThemVolbeat'\n",
            " 'No Flex Zone (Remix)Karmin' 'InsideSoil' 'New RulesDua Lipa'\n",
            " 'Super Rich KidsFrank Ocean' '18 and LifeSkid Row'\n",
            " 'Windows (Feat. Prof)Atmosphere' 'PainEarl St. Clair'\n",
            " 'Animals - ExtendedMartin Garrix']\n",
            "True Sequence: ['Inside OutEVE 6', 'Fire on the Horizon (LabRat Remix)Stick Figure', 'Make It Bun DemSkrillex', 'Mannish Boy - Ruckus Roboticus RemixMuddy Waters', 'PonyFar', 'like u doMarian Hill', \"Hollywood's BleedingPost Malone\", 'Could You Be LovedBob Marley & The Wailers', 'One Love / People Get Ready - MedleyBob Marley & The Wailers', 'Waste the NightKatastro']\n",
            "Session recall: 0.0000\n",
            "Processed session with 12 items.\n",
            "Predicted Sequence: ['The JokerSteve Miller Band'\n",
            " \"A Hard Day's Night - Remastered 2009The Beatles\"\n",
            " \"That's All I NeedDirty Heads\" \"Come As Your KidsUmphrey's McGee\"\n",
            " 'Edge of SeventeenStevie Nicks' 'Honky RedWidespread Panic'\n",
            " 'Honky RedWidespread Panic' 'RnRTranslee'\n",
            " 'Hey Hey What Can I Do - 2012 RemasterLed Zeppelin'\n",
            " 'All My Friends Are DeadDirty Bourbon River Show']\n",
            "True Sequence: [\"That's All I NeedDirty Heads\", 'BadfishSublime', 'Lone RiderGreyhounds', 'Lone RiderGreyhounds', 'HussyBlack Carl', 'WidowmakerFive Alarm Funk', 'Hey Hey What Can I Do - 2012 RemasterLed Zeppelin', 'All My Friends Are DeadDirty Bourbon River Show', 'Feeling InsideParanoid Castle', 'Exclamation PointLatyrx']\n",
            "Session recall: 0.0000\n",
            "Processed session with 14 items.\n",
            "Predicted Sequence: ['WastedParanoid Social Club' 'Come HomeAntix' 'No TypeNew Beat Fund'\n",
            " 'The PusherBlind Melon' 'Diner SongChadwick Stokes'\n",
            " 'All These LightsThe Grouch & Eligh' 'All These LightsThe Grouch & Eligh'\n",
            " 'MagentaDeqn Sue' 'WastedParanoid Social Club'\n",
            " 'Easy to Be Your LoverTea Leaf Green']\n",
            "True Sequence: ['Diner SongChadwick Stokes', 'All These LightsThe Grouch & Eligh', \"Hand's UpAntix\", \"Don't You Stay GoneDani Bell and the Tarantist\", 'WastedParanoid Social Club', 'Easy to Be Your LoverTea Leaf Green', 'Lord of the FriesPolyrhythmics', 'Kiss ThisThe Struts', 'Kiss ThisThe Struts', 'SwitchbladeHoly White Hounds']\n",
            "Session recall: 0.0000\n",
            "Processed session with 12 items.\n",
            "Predicted Sequence: ['A Wake (feat. Evan Roman)Macklemore & Ryan Lewis' 'StayRihanna'\n",
            " 'Riot (feat. Damian Jr. Gong Marley)Sean Paul' 'FlowKatastro'\n",
            " 'Fantastic LoveThe Floozies' 'Supernova Goes PopPowerman 5000' 'H.Tool'\n",
            " 'Fat LipSum 41' 'Forty Six & 2Tool'\n",
            " 'Clean My WoundsCorrosion of Conformity']\n",
            "True Sequence: ['Riot (feat. Damian Jr. Gong Marley)Sean Paul', 'Goldmine JunkieBig Grams', 'StinkfistTool', 'EulogyTool', 'H.Tool', 'Useful IdiotTool', 'Forty Six & 2Tool', 'Hooker With a PenisTool', 'IntermissionTool', 'jimmyTool']\n",
            "Session recall: 0.0000\n",
            "Processed session with 15 items.\n",
            "Predicted Sequence: ['Dance Me a NumberThe Steepwater Band' 'BelieverImagine Dragons'\n",
            " 'Inside OutSpoon' 'King and CrossÁsgeir' 'El PasoMarty Robbins'\n",
            " 'El PasoMarty Robbins' 'UPTOWN BOYSNetherfriends'\n",
            " \"Won't Go QuietlyAll That Remains\" 'UpsideAllen Stone'\n",
            " 'The Void (feat. Sherry St. Germain)Cage']\n",
            "True Sequence: ['Venice VentureBig Wild', 'Wild RideJimkata', 'LeviathanTropidelic', 'Something From Outer SpaceVibe Street', 'HereAlessia Cara', 'In This LoveStick Figure', 'Call on the WolvesDustin Thomas', 'Joe - Live From Austin City LimitsAlabama Shakes', 'ConfidenceConner Youngblood', 'Jesus Came to Tennessee - LiveWill Hoge']\n",
            "Session recall: 0.0000\n",
            "Overall Recall@10: 0.0000\n"
          ]
        }
      ],
      "source": [
        "def predict_next(model, item_sequences, item_features, item_genres):\n",
        "    \"\"\"\n",
        "    Predict the next item for a given input sequence.\n",
        "\n",
        "    Args:\n",
        "    - model: The trained model.\n",
        "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
        "    - item_features: Input item features (batch_size, feature_length).\n",
        "    - item_genres: Input item genres (batch_size, genre_length).\n",
        "\n",
        "    Returns:\n",
        "    - predicted_items: Predicted next item (batch_size,).\n",
        "    \"\"\"\n",
        "    # Run inference\n",
        "    _, logits = model((item_sequences, item_features, item_genres), training=False)\n",
        "    # print(\"Logits:\", logits)\n",
        "\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Select the item with the highest probability\n",
        "    predicted_items = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n",
        "\n",
        "    return predicted_items.numpy()\n",
        "\n",
        "\n",
        "def compute_recall_at_k(predicted_sequence, target_sequence, k):\n",
        "    \"\"\"\n",
        "    Compute Recall@k for a given session.\n",
        "\n",
        "    Args:\n",
        "    - predicted_sequence: The predicted sequence of items.\n",
        "    - target_sequence: The actual target sequence of items.\n",
        "    - k: The number of items in the predicted sequence.\n",
        "\n",
        "    Returns:\n",
        "    - recall_at_k: Recall@k value.\n",
        "    \"\"\"\n",
        "    # Count how many of the target items appear in the predicted sequence\n",
        "    predicted_ids = [item['SongID'] if isinstance(item, dict) else item for item in predicted_sequence]\n",
        "    target_ids = [item['SongID'] if isinstance(item, dict) else item for item in target_sequence]\n",
        "\n",
        "    # Count how many of the target items appear in the predicted sequence\n",
        "    hits = len(set(predicted_ids[:k]) & set(target_ids[:k]))\n",
        "    return hits / len(target_ids[:k]) if target_ids[:k] else 0.0\n",
        "    return hits / len(target_sequence[:k])\n",
        "\n",
        "\n",
        "# Initialize overall metrics\n",
        "total_recall = 0\n",
        "session_count = 0\n",
        "k = 10  # Set the value of k\n",
        "print(\"Creating session dataset\")\n",
        "sessions_data = preprocessor.create_session_dataset(preprocessor.train_df)  # Use train data for training\n",
        "print(\"Creating tensor dataset\")\n",
        "# Process each session in the dataset\n",
        "for session in sessions_data[:100]:  # Assume train_sessions is your preprocessed session data\n",
        "    if len(session) <= k:\n",
        "      continue\n",
        "    # print(session)\n",
        "    context_length = len(session) - k\n",
        "    context = session[:context_length]\n",
        "    target = session[context_length:]\n",
        "    dataset = preprocess_data_single_session(session, feature_columns, k=k)\n",
        "    if dataset is None:\n",
        "        continue\n",
        "    dataset = dataset.batch(1)\n",
        "    predicted_sequence = []\n",
        "    current_sequence = context\n",
        "\n",
        "    # Generate k predictions iteratively\n",
        "    for batch in dataset:\n",
        "        # Prepare input features for the current sequence\n",
        "        item_sequences = batch['item']  # Batch of size 1\n",
        "        item_features = batch['features']  # Extract corresponding features\n",
        "        item_genres = batch['genre']  # Extract corresponding genres\n",
        "\n",
        "        # Predict the next item\n",
        "        predicted_item = predict_next(model, item_sequences, item_features, item_genres)\n",
        "        predicted_sequence.append(predicted_item[0])  # Append the prediction\n",
        "\n",
        "        # Update the current sequence\n",
        "        current_sequence = current_sequence[1:] + [predicted_item[0]]\n",
        "\n",
        "        if len(predicted_sequence) >= k:\n",
        "            break\n",
        "    ori_pred_seq = preprocessor.song_id_encoder.inverse_transform(predicted_sequence)\n",
        "    print(f\"Predicted Sequence: {ori_pred_seq}\")\n",
        "    # target_ids = preprocessor.song_id_encoder.transform([item['SongID'] if isinstance(item, dict) else item for item in target])\n",
        "    print(f\"True Sequence: {[item['SongID'] for item in target]}\")\n",
        "\n",
        "    # check genre\n",
        "    original_item_genres = preprocessor.train_df.loc[preprocessor.train_df['SongID'].isin([item['SongID'] if isinstance(item, dict) else item for item in target]), 'spotify_genre'].values\n",
        "    # print(f\"Original Item Genres: {original_item_genres}\")\n",
        "    predicted_item_genres = preprocessor.train_df.loc[preprocessor.train_df['SongID'].isin([item['SongID'] if isinstance(item, dict) else item for item in ori_pred_seq]), 'spotify_genre'].values\n",
        "    # print(f\"Predicted Item Genres: {predicted_item_genres}\")\n",
        "    # Calculate Recall@k for the current session\n",
        "    session_recall = compute_recall_at_k(predicted_sequence, target, k)\n",
        "    total_recall += session_recall\n",
        "    session_count += 1\n",
        "\n",
        "    print(f\"Session recall: {session_recall:.4f}\")\n",
        "\n",
        "# Calculate overall Recall@k\n",
        "overall_recall_at_k = total_recall / session_count if session_count > 0 else 0\n",
        "print(f\"Overall Recall@{k}: {overall_recall_at_k:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbOSEngl7lMn"
      },
      "source": [
        "## Cara 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRbYXzqWwHJa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def predict_next(model, item_sequences, item_features, item_genres, k=5):\n",
        "    \"\"\"\n",
        "    Predict the next item for a given input sequence.\n",
        "\n",
        "    Args:\n",
        "    - model: The trained model.\n",
        "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
        "    - item_features: Input item features (batch_size, feature_length).\n",
        "    - item_genres: Input item genres (batch_size, genre_length).\n",
        "    - k: Number of top predictions to consider.\n",
        "\n",
        "    Returns:\n",
        "    - predicted_items: Top k predicted next items (batch_size, k).\n",
        "    \"\"\"\n",
        "    # Run inference\n",
        "    _, logits = model((item_sequences, item_features, item_genres), training=False)\n",
        "\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Get the top k predictions\n",
        "    top_k_values, top_k_indices = tf.nn.top_k(probabilities, k=k, sorted=True)\n",
        "\n",
        "    return top_k_indices.numpy()  # Return top k item indices\n",
        "\n",
        "def compute_recall_at_k(predicted_sequence, target_sequence, k):\n",
        "    \"\"\"\n",
        "    Compute Recall@k for a given session.\n",
        "\n",
        "    Args:\n",
        "    - predicted_sequence: The predicted sequence of items.\n",
        "    - target_sequence: The actual target sequence of items.\n",
        "    - k: The number of top predictions to consider.\n",
        "\n",
        "    Returns:\n",
        "    - recall_at_k: Recall@k value.\n",
        "    \"\"\"\n",
        "    # Extract 'SongID' from dictionaries if needed\n",
        "    predicted_ids = [item['SongID'] if isinstance(item, dict) else item for item in predicted_sequence]\n",
        "    target_ids = [item['SongID'] if isinstance(item, dict) else item for item in target_sequence]\n",
        "\n",
        "    # Select the top k items from predicted sequence\n",
        "    top_k_predicted = predicted_ids[:k]\n",
        "\n",
        "    # Count how many of the target items appear in the top k predictions\n",
        "    hits = len(set(top_k_predicted) & set(target_ids[:k]))\n",
        "    return hits / len(target_ids[:k]) if target_ids[:k] else 0.0\n",
        "\n",
        "\n",
        "# Initialize overall metrics\n",
        "total_recall = 0\n",
        "session_count = 0\n",
        "k = 5  # Set the value of k\n",
        "\n",
        "print(\"Creating session dataset\")\n",
        "sessions_data = preprocessor.create_session_dataset(preprocessor.train_df)  # Use train data for training\n",
        "print(\"Creating tensor dataset\")\n",
        "\n",
        "# Process each session in the dataset\n",
        "for session in sessions_data:  # Assume sessions_data is your preprocessed session data\n",
        "    if len(session) <= k:\n",
        "        continue\n",
        "    print(session)\n",
        "    context_length = len(session) - k\n",
        "    context = session[:context_length]\n",
        "    target = session[context_length:]\n",
        "    dataset = preprocess_data_single_session(session, feature_columns, k=k)\n",
        "    if dataset is None:\n",
        "        continue\n",
        "    dataset = dataset.batch(1)\n",
        "    predicted_sequence = []\n",
        "    current_sequence = context\n",
        "\n",
        "    # Generate k predictions iteratively\n",
        "    for batch in dataset:\n",
        "        # Prepare input features for the current sequence\n",
        "        item_sequences = batch['item']  # Batch of size 1\n",
        "        item_features = batch['features']  # Extract corresponding features\n",
        "        item_genres = batch['genre']  # Extract corresponding genres\n",
        "\n",
        "        # Predict the top k items\n",
        "        top_k_predictions = predict_next(model, item_sequences, item_features, item_genres, k=k)\n",
        "\n",
        "        # Append the top k predictions to the sequence (only append first item from the top k)\n",
        "        predicted_sequence.extend(top_k_predictions[0])  # Assuming batch size is 1\n",
        "\n",
        "        # Update the current sequence\n",
        "        current_sequence = current_sequence[1:] + [top_k_predictions[0][0]]  # Only use first predicted item for the next context\n",
        "\n",
        "        if len(predicted_sequence) >= k:\n",
        "            break\n",
        "\n",
        "    # Calculate Recall@k for the current session\n",
        "    session_recall = compute_recall_at_k(predicted_sequence, target, k)\n",
        "    total_recall += session_recall\n",
        "    session_count += 1\n",
        "\n",
        "    print(f\"Session recall: {session_recall:.4f}\")\n",
        "\n",
        "# Calculate overall Recall@k\n",
        "overall_recall_at_k = total_recall / session_count if session_count > 0 else 0\n",
        "print(f\"Overall Recall@{k}: {overall_recall_at_k:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVjdRzAd7th2"
      },
      "source": [
        "## Cara 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdr6XBLu6sCq",
        "outputId": "1863021a-582d-4fc7-8581-427a2026707d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating session dataset\n",
            "Creating tensor dataset\n",
            "Processed session with 5 items.\n",
            "['Off the GroundThe Record Company' 'The Moon Is DisgustingThat 1 Guy']\n",
            "Predicted SongID: Off the GroundThe Record Company; True SongID: ConfidenceConner Youngblood\n",
            "Predicted Genre: ['funk', 'garage rock', 'la indie', 'modern blues', 'modern blues rock']; True Genre: ['shimmer pop', 'shiver pop']\n",
            "=====================================================================\n",
            "Predicted SongID: The Moon Is DisgustingThat 1 Guy; True SongID: You Could Be MineGuns N' Roses\n",
            "Predicted Genre: ['one-person band']; True Genre: ['glam metal', 'hard rock', 'rock']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Off the GroundThe Record Company', 'Genre': \"['funk', 'garage rock', 'la indie', 'modern blues', 'modern blues rock']\"}, {'SongID': 'The Moon Is DisgustingThat 1 Guy', 'Genre': \"['one-person band']\"}]\n",
            "True Sequence: [{'SongID': 'ConfidenceConner Youngblood', 'Genre': \"['shimmer pop', 'shiver pop']\"}, {'SongID': \"You Could Be MineGuns N' Roses\", 'Genre': \"['glam metal', 'hard rock', 'rock']\"}]\n",
            "Session recall: 0.0000\n",
            "Session skipped because next item sequence length is 2.\n",
            "Processed session with 11 items.\n",
            "[\"I'm Still StandingElton John\"\n",
            " 'I Wanna Dance with Somebody (Who Loves Me)Whitney Houston']\n",
            "Predicted SongID: I'm Still StandingElton John; True SongID: I Love Rock 'n' RollJoan Jett and the Blackhearts\n",
            "Predicted Genre: ['glam rock', 'mellow gold', 'piano rock', 'soft rock']; True Genre: ['rock']\n",
            "=====================================================================\n",
            "Predicted SongID: I Wanna Dance with Somebody (Who Loves Me)Whitney Houston; True SongID: Get Outta My Dreams Get into My CarBilly Ocean\n",
            "Predicted Genre: ['dance pop', 'pop', 'r&b']; True Genre: ['dance rock', 'disco', 'europop', 'mellow gold', 'motown', 'new romantic', 'new wave pop', 'soft rock', 'yacht rock']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': \"I'm Still StandingElton John\", 'Genre': \"['glam rock', 'mellow gold', 'piano rock', 'soft rock']\"}, {'SongID': 'I Wanna Dance with Somebody (Who Loves Me)Whitney Houston', 'Genre': \"['dance pop', 'pop', 'r&b']\"}]\n",
            "True Sequence: [{'SongID': \"I Love Rock 'n' RollJoan Jett and the Blackhearts\", 'Genre': \"['rock']\"}, {'SongID': 'Get Outta My Dreams Get into My CarBilly Ocean', 'Genre': \"['dance rock', 'disco', 'europop', 'mellow gold', 'motown', 'new romantic', 'new wave pop', 'soft rock', 'yacht rock']\"}]\n",
            "Session recall: 0.0000\n",
            "Processed session with 3 items.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Three WeeksPerpetual Groove' 'Smoke Out The WindowTribe Society']\n",
            "Predicted SongID: Three WeeksPerpetual Groove; True SongID: Chest Wide OpenLyrics Born\n",
            "Predicted Genre: ['jam band']; True Genre: ['alternative hip hop', 'hip hop', 'turntablism']\n",
            "=====================================================================\n",
            "Predicted SongID: Smoke Out The WindowTribe Society; True SongID: This Is AmericaChildish Gambino\n",
            "Predicted Genre: ['electronic rock']; True Genre: ['atl hip hop', 'hip hop', 'pop rap', 'rap']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Three WeeksPerpetual Groove', 'Genre': \"['jam band']\"}, {'SongID': 'Smoke Out The WindowTribe Society', 'Genre': \"['electronic rock']\"}]\n",
            "True Sequence: [{'SongID': 'Chest Wide OpenLyrics Born', 'Genre': \"['alternative hip hop', 'hip hop', 'turntablism']\"}, {'SongID': 'This Is AmericaChildish Gambino', 'Genre': \"['atl hip hop', 'hip hop', 'pop rap', 'rap']\"}]\n",
            "Session recall: 0.0000\n",
            "Processed session with 16 items.\n",
            "['Better on a SundaySTS' 'Sussudio - 2016 RemasterPhil Collins']\n",
            "Predicted SongID: Better on a SundaySTS; True SongID: Papa Come Quick (Jody And Chico)Bonnie Raitt\n",
            "Predicted Genre: []; True Genre: ['country rock', 'electric blues', 'folk', 'folk rock', 'lilith', 'mellow gold', 'pop rock', 'rock', 'roots rock', 'singer-songwriter', 'soft rock']\n",
            "=====================================================================\n",
            "Predicted SongID: Sussudio - 2016 RemasterPhil Collins; True SongID: DragulaRob Zombie\n",
            "Predicted Genre: ['mellow gold', 'soft rock']; True Genre: ['alternative metal', 'comic', 'groove metal', 'industrial metal', 'industrial rock', 'nu metal', 'post-grunge', 'rap rock', 'rock']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Better on a SundaySTS', 'Genre': '[]'}, {'SongID': 'Sussudio - 2016 RemasterPhil Collins', 'Genre': \"['mellow gold', 'soft rock']\"}]\n",
            "True Sequence: [{'SongID': 'Papa Come Quick (Jody And Chico)Bonnie Raitt', 'Genre': \"['country rock', 'electric blues', 'folk', 'folk rock', 'lilith', 'mellow gold', 'pop rock', 'rock', 'roots rock', 'singer-songwriter', 'soft rock']\"}, {'SongID': 'DragulaRob Zombie', 'Genre': \"['alternative metal', 'comic', 'groove metal', 'industrial metal', 'industrial rock', 'nu metal', 'post-grunge', 'rap rock', 'rock']\"}]\n",
            "Session recall: 0.0000\n",
            "Processed session with 9 items.\n",
            "['Royal JellyDeap Vally' 'Sussudio - 2016 RemasterPhil Collins']\n",
            "Predicted SongID: Royal JellyDeap Vally; True SongID: Bring It OnSpiritual Rez\n",
            "Predicted Genre: ['garage rock', 'modern blues rock']; True Genre: ['east coast reggae']\n",
            "=====================================================================\n",
            "Predicted SongID: Sussudio - 2016 RemasterPhil Collins; True SongID: The HealingGary Clark Jr.\n",
            "Predicted Genre: ['mellow gold', 'soft rock']; True Genre: ['blues rock', 'electric blues', 'funk', 'garage rock', 'modern blues', 'modern blues rock', 'rock', 'soul blues', 'texas blues']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Royal JellyDeap Vally', 'Genre': \"['garage rock', 'modern blues rock']\"}, {'SongID': 'Sussudio - 2016 RemasterPhil Collins', 'Genre': \"['mellow gold', 'soft rock']\"}]\n",
            "True Sequence: [{'SongID': 'Bring It OnSpiritual Rez', 'Genre': \"['east coast reggae']\"}, {'SongID': 'The HealingGary Clark Jr.', 'Genre': \"['blues rock', 'electric blues', 'funk', 'garage rock', 'modern blues', 'modern blues rock', 'rock', 'soul blues', 'texas blues']\"}]\n",
            "Session recall: 0.0000\n",
            "Processed session with 12 items.\n",
            "['Knights of CydoniaMuse' 'She Drives Me CrazyFine Young Cannibals']\n",
            "Predicted SongID: Knights of CydoniaMuse; True SongID: Down UnderMen at Work\n",
            "Predicted Genre: ['modern rock', 'permanent wave', 'piano rock', 'post-grunge', 'rock']; True Genre: ['album rock', 'australian rock', 'dance rock', 'mellow gold', 'new romantic', 'new wave', 'new wave pop', 'rock', 'soft rock']\n",
            "=====================================================================\n",
            "Predicted SongID: She Drives Me CrazyFine Young Cannibals; True SongID: Don't Stop Believin'Journey\n",
            "Predicted Genre: ['art rock', 'dance rock', 'mellow gold', 'new romantic', 'new wave', 'new wave pop', 'rock', 'soft rock', 'synthpop']; True Genre: ['album rock', 'classic rock', 'hard rock', 'mellow gold', 'rock', 'soft rock']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Knights of CydoniaMuse', 'Genre': \"['modern rock', 'permanent wave', 'piano rock', 'post-grunge', 'rock']\"}, {'SongID': 'She Drives Me CrazyFine Young Cannibals', 'Genre': \"['art rock', 'dance rock', 'mellow gold', 'new romantic', 'new wave', 'new wave pop', 'rock', 'soft rock', 'synthpop']\"}]\n",
            "True Sequence: [{'SongID': 'Down UnderMen at Work', 'Genre': \"['album rock', 'australian rock', 'dance rock', 'mellow gold', 'new romantic', 'new wave', 'new wave pop', 'rock', 'soft rock']\"}, {'SongID': \"Don't Stop Believin'Journey\", 'Genre': \"['album rock', 'classic rock', 'hard rock', 'mellow gold', 'rock', 'soft rock']\"}]\n",
            "Session recall: 0.0000\n",
            "Processed session with 5 items.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sussudio - 2016 RemasterPhil Collins'\n",
            " 'Land Of Confusion - 2007 RemasterGenesis']\n",
            "Predicted SongID: Sussudio - 2016 RemasterPhil Collins; True SongID: ParalyzerFinger Eleven\n",
            "Predicted Genre: ['mellow gold', 'soft rock']; True Genre: ['alternative metal', 'canadian rock', 'funk metal', 'nu metal', 'post-grunge', 'rap rock', 'wrestling']\n",
            "=====================================================================\n",
            "Predicted SongID: Land Of Confusion - 2007 RemasterGenesis; True SongID: Saving GraceTom Petty\n",
            "Predicted Genre: ['album rock', 'art rock', 'classic rock', 'mellow gold', 'progressive rock', 'rock', 'soft rock', 'symphonic rock']; True Genre: ['album rock', 'classic rock', 'folk rock', 'heartland rock', 'mellow gold', 'pop rock', 'rock', 'roots rock', 'singer-songwriter', 'soft rock']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Sussudio - 2016 RemasterPhil Collins', 'Genre': \"['mellow gold', 'soft rock']\"}, {'SongID': 'Land Of Confusion - 2007 RemasterGenesis', 'Genre': \"['album rock', 'art rock', 'classic rock', 'mellow gold', 'progressive rock', 'rock', 'soft rock', 'symphonic rock']\"}]\n",
            "True Sequence: [{'SongID': 'ParalyzerFinger Eleven', 'Genre': \"['alternative metal', 'canadian rock', 'funk metal', 'nu metal', 'post-grunge', 'rap rock', 'wrestling']\"}, {'SongID': 'Saving GraceTom Petty', 'Genre': \"['album rock', 'classic rock', 'folk rock', 'heartland rock', 'mellow gold', 'pop rock', 'rock', 'roots rock', 'singer-songwriter', 'soft rock']\"}]\n",
            "Session recall: 0.0000\n",
            "Processed session with 4 items.\n",
            "['Johnny B. GoodeChuck Berry' 'Smoke Out The WindowTribe Society']\n",
            "Predicted SongID: Johnny B. GoodeChuck Berry; True SongID: Pussy and PizzaMurs\n",
            "Predicted Genre: ['blues rock', 'classic rock', 'rock', 'rock-and-roll', 'rockabilly', 'soul']; True Genre: ['dance pop', 'europop', 'neo mellow', 'pop', 'pop rap', 'pop rock', 'post-teen pop', 'talent show', 'uk pop']\n",
            "=====================================================================\n",
            "Predicted SongID: Smoke Out The WindowTribe Society; True SongID: Devil's EyesGreyhounds\n",
            "Predicted Genre: ['electronic rock']; True Genre: ['deep new americana', 'funk']\n",
            "=====================================================================\n",
            "Predicted Sequence: [{'SongID': 'Johnny B. GoodeChuck Berry', 'Genre': \"['blues rock', 'classic rock', 'rock', 'rock-and-roll', 'rockabilly', 'soul']\"}, {'SongID': 'Smoke Out The WindowTribe Society', 'Genre': \"['electronic rock']\"}]\n",
            "True Sequence: [{'SongID': 'Pussy and PizzaMurs', 'Genre': \"['dance pop', 'europop', 'neo mellow', 'pop', 'pop rap', 'pop rock', 'post-teen pop', 'talent show', 'uk pop']\"}, {'SongID': \"Devil's EyesGreyhounds\", 'Genre': \"['deep new americana', 'funk']\"}]\n",
            "Session recall: 0.0000\n",
            "Overall Recall@2: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def predict_next(model, item_sequences, item_features, item_genres, k=5):\n",
        "    \"\"\"\n",
        "    Predict the next item for a given input sequence.\n",
        "\n",
        "    Args:\n",
        "    - model: The trained model.\n",
        "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
        "    - item_features: Input item features (batch_size, feature_length).\n",
        "    - item_genres: Input item genres (batch_size, genre_length).\n",
        "    - k: The number of top predictions to return.\n",
        "\n",
        "    Returns:\n",
        "    - predicted_items: Top-k predicted items (batch_size, k).\n",
        "    \"\"\"\n",
        "    # Run inference\n",
        "    logits = model((item_sequences, item_features, item_genres), training=False)\n",
        "\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Get the top-k predicted item indices based on probabilities\n",
        "    top_k_indices = tf.argsort(probabilities, axis=-1, direction='DESCENDING')[:, :k]\n",
        "\n",
        "    return top_k_indices.numpy()\n",
        "\n",
        "\n",
        "# Initialize overall metrics\n",
        "total_recall = 0\n",
        "session_count = 0\n",
        "k = 2 # Set the value of k\n",
        "print(\"Creating session dataset\")\n",
        "sessions_data = preprocessor.create_session_dataset(preprocessor.train_df)  # Use train data for training\n",
        "print(\"Creating tensor dataset\")\n",
        "\n",
        "# Process each session in the dataset\n",
        "for session in sessions_data:  # Assume train_sessions is your preprocessed session data\n",
        "    if len(session) <= k:\n",
        "        continue\n",
        "\n",
        "    context_length = len(session) - k\n",
        "    context = session[:context_length]\n",
        "    target = session[context_length:]\n",
        "    dataset_for_prediction = preprocess_data_single_session(session, feature_columns, k=k)\n",
        "    if dataset_for_prediction is None:\n",
        "        continue\n",
        "    dataset_for_prediction = dataset_for_prediction.batch(1)\n",
        "    predicted_sequence = []\n",
        "\n",
        "    # Generate k predictions iteratively\n",
        "    for batch in dataset_for_prediction:\n",
        "        # Prepare input features for the current sequence\n",
        "        item_sequences = batch['item']  # Batch of size 1\n",
        "        item_features = batch['features']  # Extract corresponding features\n",
        "        item_genres = batch['genre']  # Extract corresponding genres\n",
        "\n",
        "        # Predict the top k items\n",
        "        top_k_predictions = predict_next(model, item_sequences, item_features, item_genres, k)\n",
        "\n",
        "        # Add the top-k predictions to the predicted sequence\n",
        "        predicted_sequence.extend(top_k_predictions[0])  # Extend by top-k predicted items\n",
        "\n",
        "        if len(predicted_sequence) >= k:\n",
        "            break\n",
        "\n",
        "    ori_pred_seq = preprocessor.song_id_encoder.inverse_transform(predicted_sequence)\n",
        "    print(ori_pred_seq)\n",
        "    predicted_song_genre = [\n",
        "      {'SongID': item, 'Genre': preprocessor.train_df.loc[preprocessor.train_df['SongID'] == item, 'spotify_genre'].values[0]}\n",
        "      for item in ori_pred_seq\n",
        "    ]\n",
        "\n",
        "    # Extract SongID and genre for the true sequence\n",
        "    true_song_genre = [\n",
        "        {'SongID': item['SongID'], 'Genre': preprocessor.train_df.loc[preprocessor.train_df['SongID'] == item['SongID'], 'spotify_genre'].values[0]}\n",
        "        for item in target\n",
        "    ]\n",
        "\n",
        "    for true, predicted in zip(true_song_genre, predicted_song_genre):\n",
        "        print(f\"Predicted SongID: {predicted['SongID']}; True SongID: {true['SongID']}\")\n",
        "        print(f\"Predicted Genre: {predicted['Genre']}; True Genre: {true['Genre']}\")\n",
        "        print('=====================================================================')\n",
        "\n",
        "    # Print the predicted and true sequences with SongID and Genre\n",
        "    print(f\"Predicted Sequence: {predicted_song_genre}\")\n",
        "    print(f\"True Sequence: {true_song_genre}\")\n",
        "    # Calculate Recall@k for the current session\n",
        "    session_recall = compute_recall_at_k(predicted_sequence, target, k)\n",
        "    total_recall += session_recall\n",
        "    session_count += 1\n",
        "\n",
        "    print(f\"Session recall: {session_recall:.4f}\")\n",
        "\n",
        "# Calculate overall Recall@k\n",
        "overall_recall_at_k = total_recall / session_count if session_count > 0 else 0\n",
        "print(f\"Overall Recall@{k}: {overall_recall_at_k:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display training history"
      ],
      "metadata": {
        "id": "zDXnvTfLpp-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(loss_history, metric_name=\"Metric\", metric_history=None):\n",
        "    \"\"\"Plot the training loss and specified metric.\"\"\"\n",
        "    epochs = range(1, len(loss_history) + 1)\n",
        "\n",
        "    # Create subplots\n",
        "    if metric_history is not None:\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
        "    else:\n",
        "        fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plot the training loss\n",
        "    ax1.plot(epochs, loss_history, label='Loss', color='blue', linestyle='-', marker='o')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot the specified metric if provided\n",
        "    if metric_history is not None:\n",
        "        ax2.plot(epochs, metric_history, label=metric_name, color='green', linestyle='-', marker='o')\n",
        "        ax2.set_title(f'Training {metric_name}')\n",
        "        ax2.set_xlabel('Epochs')\n",
        "        ax2.set_ylabel(metric_name)\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d5VUMhUapxyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DEDRp9VQ676h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "5bdbc2e5-54bb-4702-f00a-7b0ef7f4ef37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_training_history(history['loss_history'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "Ye4bsSrJrlW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to an H5 file\n",
        "model.save(\"gru4rec_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soidj3f9qNgh",
        "outputId": "4becb233-e371-4115-bb5f-7b9a5ceb2a1d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Load model"
      ],
      "metadata": {
        "id": "WSiZRK4I3nGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(\n",
        "    \"gru4rec_model.h5\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "0WgtMdwfrnDW",
        "outputId": "ddcf75fd-cefe-41d0-ba43-7c3ac2cf3f5e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown layer: 'GRU4REC'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4e545521f51a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m loaded_model = load_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"gru4rec_model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# In this case we are dealing with a Keras config dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintable_module_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;34mf\"Unknown {printable_module_name}: '{class_name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;34m\"Please ensure you are using a `keras.utils.custom_object_scope` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown layer: 'GRU4REC'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hZiuO9Lrvrn"
      },
      "execution_count": 29,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}