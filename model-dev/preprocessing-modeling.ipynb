{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.api.layers import Dense, Embedding, GRU, LeakyReLU, Concatenate, Masking, Layer, StringLookup, Normalization, BatchNormalization\n",
    "from keras.api import Input\n",
    "from keras.api.models import Model\n",
    "from keras.api.losses import SparseCategoricalCrossentropy\n",
    "from keras.api.metrics import SparseCategoricalAccuracy, Mean, TopKCategoricalAccuracy\n",
    "# from transformers.models.bert import TFBertTokenizer, TFBertEmbeddings  # embedding and tokenizer for description/nlp related stufff\n",
    "from keras.api.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>SongID</th>\n",
       "      <th>TimeStamp_Central</th>\n",
       "      <th>Performer_x</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song_x</th>\n",
       "      <th>TimeStamp_UTC</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Performer_y</th>\n",
       "      <th>Song_y</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>session_3_hour</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Twenty Five MilesEdwin Starr</td>\n",
       "      <td>5/25/2021 5:18:00 PM</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>25 Miles</td>\n",
       "      <td>Twenty Five Miles</td>\n",
       "      <td>2021-05-25 23:18:00</td>\n",
       "      <td>9761</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>Twenty Five Miles</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.964</td>\n",
       "      <td>124.567</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Devil's EyesGreyhounds</td>\n",
       "      <td>5/25/2021 5:15:00 PM</td>\n",
       "      <td>Greyhounds</td>\n",
       "      <td>Change of Pace</td>\n",
       "      <td>Devil's Eyes</td>\n",
       "      <td>2021-05-25 23:15:00</td>\n",
       "      <td>206</td>\n",
       "      <td>Greyhounds</td>\n",
       "      <td>Devil's Eyes</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.858</td>\n",
       "      <td>113.236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pussy and PizzaMurs</td>\n",
       "      <td>5/25/2021 5:12:00 PM</td>\n",
       "      <td>Murs</td>\n",
       "      <td>Have a Nice Life</td>\n",
       "      <td>Pussy and Pizza</td>\n",
       "      <td>2021-05-25 23:12:00</td>\n",
       "      <td>6404</td>\n",
       "      <td>Murs</td>\n",
       "      <td>Pussy and Pizza</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.381</td>\n",
       "      <td>93.991</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Our Special PlaceThe Heavy</td>\n",
       "      <td>5/25/2021 4:46:00 PM</td>\n",
       "      <td>The Heavy</td>\n",
       "      <td>Great Vengeance and Furious Fire</td>\n",
       "      <td>Our Special Place</td>\n",
       "      <td>2021-05-25 22:46:00</td>\n",
       "      <td>6205</td>\n",
       "      <td>The Heavy</td>\n",
       "      <td>Our Special Place</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.939</td>\n",
       "      <td>193.996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Make Peace and be FreePerfect Confusion</td>\n",
       "      <td>5/25/2021 4:39:00 PM</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Make Peace and be Free</td>\n",
       "      <td>2021-05-25 22:39:00</td>\n",
       "      <td>6051</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Make Peace and be Free</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.431</td>\n",
       "      <td>78.037</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50013</th>\n",
       "      <td>62902</td>\n",
       "      <td>From Me To You - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 10:04:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Past Masters (Vols. 1 &amp; 2 / Remastered)</td>\n",
       "      <td>From Me To You - Remastered 2009</td>\n",
       "      <td>2017-01-01 16:04:00</td>\n",
       "      <td>5693</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>From Me To You - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.6130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.966</td>\n",
       "      <td>136.125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50014</th>\n",
       "      <td>62903</td>\n",
       "      <td>And I Love Her - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 10:01:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>A Hard Day's Night (Remastered)</td>\n",
       "      <td>And I Love Her - Remastered 2009</td>\n",
       "      <td>2017-01-01 16:01:00</td>\n",
       "      <td>360</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>And I Love Her - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.636</td>\n",
       "      <td>113.312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50015</th>\n",
       "      <td>62904</td>\n",
       "      <td>Ticket To Ride - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:58:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Help! (Remastered)</td>\n",
       "      <td>Ticket To Ride - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:58:00</td>\n",
       "      <td>9715</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Ticket To Ride - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.749</td>\n",
       "      <td>123.419</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50016</th>\n",
       "      <td>62905</td>\n",
       "      <td>Come Together - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:54:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Abbey Road (Remastered)</td>\n",
       "      <td>Come Together - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:54:00</td>\n",
       "      <td>7425</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Come Together - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.187</td>\n",
       "      <td>165.007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50017</th>\n",
       "      <td>62906</td>\n",
       "      <td>Penny Lane - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:51:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Magical Mystery Tour (Remastered)</td>\n",
       "      <td>Penny Lane - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:51:00</td>\n",
       "      <td>4401</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Penny Lane - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.490</td>\n",
       "      <td>113.038</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50018 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index_x                                       SongID  \\\n",
       "0            0                 Twenty Five MilesEdwin Starr   \n",
       "1            1                       Devil's EyesGreyhounds   \n",
       "2            2                          Pussy and PizzaMurs   \n",
       "3            8                   Our Special PlaceThe Heavy   \n",
       "4           10      Make Peace and be FreePerfect Confusion   \n",
       "...        ...                                          ...   \n",
       "50013    62902  From Me To You - Remastered 2009The Beatles   \n",
       "50014    62903  And I Love Her - Remastered 2009The Beatles   \n",
       "50015    62904  Ticket To Ride - Remastered 2009The Beatles   \n",
       "50016    62905   Come Together - Remastered 2009The Beatles   \n",
       "50017    62906      Penny Lane - Remastered 2009The Beatles   \n",
       "\n",
       "          TimeStamp_Central        Performer_x  \\\n",
       "0      5/25/2021 5:18:00 PM        Edwin Starr   \n",
       "1      5/25/2021 5:15:00 PM         Greyhounds   \n",
       "2      5/25/2021 5:12:00 PM               Murs   \n",
       "3      5/25/2021 4:46:00 PM          The Heavy   \n",
       "4      5/25/2021 4:39:00 PM  Perfect Confusion   \n",
       "...                     ...                ...   \n",
       "50013  1/1/2017 10:04:00 AM        The Beatles   \n",
       "50014  1/1/2017 10:01:00 AM        The Beatles   \n",
       "50015   1/1/2017 9:58:00 AM        The Beatles   \n",
       "50016   1/1/2017 9:54:00 AM        The Beatles   \n",
       "50017   1/1/2017 9:51:00 AM        The Beatles   \n",
       "\n",
       "                                         Album  \\\n",
       "0                                     25 Miles   \n",
       "1                               Change of Pace   \n",
       "2                             Have a Nice Life   \n",
       "3             Great Vengeance and Furious Fire   \n",
       "4                            Perfect Confusion   \n",
       "...                                        ...   \n",
       "50013  Past Masters (Vols. 1 & 2 / Remastered)   \n",
       "50014          A Hard Day's Night (Remastered)   \n",
       "50015                       Help! (Remastered)   \n",
       "50016                  Abbey Road (Remastered)   \n",
       "50017        Magical Mystery Tour (Remastered)   \n",
       "\n",
       "                                 Song_x        TimeStamp_UTC  index_y  \\\n",
       "0                     Twenty Five Miles  2021-05-25 23:18:00     9761   \n",
       "1                          Devil's Eyes  2021-05-25 23:15:00      206   \n",
       "2                       Pussy and Pizza  2021-05-25 23:12:00     6404   \n",
       "3                     Our Special Place  2021-05-25 22:46:00     6205   \n",
       "4                Make Peace and be Free  2021-05-25 22:39:00     6051   \n",
       "...                                 ...                  ...      ...   \n",
       "50013  From Me To You - Remastered 2009  2017-01-01 16:04:00     5693   \n",
       "50014  And I Love Her - Remastered 2009  2017-01-01 16:01:00      360   \n",
       "50015  Ticket To Ride - Remastered 2009  2017-01-01 15:58:00     9715   \n",
       "50016   Come Together - Remastered 2009  2017-01-01 15:54:00     7425   \n",
       "50017      Penny Lane - Remastered 2009  2017-01-01 15:51:00     4401   \n",
       "\n",
       "             Performer_y                            Song_y  ... mode  \\\n",
       "0            Edwin Starr                 Twenty Five Miles  ...  1.0   \n",
       "1             Greyhounds                      Devil's Eyes  ...  0.0   \n",
       "2                   Murs                   Pussy and Pizza  ...  1.0   \n",
       "3              The Heavy                 Our Special Place  ...  1.0   \n",
       "4      Perfect Confusion            Make Peace and be Free  ...  1.0   \n",
       "...                  ...                               ...  ...  ...   \n",
       "50013        The Beatles  From Me To You - Remastered 2009  ...  1.0   \n",
       "50014        The Beatles  And I Love Her - Remastered 2009  ...  0.0   \n",
       "50015        The Beatles  Ticket To Ride - Remastered 2009  ...  1.0   \n",
       "50016        The Beatles   Come Together - Remastered 2009  ...  0.0   \n",
       "50017        The Beatles      Penny Lane - Remastered 2009  ...  1.0   \n",
       "\n",
       "      speechiness acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0          0.0607       0.0595          0.000015    0.2240    0.964  124.567   \n",
       "1          0.0456       0.3540          0.000414    0.0974    0.858  113.236   \n",
       "2          0.0659       0.0708          0.000004    0.0780    0.381   93.991   \n",
       "3          0.0386       0.2720          0.003610    0.0991    0.939  193.996   \n",
       "4          0.0315       0.0138          0.000017    0.0649    0.431   78.037   \n",
       "...           ...          ...               ...       ...      ...      ...   \n",
       "50013      0.0309       0.6130          0.000000    0.2690    0.966  136.125   \n",
       "50014      0.0337       0.6400          0.000000    0.0681    0.636  113.312   \n",
       "50015      0.0678       0.0457          0.000000    0.2330    0.749  123.419   \n",
       "50016      0.0393       0.0302          0.248000    0.0926    0.187  165.007   \n",
       "50017      0.0316       0.2120          0.026000    0.1360    0.490  113.038   \n",
       "\n",
       "       time_signature       session_3_hour  session_id  \n",
       "0                 4.0  2021-05-25 21:00:00        4332  \n",
       "1                 4.0  2021-05-25 21:00:00        4332  \n",
       "2                 4.0  2021-05-25 21:00:00        4332  \n",
       "3                 4.0  2021-05-25 21:00:00        4332  \n",
       "4                 4.0  2021-05-25 21:00:00        4332  \n",
       "...               ...                  ...         ...  \n",
       "50013             4.0  2017-01-01 15:00:00           0  \n",
       "50014             4.0  2017-01-01 15:00:00           0  \n",
       "50015             4.0  2017-01-01 15:00:00           0  \n",
       "50016             4.0  2017-01-01 15:00:00           0  \n",
       "50017             4.0  2017-01-01 15:00:00           0  \n",
       "\n",
       "[50018 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "df = pd.read_csv(\"data/session-data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50018 entries, 0 to 50017\n",
      "Data columns (total 30 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index_x                    50018 non-null  int64  \n",
      " 1   SongID                     50018 non-null  object \n",
      " 2   TimeStamp_Central          50018 non-null  object \n",
      " 3   Performer_x                50018 non-null  object \n",
      " 4   Album                      47890 non-null  object \n",
      " 5   Song_x                     50018 non-null  object \n",
      " 6   TimeStamp_UTC              50018 non-null  object \n",
      " 7   index_y                    50018 non-null  int64  \n",
      " 8   Performer_y                50018 non-null  object \n",
      " 9   Song_y                     50018 non-null  object \n",
      " 10  spotify_genre              50018 non-null  object \n",
      " 11  spotify_track_id           50018 non-null  object \n",
      " 12  spotify_track_preview_url  36001 non-null  object \n",
      " 13  spotify_track_duration_ms  50018 non-null  float64\n",
      " 14  spotify_track_popularity   50018 non-null  float64\n",
      " 15  spotify_track_explicit     50018 non-null  bool   \n",
      " 16  danceability               50018 non-null  float64\n",
      " 17  energy                     50018 non-null  float64\n",
      " 18  key                        50018 non-null  float64\n",
      " 19  loudness                   50018 non-null  float64\n",
      " 20  mode                       50018 non-null  float64\n",
      " 21  speechiness                50018 non-null  float64\n",
      " 22  acousticness               50018 non-null  float64\n",
      " 23  instrumentalness           50018 non-null  float64\n",
      " 24  liveness                   50018 non-null  float64\n",
      " 25  valence                    50018 non-null  float64\n",
      " 26  tempo                      50018 non-null  float64\n",
      " 27  time_signature             50018 non-null  float64\n",
      " 28  session_3_hour             50018 non-null  object \n",
      " 29  session_id                 50018 non-null  int64  \n",
      "dtypes: bool(1), float64(14), int64(3), object(12)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        0.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "50013    1.0\n",
       "50014    0.0\n",
       "50015    1.0\n",
       "50016    0.0\n",
       "50017    1.0\n",
       "Name: mode, Length: 50018, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_col_name = 'mode'\n",
    "df.loc[:, test_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove N.A.N data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df[~df['danceability'].isna()]\n",
    "# df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature columns (as provided)\n",
    "feature_columns = [\n",
    "    'spotify_genre',\n",
    "    'spotify_track_popularity',\n",
    "    'danceability',\n",
    "    'loudness',\n",
    "    'acousticness',\n",
    "    'instrumentalness',\n",
    "    'tempo',\n",
    "]\n",
    "\n",
    "# Define the DataPreprocessor class\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, df, feature_columns, batch_size=16, fixed_genre_size=10, train_size=0.8):\n",
    "        \"\"\"\n",
    "        Initializes the data preprocessor with necessary parameters and preprocessing layers.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame containing session data.\n",
    "            feature_columns (list): List of feature column names.\n",
    "            batch_size (int): The batch size for dataset creation.\n",
    "            fixed_genre_size (int): The fixed size for genre vectorization.\n",
    "            train_size (float): Proportion of the data to use for training (between 0 and 1).\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_columns = feature_columns\n",
    "        self.batch_size = batch_size\n",
    "        self.fixed_genre_size = fixed_genre_size\n",
    "        self.train_size = train_size\n",
    "\n",
    "        # Split the dataset into training and testing datasets\n",
    "        self.train_df, self.test_df = train_test_split(self.df, train_size=self.train_size, random_state=42)\n",
    "        \n",
    "        # Numeric feature preprocessing\n",
    "        self.numeric_data = self.df[feature_columns[1:]].apply(pd.to_numeric, errors='coerce')\n",
    "        self.mean_values = self.numeric_data.mean()\n",
    "        self.std_values = self.numeric_data.std()\n",
    "\n",
    "        # Initialize LabelEncoder for SongID and spotify_genre\n",
    "        self.song_id_encoder = LabelEncoder()\n",
    "        self.genre_encoder = LabelEncoder()\n",
    "\n",
    "        # Extract unique SongIDs and genres\n",
    "        unique_song_ids = self.df['SongID'].unique()\n",
    "        all_genres = []\n",
    "        for genre_str in self.df['spotify_genre']:\n",
    "            try:\n",
    "                genre_list = ast.literal_eval(genre_str)  # Safely parse the string into a list\n",
    "                if isinstance(genre_list, list):\n",
    "                    all_genres.extend(genre_list)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing genre: {e}\")\n",
    "\n",
    "        unique_genres = list(set(all_genres))\n",
    "\n",
    "        # Fit the LabelEncoders on the data\n",
    "        self.song_id_encoder.fit(unique_song_ids)\n",
    "        self.genre_encoder.fit(unique_genres)\n",
    "\n",
    "        self.items_size = len(self.song_id_encoder.classes_)  # Number of unique SongIDs\n",
    "        self.genres_size = len(self.genre_encoder.classes_)  \n",
    "        \n",
    "        self.dataset = None\n",
    "\n",
    "    def preprocess_song_id(self, song_id):\n",
    "        \"\"\"\n",
    "        Encode the SongID using LabelEncoder.\n",
    "        \"\"\"\n",
    "        return self.song_id_encoder.transform([song_id])[0]\n",
    "\n",
    "    def clean_genre(self, value, default_value=0, dtype=tf.int32):\n",
    "        \"\"\"\n",
    "        Clean and process the 'spotify_genre' feature.\n",
    "        \"\"\"\n",
    "        if value is None or (isinstance(value, str) and not value.strip()):\n",
    "            return np.full((self.fixed_genre_size,), default_value, dtype=dtype.as_numpy_dtype)\n",
    "\n",
    "        try:\n",
    "            genre_list = eval(value) if isinstance(value, str) else value\n",
    "            if isinstance(genre_list, list):\n",
    "                genre_encoded = self.genre_encoder.transform(genre_list)\n",
    "            else:\n",
    "                genre_encoded = self.genre_encoder.transform([value])\n",
    "        except Exception:\n",
    "            genre_encoded = self.genre_encoder.transform([value])\n",
    "\n",
    "        # Pad or truncate to fixed size\n",
    "        return np.pad(genre_encoded, (0, max(0, self.fixed_genre_size - len(genre_encoded))),\n",
    "                      mode='constant')[:self.fixed_genre_size].astype(dtype.as_numpy_dtype)\n",
    "\n",
    "    def clean_numeric_feature(self, value, default_value=0.0, feature_name=\"feature\", mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Clean, process, and normalize numerical features using Z-score normalization.\n",
    "        \"\"\"\n",
    "        if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "            return default_value\n",
    "\n",
    "        try:\n",
    "            value = float(value)\n",
    "            # Apply Z-score normalization if mean and std are provided\n",
    "            if mean is not None and std is not None and std != 0:\n",
    "                z_score_value = (value - mean) / std\n",
    "                return z_score_value\n",
    "            return value  # Return raw value if no normalization\n",
    "        except ValueError:\n",
    "            return default_value\n",
    "\n",
    "    def create_session_dataset(self, session_df):\n",
    "        \"\"\"\n",
    "        Create session dataset as a list of dictionaries for each session.\n",
    "        \"\"\"\n",
    "        session_df = session_df.sort_values(by=['session_id', 'TimeStamp_UTC'])\n",
    "        grouped = session_df.groupby('session_id')\n",
    "        sessions_data = []\n",
    "        for session_id, group in grouped:\n",
    "            session_data = group.to_dict(orient='records')\n",
    "            sessions_data.append(session_data)\n",
    "        return sessions_data\n",
    "\n",
    "    def preprocess_data(self, sessions, k=1):\n",
    "        \"\"\"\n",
    "        Preprocess session data into TensorFlow dataset with split genre and features,\n",
    "        filtering out sequences where the next item sequence length is not greater than 10.\n",
    "        \"\"\"\n",
    "        item_sequences = []\n",
    "        next_item_sequences = []\n",
    "        genre_sequences = []\n",
    "        feature_sequences = []\n",
    "        processed_item_count = 0\n",
    "\n",
    "        for idx, session in enumerate(sessions):\n",
    "            session_item_sequences = []\n",
    "            session_next_item_sequences = []\n",
    "            session_genre_sequences = []\n",
    "            session_feature_sequences = []\n",
    "\n",
    "            for i in range(len(session) - 1):\n",
    "                # Process items\n",
    "                session_item_encoded = self.preprocess_song_id(session[i]['SongID'])\n",
    "                next_session_item_encoded = self.preprocess_song_id(session[i + 1]['SongID'])\n",
    "                session_item_sequences.append(session_item_encoded)\n",
    "                session_next_item_sequences.append(next_session_item_encoded)\n",
    "\n",
    "                # Process genre\n",
    "                genre_cleaned = self.clean_genre(session[i].get('spotify_genre', None))\n",
    "                session_genre_sequences.append(genre_cleaned)\n",
    "\n",
    "                # Process numerical features\n",
    "                numeric_features = []\n",
    "                for col in self.feature_columns:\n",
    "                    if col != 'spotify_genre':\n",
    "                        mean = self.mean_values.get(col, None)\n",
    "                        std = self.std_values.get(col, None)\n",
    "                        cleaned_feature = self.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n",
    "                        numeric_features.append(cleaned_feature)\n",
    "\n",
    "                session_feature_sequences.append(numeric_features)\n",
    "\n",
    "            # Filter out sessions where the next item sequence length is not greater than 10\n",
    "            if len(session_next_item_sequences) > k:\n",
    "                # Extend sequences only if the next item sequence length is greater than 10\n",
    "                item_sequences.extend(session_item_sequences)\n",
    "                next_item_sequences.extend(session_next_item_sequences)\n",
    "                genre_sequences.extend(session_genre_sequences)\n",
    "                feature_sequences.extend(session_feature_sequences)\n",
    "                processed_item_count += len(session_item_sequences)\n",
    "\n",
    "                # print(f\"Session {idx + 1} processed with {len(session_item_sequences)} items.\")\n",
    "            # else:\n",
    "            #     print(f\"Session {idx + 1} skipped because next item sequence length is {len(session_next_item_sequences)}.\")\n",
    "\n",
    "        print(f\"Total processed items: {processed_item_count}\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        item_sequences = tf.stack(item_sequences, axis=-1)\n",
    "        next_item_sequences = tf.stack(next_item_sequences, axis=-1)\n",
    "        genre_sequences_tensor = tf.constant(genre_sequences, dtype=tf.int32)\n",
    "        feature_sequences_tensor = tf.constant(feature_sequences, dtype=tf.float32)\n",
    "\n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'item': item_sequences,\n",
    "            'genre': genre_sequences_tensor,\n",
    "            'features': feature_sequences_tensor,\n",
    "            'next_item': next_item_sequences\n",
    "        })\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_session_dataset_tensor(self, k):\n",
    "        \"\"\"\n",
    "        Main function to create session dataset as tensors and return the dataset.\n",
    "        \"\"\"\n",
    "        if self.dataset is not None:\n",
    "            print(\"Dataset already created\")\n",
    "            return\n",
    "        \n",
    "        print(\"Creating session dataset\")\n",
    "        sessions_data = self.create_session_dataset(self.train_df)  # Use train data for training\n",
    "        print(\"Creating tensor dataset\")\n",
    "        dataset = self.preprocess_data(sessions_data, k=k)\n",
    "\n",
    "        # Shuffle and batch the training data\n",
    "        dataset = dataset.shuffle(buffer_size=1024).batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        return dataset\n",
    "\n",
    "    def get_test_data(self, k):\n",
    "        \"\"\"\n",
    "        Return preprocessed test dataset without shuffling.\n",
    "        \"\"\"\n",
    "        sessions_data = self.create_session_dataset(self.test_df)\n",
    "        dataset = self.preprocess_data(sessions_data, k)\n",
    "\n",
    "        # Batch the test data without shuffling\n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def batch_timer(self, dataset):\n",
    "        \"\"\"\n",
    "        Timer function to track the time taken for batch processing.\n",
    "        \"\"\"\n",
    "        for batch in dataset:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Simulate processing (e.g., model training or data transformation)\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            print(f\"Batch processing time: {batch_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating session dataset\n",
      "Creating tensor dataset\n",
      "Total processed items: 664\n",
      "Total processed items: 73\n"
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor(df[:1000], feature_columns)\n",
    "\n",
    "# Create the session dataset tensor\n",
    "train_dataset = preprocessor.create_session_dataset_tensor(k=2)\n",
    "test_dataset = preprocessor.get_test_data(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items (SongID): [[286]\n",
      " [229]\n",
      " [472]\n",
      " [624]\n",
      " [ 57]\n",
      " [ 47]\n",
      " [282]\n",
      " [  0]\n",
      " [ 87]\n",
      " [220]\n",
      " [530]\n",
      " [ 30]\n",
      " [488]\n",
      " [230]\n",
      " [535]\n",
      " [546]]\n",
      "Genre: [[122 128 147 151 201 205 216 271 299   0]\n",
      " [102 243 315   0   0   0   0   0   0   0]\n",
      " [  6  21  38  68 118 256 271 276   0   0]\n",
      " [  6  21  56  66  68 140 191 254 271 303]\n",
      " [115 167   0   0   0   0   0   0   0   0]\n",
      " [122 128 147 151 201 205 216 271 299   0]\n",
      " [ 13  26 128 201 205 247 271   0   0   0]\n",
      " [  6  21  38  68 140 191 256 271 290   0]\n",
      " [ 68  78 118 163 186 256 271 276   0   0]\n",
      " [  4  92 186 207 259 290   0   0   0   0]\n",
      " [  6  68 118 142 186 246 271 276 286 290]\n",
      " [ 13  26 128 201 205 247 271   0   0   0]\n",
      " [ 12 227 247 262   0   0   0   0   0   0]\n",
      " [  6  38  68 140 191 256 271   0   0   0]\n",
      " [234   0   0   0   0   0   0   0   0   0]\n",
      " [109 146 148 150 152 153 205 212 243 246]]\n",
      "Features: [[-1.371887    0.25843132 -0.07427277 -0.10605697  0.54191977  0.4642636 ]\n",
      " [ 1.8506663   0.70177585  0.14341177 -0.77978295 -0.35405225 -0.59098107]\n",
      " [ 0.7573     -0.5126026  -0.43825653  0.90926445 -0.3540921   0.3735671 ]\n",
      " [-0.62379426 -1.5470732  -0.37447116 -0.2996564  -0.339466    0.17505842]\n",
      " [-1.4294325   0.94593656 -0.32317144  0.26393303 -0.3540921   0.22332142]\n",
      " [-1.7171605  -1.8233603  -4.1280365   1.7395908   4.8902125  -0.32119635]\n",
      " [ 0.5846632  -2.0546706   1.3425767  -0.6515771  -0.20783132  1.1519096 ]\n",
      " [ 1.1601192  -1.611326   -0.23414335 -0.8335605  -0.35270196  0.68059975]\n",
      " [-0.50870305  0.6246724  -0.31014293  1.5330848  -0.35406336 -0.7446499 ]\n",
      " [ 0.18184407 -0.62183243 -1.7416495   0.93507767 -0.3540921  -2.0438871 ]\n",
      " [-1.6020694   0.20702904  0.10595483  2.234345   -0.35150948  0.02810032]\n",
      " [-1.1417046  -1.8554868   1.2253202  -0.8453013  -0.33821422 -0.03968479]\n",
      " [ 0.92993677 -1.155131    1.2891057  -0.8456708  -0.2789852   1.7789897 ]\n",
      " [ 0.23938967 -1.836211    0.01095534 -0.7242844  -0.31627512  1.7742108 ]\n",
      " [-1.0266134  -0.49332678  0.49952415 -0.8447807  -0.00622866  2.1739736 ]\n",
      " [ 0.92993677  0.296983   -0.43228513  0.7586871  -0.35371327  0.7618741 ]]\n",
      "Next Items (Next SongID): [354  68 188 429 366 610 535  96  25 225 634 178 435 382 178  87]\n",
      "Items (SongID): [[526]\n",
      " [110]\n",
      " [ 35]\n",
      " [434]\n",
      " [ 13]\n",
      " [ 15]\n",
      " [521]\n",
      " [400]\n",
      " [409]\n",
      " [445]\n",
      " [580]\n",
      " [501]\n",
      " [191]\n",
      " [458]\n",
      " [379]\n",
      " [450]]\n",
      "Genre: [[ 11  76 198 245   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 23 121 122 128 151 201 205 213   0   0]\n",
      " [  6  68 118 142 186 246 271 276 286 290]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [  6 186 271 290 334   0   0   0   0   0]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [  6 123 133 140 271   0   0   0   0   0]\n",
      " [  6  38  68 140 256 271   0   0   0   0]\n",
      " [ 45 190 256 271   0   0   0   0   0   0]\n",
      " [  6  38  68 140 256 271   0   0   0   0]\n",
      " [ 12  13  73 137 138 140 155 156 191 227]\n",
      " [ 12 123 227 247 262 271   0   0   0   0]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [266   0   0   0   0   0   0   0   0   0]\n",
      " [266   0   0   0   0   0   0   0   0   0]]\n",
      "Features: [[ 0.0092073   1.9161543   0.628452    0.69415396 -0.3540921  -0.830093  ]\n",
      " [-1.8322517   1.0294652  -0.07182993 -0.6657744  -0.347372   -1.0301946 ]\n",
      " [-0.68133986 -0.28129247  0.48323852 -0.5952182  -0.3540921  -1.1172308 ]\n",
      " [ 0.6997544  -0.724637    0.35729635 -0.84347284  3.7306678   1.640708  ]\n",
      " [-0.68133986  0.7596034  -0.42414233 -0.76214385  1.8134661   0.31594977]\n",
      " [ 1.6204839   0.5604196  -2.816501    0.25963083 -0.35356832 -0.9717639 ]\n",
      " [ 0.23938967  0.72747695 -0.06884423  0.11765791 -0.26317325 -1.1419723 ]\n",
      " [-0.50870305 -0.57043016 -0.32642856 -0.820826   -0.3449343  -0.6793051 ]\n",
      " [ 0.92993677 -0.32626945  0.6561376  -0.8083496   3.4539583  -0.42328072]\n",
      " [ 0.5271176  -0.8531426  -0.38749966 -0.47733763 -0.33926836 -0.87700033]\n",
      " [ 0.6422088  -2.3759346   0.27831104  1.2190235  -0.29881606  1.8360308 ]\n",
      " [-0.62379426 -1.1101539   0.7077087  -0.8459247   1.8200543   2.1206267 ]\n",
      " [ 0.98748237 -1.4828204   0.49138132 -0.845941    0.33768177 -0.61331624]\n",
      " [-2.0048885  -0.76318866  0.07121216  2.38062    -0.35408017 -1.3149599 ]\n",
      " [ 0.06675289 -1.1744068   0.39611042  1.1157705  -0.2986184  -1.0298219 ]\n",
      " [-0.10588389  1.1900973   0.45853865 -0.79225934 -0.06684123  1.6466392 ]]\n",
      "Next Items (Next SongID): [110  35  53  13  15 521 400 591 445 580 222 191 458 369 450  21]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "    print(\"Items (SongID):\", batch['item'].numpy())\n",
    "    print(\"Genre:\", batch['genre'].numpy())\n",
    "    print(\"Features:\", batch['features'].numpy())\n",
    "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())\n",
    "    \n",
    "for batch in test_dataset.take(1):\n",
    "    print(\"Items (SongID):\", batch['item'].numpy())\n",
    "    print(\"Genre:\", batch['genre'].numpy())\n",
    "    print(\"Features:\", batch['features'].numpy())\n",
    "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemEmbedding(Layer):\n",
    "    def __init__(self, num_items, item_embed_dim):\n",
    "        super(ItemEmbedding, self).__init__()\n",
    "        \n",
    "        self.item_embedding = Embedding(input_dim=num_items, output_dim=item_embed_dim, mask_zero=True, name='item_embedding')\n",
    "\n",
    "    def call(self, items):\n",
    "        # Embed items\n",
    "        items_embedded = self.item_embedding(items)\n",
    "        return items_embedded\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, name='attention'):\n",
    "        super(Attention, self).__init__(name=name)\n",
    "        self.attention_dense = Dense(1, activation='tanh', name='attention_dense')\n",
    "\n",
    "    def call(self, rnn_outputs):\n",
    "        \"\"\"\n",
    "        Apply attention mechanism to the RNN outputs.\n",
    "        :param rnn_outputs: Tensor of shape (batch_size, sequence_length, rnn_units)\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = self.attention_dense(rnn_outputs)  # Shape: (batch_size, sequence_length, 1)\n",
    "        scores = tf.nn.softmax(scores, axis=1)  # Normalize scores along the sequence dimension\n",
    "\n",
    "        # Compute context vector as a weighted sum of RNN outputs\n",
    "        context_vector = tf.reduce_sum(rnn_outputs * scores, axis=1)  # Shape: (batch_size, rnn_units)\n",
    "        return context_vector, scores\n",
    "\n",
    "class GRU4REC(Model):\n",
    "    def __init__(self, rnn_params, genre_embed_dim, item_embed_dim, ffn1_units, feature_dense_units, preprocessed_data: DataPreprocessor):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        print(f\"items size: {preprocessed_data.items_size}\")\n",
    "        print(f\"genres size: {preprocessed_data.genres_size}\")\n",
    "\n",
    "        self.embedding = ItemEmbedding(preprocessed_data.items_size, item_embed_dim)\n",
    "\n",
    "        # Genre embedding (only for genre, which is categorical and a string)\n",
    "        self.genre_embedding = Embedding(input_dim=preprocessed_data.genres_size, output_dim=genre_embed_dim, mask_zero=True, name='genre_embedding')\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn_layers = []\n",
    "        self.rnn_layers.append(GRU(**rnn_params[0], return_sequences=True, name='gru_0'))\n",
    "        for i in range(1, len(rnn_params) - 1):\n",
    "            self.rnn_layers.append(GRU(**rnn_params[i], return_sequences=True, name=f'gru_{i}'))\n",
    "        self.rnn_layers.append(GRU(**rnn_params[-1], return_sequences=True, name=f\"gru_{len(rnn_params)-1})\"))\n",
    "\n",
    "        self.attention = Attention(name='attention')\n",
    "\n",
    "        self.concat = Concatenate(axis=-1, name='concat_1')\n",
    "        self.batch_norm = BatchNormalization(name='batchnorm')\n",
    "\n",
    "        # Feed-forward layers\n",
    "        self.feature_dense = Dense(feature_dense_units, activation='relu', name='feature_dense')  # Dense layer for features (if required)\n",
    "        self.ffn1 = Dense(ffn1_units, name='feed_forward_1')\n",
    "        self.activation1 = LeakyReLU(alpha=0.2, name='freaky_relu')\n",
    "        self.out = Dense(preprocessed_data.items_size, activation='softmax', name='output_layer')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass for the GRU4REC model.\n",
    "        :param inputs: Tuple (item_sequences, item_features, item_genres)\n",
    "        :param training: Boolean indicating if the model is in training mode\n",
    "        \"\"\"\n",
    "        item_sequences, item_features, item_genres = inputs\n",
    "\n",
    "        # Embed items (shape: (batch_size, sequence_length, item_embed_dim))\n",
    "        item_embedded = self.embedding(item_sequences)\n",
    "\n",
    "        # Pass through RNN layers (shape: (batch_size, sequence_length, rnn_units))\n",
    "        x = item_embedded\n",
    "        for rnn_layer in self.rnn_layers:\n",
    "            x = rnn_layer(x)\n",
    "\n",
    "        # Apply attention (shape: (batch_size, rnn_units))\n",
    "        context_vector, attention_scores = self.attention(x)\n",
    "\n",
    "        # Feature transformation (features are passed directly as floats, so no embedding is needed)\n",
    "        feature_transformed = self.feature_dense(item_features)\n",
    "\n",
    "        # Embed genres and compute mean across genre sequence (shape: (batch_size, genre_embed_dim))\n",
    "        genre_embedded = self.genre_embedding(item_genres)\n",
    "        genre_embedded = tf.reduce_mean(genre_embedded, axis=1)\n",
    "\n",
    "        # Concatenate context vector with feature embeddings and genre embeddings\n",
    "        x = self.concat([context_vector, feature_transformed, genre_embedded])\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        # Feed-forward layers\n",
    "        x = self.ffn1(x)\n",
    "        x = self.activation1(x)\n",
    "        logits = self.out(x)  # Shape: (batch_size, num_items)\n",
    "\n",
    "        # Generate the sequence of items (choose the item with the highest probability using argmax)\n",
    "        predicted_items = tf.argmax(logits, axis=-1)  # (batch_size,)\n",
    "\n",
    "        assert len(item_sequences.shape) == 3, f\"Expected shape (batch_size, seq_len, feature_dim), got {item_sequences.shape}\"\n",
    "        assert len(item_features.shape) == 3, f\"Expected shape (batch_size, seq_len, num_features), got {item_features.shape}\"\n",
    "        assert len(item_genres.shape) == 2\n",
    "\n",
    "        return predicted_items, logits, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.metrics import Recall\n",
    "\n",
    "class RecallAtK(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k=10, name=\"recall_at_k\", **kwargs):\n",
    "        super(RecallAtK, self).__init__(name=name, **kwargs)\n",
    "        self.k = k\n",
    "        self.recall_at_k = Recall(top_k=self.k)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Update the state of the metric.\n",
    "        \"\"\"\n",
    "        # Since y_true is a list of true items and y_pred are the predicted scores,\n",
    "        # we need to calculate recall for top-k predicted items\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Calculate the top-k predicted items\n",
    "        top_k_preds = tf.argsort(y_pred, axis=-1, direction='DESCENDING')[:, :self.k]\n",
    "\n",
    "        # Calculate recall by comparing true labels with the top-k predictions\n",
    "        recall = tf.reduce_mean(tf.cast(tf.equal(y_true, top_k_preds), tf.float32), axis=-1)\n",
    "        return recall\n",
    "\n",
    "    def result(self):\n",
    "        return self.recall_at_k.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.recall_at_k.reset_state()\n",
    "\n",
    "def train_gru4rec(model, train_dataset, optimizer, loss_fn, epochs, k, val_dataset=None):\n",
    "    # Create the RecallAtK metric\n",
    "    recall_at_k = RecallAtK(k=k)\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Reset metrics for the epoch\n",
    "        # recall_at_k.reset_state()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Train the model using the dataset\n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            item_sequences = batch['item']\n",
    "            item_genres = batch['genre']\n",
    "            item_features = batch['features']\n",
    "            targets = batch['next_item']\n",
    "            # print(f\"item_sequences:\", item_sequences)\n",
    "            # print(f\"item_genres: \", item_genres)\n",
    "            # print(f\"item_features:\", item_features)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass: outputs is a tuple (predicted_items, logits)\n",
    "                predicted_items, logits, attention_scores = model((item_sequences, item_features, item_genres), training=True)\n",
    "                logits = tf.squeeze(logits, axis=1)\n",
    "                # print(logits)  # Log logits for debugging\n",
    "                \n",
    "                # Compute the loss using logits (not predicted_items)\n",
    "                loss = loss_fn(targets, logits)  # Use logits for loss calculation\n",
    "                # print(f\"Epoch {epoch+1}, Step {step+1} Loss = {loss}, Attention score: {attention_scores}\")\n",
    "                epoch_loss += loss.numpy()\n",
    "            \n",
    "            # Compute gradients and apply them\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Update Recall@k metric using logits\n",
    "            # recall_at_k.update_state(targets, logits)  # Use logits for metric calculation\n",
    "\n",
    "            # # Generate the predicted sequence (for Recall at k)\n",
    "            # input_sequence = item_sequences\n",
    "            # num_predictions = k  # Length of the sequence to predict\n",
    "\n",
    "            # predicted_sequence = []\n",
    "            # input_seq = input_sequence  # Start with the initial sequence\n",
    "\n",
    "            # for _ in range(num_predictions):\n",
    "            #     # Get the model's prediction (logits) for the next item\n",
    "            #     predicted_items, logits = model(input_seq, training=False)\n",
    "\n",
    "            #     # Add the predicted item (argmax) to the sequence\n",
    "            #     predicted_sequence.append(predicted_items)\n",
    "\n",
    "            #     # Update the input sequence by appending the predicted item\n",
    "            #     input_seq = tf.concat([input_seq, predicted_items[:, -1:]], axis=-1)  # Append the last predicted item\n",
    "\n",
    "            # predicted_sequence = tf.stack(predicted_sequence, axis=1)  # Shape: [batch_size, num_predictions]\n",
    "\n",
    "            # # Calculate recall by comparing predicted sequence with targets\n",
    "            # for batch_idx in range(predicted_sequence.shape[0]):\n",
    "            #     top_k_preds = predicted_sequence[batch_idx]  # Predicted top-k items for this batch item\n",
    "            #     true_item = targets[batch_idx]  # True next item for this batch item\n",
    "                \n",
    "            #     # Check if true item is in the top-k predictions\n",
    "            #     if true_item in top_k_preds.numpy():\n",
    "            #         recall_at_k.update_state(tf.convert_to_tensor([true_item]), predicted_sequence)  # Update recall metric\n",
    "        \n",
    "        # Calculate average loss and Recall@k for the training epoch\n",
    "        avg_loss = epoch_loss / (step + 1)\n",
    "        # train_recall_at_k = recall_at_k.result().numpy()\n",
    "        print(f\"Epoch {epoch+1}, Training loss: {avg_loss:.4f}, Attention scores: {attention_scores}\")\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            continue\n",
    "        \n",
    "        # Validation step\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(val_dataset):\n",
    "            item_sequences = batch['item']\n",
    "            item_genres = batch['genre']\n",
    "            item_features = batch['features']\n",
    "            targets = batch['next_item']\n",
    "            # print(f\"item_sequences:\", item_sequences)\n",
    "            # print(f\"item_genres: \", item_genres)\n",
    "            # print(f\"item_features:\", item_features)\n",
    "            # Forward pass on validation set\n",
    "            predicted_items, logits = model((item_sequences, item_features, item_genres), training=False)\n",
    "            logits = tf.squeeze(logits, axis=1)\n",
    "            loss = loss_fn(targets, logits)\n",
    "            val_loss += loss.numpy()\n",
    "            \n",
    "            # # Update validation metrics\n",
    "            # recall_at_k.update_state(targets, logits)\n",
    "        \n",
    "        avg_val_loss = val_loss / (step + 1)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "def plot_training_history(loss_history, metric_history, metric_name, top_k):\n",
    "    \"\"\"Plot the training loss and accuracy.\"\"\"\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    # Create subplots for loss and accuracy\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Plot the training loss\n",
    "    ax1.plot(epochs, loss_history, label='Loss', color='blue', linestyle='-', marker='o')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot the top-k accuracy\n",
    "    ax2.plot(epochs, metric_history, label=metric_name, color='green', linestyle='-', marker='o')\n",
    "    ax2.set_title(f'Training {metric_name}')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel(metric_name)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items size: 635\n",
      "genres size: 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# num_items = len()\n",
    "# feature_vocab_size = len(feature_columns)\n",
    "\n",
    "model = GRU4REC(\n",
    "    rnn_params=[\n",
    "        {\"units\": 128, \"dropout\": 0.3},\n",
    "        {\"units\": 64, \"dropout\": 0.3}\n",
    "    ],\n",
    "    item_embed_dim=16,\n",
    "    genre_embed_dim=16,\n",
    "    ffn1_units=128,\n",
    "    feature_dense_units=32,\n",
    "    preprocessed_data=preprocessor\n",
    ")\n",
    "\n",
    "# model = GRU4REC(\n",
    "#     rnn_params=[\n",
    "#         {\"units\": 128},\n",
    "#         {\"units\": 128},\n",
    "#         {\"units\": 64}\n",
    "#     ],\n",
    "#     item_embed_dim=32,\n",
    "#     genre_embed_dim=16,\n",
    "#     ffn1_units=128,\n",
    "#     feature_dense_units=64,\n",
    "#     preprocessed_data=preprocessor\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1381: UserWarning: Layer 'gru4rec' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: '''gru_1)' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$''\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'gru4rec', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'attention' (of type Attention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Exception encountered when calling GRU4REC.call().\n\n\u001b[1mExpected shape (batch_size, seq_len, feature_dim), got (16, 1)\u001b[0m\n\nArguments received by GRU4REC.call():\n  â€¢ inputs=('tf.Tensor(shape=(16, 1), dtype=int32)', 'tf.Tensor(shape=(16, 6), dtype=float32)', 'tf.Tensor(shape=(16, 10), dtype=int32)')\n  â€¢ training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_gru4rec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 54\u001b[0m, in \u001b[0;36mtrain_gru4rec\u001b[1;34m(model, train_dataset, optimizer, loss_fn, epochs, k, val_dataset)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# print(f\"item_sequences:\", item_sequences)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# print(f\"item_genres: \", item_genres)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# print(f\"item_features:\", item_features)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Forward pass: outputs is a tuple (predicted_items, logits)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     predicted_items, logits, attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_genres\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     logits \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# print(logits)  # Log logits for debugging\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Compute the loss using logits (not predicted_items)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[9], line 97\u001b[0m, in \u001b[0;36mGRU4REC.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Generate the sequence of items (choose the item with the highest probability using argmax)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m predicted_items \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size,)\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item_sequences\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape (batch_size, seq_len, feature_dim), got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_sequences\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item_features\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape (batch_size, seq_len, num_features), got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_features\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item_genres\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Exception encountered when calling GRU4REC.call().\n\n\u001b[1mExpected shape (batch_size, seq_len, feature_dim), got (16, 1)\u001b[0m\n\nArguments received by GRU4REC.call():\n  â€¢ inputs=('tf.Tensor(shape=(16, 1), dtype=int32)', 'tf.Tensor(shape=(16, 6), dtype=float32)', 'tf.Tensor(shape=(16, 10), dtype=int32)')\n  â€¢ training=True"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Train the model\n",
    "train_gru4rec(model=model, train_dataset=train_dataset,optimizer=optimizer, loss_fn=loss_fn, epochs=50, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 recall: 0.9375\n",
      "Batch 2 recall: 0.7500\n",
      "Batch 3 recall: 0.6250\n",
      "Batch 4 recall: 0.9375\n",
      "Batch 5 recall: 0.6250\n",
      "Batch 6 recall: 0.8750\n",
      "Batch 7 recall: 0.8125\n",
      "Batch 8 recall: 0.8125\n",
      "Batch 9 recall: 0.8125\n",
      "Batch 10 recall: 0.8125\n",
      "Batch 11 recall: 0.6875\n",
      "Batch 12 recall: 0.7500\n",
      "Batch 13 recall: 0.8125\n",
      "Batch 14 recall: 0.7500\n",
      "Batch 15 recall: 0.7500\n",
      "Batch 16 recall: 0.8125\n",
      "Batch 17 recall: 0.7500\n",
      "Batch 18 recall: 0.7500\n",
      "Batch 19 recall: 0.8125\n",
      "Batch 20 recall: 1.0625\n",
      "Batch 21 recall: 0.5000\n",
      "Batch 22 recall: 0.8750\n",
      "Batch 23 recall: 0.7500\n",
      "Batch 24 recall: 0.6875\n",
      "Batch 25 recall: 0.7500\n",
      "Batch 26 recall: 0.8125\n",
      "Batch 27 recall: 1.0000\n",
      "Batch 28 recall: 0.5000\n",
      "Batch 29 recall: 0.6875\n",
      "Batch 30 recall: 0.8125\n",
      "Batch 31 recall: 0.6250\n",
      "Batch 32 recall: 0.7500\n",
      "Batch 33 recall: 0.6250\n",
      "Batch 34 recall: 0.5625\n",
      "Batch 35 recall: 0.8125\n",
      "Batch 36 recall: 0.8750\n",
      "Batch 37 recall: 0.6875\n",
      "Batch 38 recall: 0.6250\n",
      "Batch 39 recall: 0.7500\n",
      "Batch 40 recall: 0.6875\n",
      "Batch 41 recall: 0.8125\n",
      "Overall recall: 0.7591\n"
     ]
    }
   ],
   "source": [
    "def predict(model, item_sequences, item_features, item_genres):\n",
    "    \"\"\"\n",
    "    Predict the item with the highest probability for the given input sequences using argmax of softmax.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
    "    - item_features: Input item features (batch_size, feature_length).\n",
    "    - item_genres: Input item genres (batch_size, genre_length).\n",
    "\n",
    "    Returns:\n",
    "    - predicted_items: A list of predicted items with the highest probability for each input sequence.\n",
    "    \"\"\"\n",
    "    # Run the model in inference mode (not training)\n",
    "    predicted_items, logits = model((item_sequences, item_features, item_genres), training=False)\n",
    "    \n",
    "    # Apply softmax to the logits to get probabilities\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Get the item with the highest probability by finding the index of the maximum probability\n",
    "    predicted_items = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    # Convert to numpy array for easier handling\n",
    "    predicted_items = predicted_items.numpy()\n",
    "\n",
    "    return predicted_items\n",
    "\n",
    "def compute_recall(predicted_items, targets):\n",
    "    \"\"\"\n",
    "    Compute the recall for the given predictions and targets.\n",
    "\n",
    "    Args:\n",
    "    - predicted_items: The predicted items (batch_size,).\n",
    "    - targets: The actual next items (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "    - recall: The recall metric.\n",
    "    \"\"\"\n",
    "    # True positives: Predicted item matches the target\n",
    "    true_positives = np.sum(predicted_items == targets)\n",
    "    \n",
    "    # Total relevant items (in this case, it is the number of items in the batch)\n",
    "    total_items = len(targets)\n",
    "    \n",
    "    # Recall calculation\n",
    "    recall = true_positives / total_items if total_items > 0 else 0\n",
    "    return recall\n",
    "\n",
    "# Initialize variables to calculate overall recall\n",
    "total_true_positives = 0\n",
    "total_items = 0\n",
    "\n",
    "# Loop through training dataset and predict the most probable item\n",
    "for step, batch in enumerate(train_dataset):\n",
    "    item_sequences = batch['item']\n",
    "    item_genres = batch['genre']\n",
    "    item_features = batch['features']\n",
    "    targets = batch['next_item']\n",
    "    \n",
    "    # Get the predicted item with the highest probability for each sequence in the batch\n",
    "    predicted_items = predict(model, item_sequences, item_features, item_genres)\n",
    "    \n",
    "    # Compute recall for the current batch\n",
    "    batch_recall = compute_recall(predicted_items, targets)\n",
    "    print(f\"Batch {step + 1} recall: {batch_recall:.4f}\")\n",
    "    \n",
    "    # Accumulate for overall recall\n",
    "    total_true_positives += np.sum(predicted_items == targets)\n",
    "    total_items += len(targets)\n",
    "\n",
    "# Calculate overall recall\n",
    "overall_recall = total_true_positives / total_items if total_items > 0 else 0\n",
    "print(f\"Overall recall: {overall_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('gru4rec_model.h5', save_format='h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
