{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true,"include_colab_link":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/zeev-haydar/ML-TuneHive/blob/model-development%2Fhaidar/model-dev/preprocessing-modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1TZ3huHSIkB","outputId":"3a68ad79-241c-445a-a509-0e1b6b870fa5","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:54:00.943889Z","iopub.execute_input":"2024-12-12T11:54:00.944493Z","iopub.status.idle":"2024-12-12T11:54:02.064404Z","shell.execute_reply.started":"2024-12-12T11:54:00.944450Z","shell.execute_reply":"2024-12-12T11:54:02.063451Z"}},"outputs":[{"name":"stdout","text":"Thu Dec 12 11:54:01 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Preprocessing and Modeling","metadata":{"id":"k7iFBw8Iau9f"}},{"cell_type":"markdown","source":"## Import required modules","metadata":{"id":"R7tdLXmCau9h"}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow_datasets as tfds\n\nfrom keras.api.layers import Dense, Embedding, GRU, LeakyReLU, Concatenate, Masking, Layer, StringLookup, Normalization, BatchNormalization, Attention, Dropout, Lambda\n\nfrom keras.api import Input\n\nfrom keras.api.models import Model\n\nfrom keras.api.losses import SparseCategoricalCrossentropy\n\nfrom keras.api.metrics import SparseCategoricalAccuracy, Mean, TopKCategoricalAccuracy\n\n# from transformers.models.bert import TFBertTokenizer, TFBertEmbeddings  # embedding and tokenizer for description/nlp related stufff\n\nfrom keras.api.optimizers import Adam\n\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nimport numpy as np\n\nimport ast","metadata":{"id":"ni2TKE2oau9i","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:26:36.401607Z","iopub.execute_input":"2024-12-12T12:26:36.402287Z","iopub.status.idle":"2024-12-12T12:26:36.407635Z","shell.execute_reply.started":"2024-12-12T12:26:36.402255Z","shell.execute_reply":"2024-12-12T12:26:36.406678Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"id":"PEtBlEo2au9j"}},{"cell_type":"markdown","source":"### Load CSV","metadata":{"id":"hkeNofysau9k"}},{"cell_type":"code","source":"# For Running in Google Colab\n\n# \"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{file}\"\n\nurl = \"https://raw.githubusercontent.com/zeev-haydar/ML-TuneHive/main/model-dev/data/session-data.csv\"\n\n!wget --no-cache --backups=1 {url}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhWy8OaOaxMX","outputId":"4868560b-25dd-46bb-969a-7bddf1508b4f","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:57:06.026994Z","iopub.execute_input":"2024-12-12T11:57:06.027375Z","iopub.status.idle":"2024-12-12T11:57:09.663648Z","shell.execute_reply.started":"2024-12-12T11:57:06.027344Z","shell.execute_reply":"2024-12-12T11:57:09.662762Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-12-12 11:57:06--  https://raw.githubusercontent.com/zeev-haydar/ML-TuneHive/main/model-dev/data/session-data.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21952709 (21M) [text/plain]\nFailed to rename session-data.csv to session-data.csv.1: (2) No such file or directory\nSaving to: 'session-data.csv'\n\nsession-data.csv    100%[===================>]  20.94M  41.5MB/s    in 0.5s    \n\n2024-12-12 11:57:09 (41.5 MB/s) - 'session-data.csv' saved [21952709/21952709]\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import keras\n\nimport os\n\nprint(keras.__version__)\n\n\n\n# root_path = \"data\"\n\nroot_path = \"\" # if using colab\n\ndf = pd.read_csv(os.path.join(root_path, \"session-data.csv\"))\n\ndf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":947},"id":"hFAqYSf1au9k","outputId":"c024e548-6dd9-48ee-8490-b72022e84286","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:07:52.909466Z","iopub.execute_input":"2024-12-12T12:07:52.910449Z","iopub.status.idle":"2024-12-12T12:07:53.369835Z","shell.execute_reply.started":"2024-12-12T12:07:52.910414Z","shell.execute_reply":"2024-12-12T12:07:53.368813Z"}},"outputs":[{"name":"stdout","text":"3.3.3\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       index_x                                       SongID  \\\n0            0                 Twenty Five MilesEdwin Starr   \n1            1                       Devil's EyesGreyhounds   \n2            2                          Pussy and PizzaMurs   \n3            8                   Our Special PlaceThe Heavy   \n4           10      Make Peace and be FreePerfect Confusion   \n...        ...                                          ...   \n50013    62902  From Me To You - Remastered 2009The Beatles   \n50014    62903  And I Love Her - Remastered 2009The Beatles   \n50015    62904  Ticket To Ride - Remastered 2009The Beatles   \n50016    62905   Come Together - Remastered 2009The Beatles   \n50017    62906      Penny Lane - Remastered 2009The Beatles   \n\n          TimeStamp_Central        Performer_x  \\\n0      5/25/2021 5:18:00 PM        Edwin Starr   \n1      5/25/2021 5:15:00 PM         Greyhounds   \n2      5/25/2021 5:12:00 PM               Murs   \n3      5/25/2021 4:46:00 PM          The Heavy   \n4      5/25/2021 4:39:00 PM  Perfect Confusion   \n...                     ...                ...   \n50013  1/1/2017 10:04:00 AM        The Beatles   \n50014  1/1/2017 10:01:00 AM        The Beatles   \n50015   1/1/2017 9:58:00 AM        The Beatles   \n50016   1/1/2017 9:54:00 AM        The Beatles   \n50017   1/1/2017 9:51:00 AM        The Beatles   \n\n                                         Album  \\\n0                                     25 Miles   \n1                               Change of Pace   \n2                             Have a Nice Life   \n3             Great Vengeance and Furious Fire   \n4                            Perfect Confusion   \n...                                        ...   \n50013  Past Masters (Vols. 1 & 2 / Remastered)   \n50014          A Hard Day's Night (Remastered)   \n50015                       Help! (Remastered)   \n50016                  Abbey Road (Remastered)   \n50017        Magical Mystery Tour (Remastered)   \n\n                                 Song_x        TimeStamp_UTC  index_y  \\\n0                     Twenty Five Miles  2021-05-25 23:18:00     9761   \n1                          Devil's Eyes  2021-05-25 23:15:00      206   \n2                       Pussy and Pizza  2021-05-25 23:12:00     6404   \n3                     Our Special Place  2021-05-25 22:46:00     6205   \n4                Make Peace and be Free  2021-05-25 22:39:00     6051   \n...                                 ...                  ...      ...   \n50013  From Me To You - Remastered 2009  2017-01-01 16:04:00     5693   \n50014  And I Love Her - Remastered 2009  2017-01-01 16:01:00      360   \n50015  Ticket To Ride - Remastered 2009  2017-01-01 15:58:00     9715   \n50016   Come Together - Remastered 2009  2017-01-01 15:54:00     7425   \n50017      Penny Lane - Remastered 2009  2017-01-01 15:51:00     4401   \n\n             Performer_y                            Song_y  ... mode  \\\n0            Edwin Starr                 Twenty Five Miles  ...  1.0   \n1             Greyhounds                      Devil's Eyes  ...  0.0   \n2                   Murs                   Pussy and Pizza  ...  1.0   \n3              The Heavy                 Our Special Place  ...  1.0   \n4      Perfect Confusion            Make Peace and be Free  ...  1.0   \n...                  ...                               ...  ...  ...   \n50013        The Beatles  From Me To You - Remastered 2009  ...  1.0   \n50014        The Beatles  And I Love Her - Remastered 2009  ...  0.0   \n50015        The Beatles  Ticket To Ride - Remastered 2009  ...  1.0   \n50016        The Beatles   Come Together - Remastered 2009  ...  0.0   \n50017        The Beatles      Penny Lane - Remastered 2009  ...  1.0   \n\n      speechiness acousticness  instrumentalness  liveness  valence    tempo  \\\n0          0.0607       0.0595          0.000015    0.2240    0.964  124.567   \n1          0.0456       0.3540          0.000414    0.0974    0.858  113.236   \n2          0.0659       0.0708          0.000004    0.0780    0.381   93.991   \n3          0.0386       0.2720          0.003610    0.0991    0.939  193.996   \n4          0.0315       0.0138          0.000017    0.0649    0.431   78.037   \n...           ...          ...               ...       ...      ...      ...   \n50013      0.0309       0.6130          0.000000    0.2690    0.966  136.125   \n50014      0.0337       0.6400          0.000000    0.0681    0.636  113.312   \n50015      0.0678       0.0457          0.000000    0.2330    0.749  123.419   \n50016      0.0393       0.0302          0.248000    0.0926    0.187  165.007   \n50017      0.0316       0.2120          0.026000    0.1360    0.490  113.038   \n\n       time_signature       session_3_hour  session_id  \n0                 4.0  2021-05-25 21:00:00        4332  \n1                 4.0  2021-05-25 21:00:00        4332  \n2                 4.0  2021-05-25 21:00:00        4332  \n3                 4.0  2021-05-25 21:00:00        4332  \n4                 4.0  2021-05-25 21:00:00        4332  \n...               ...                  ...         ...  \n50013             4.0  2017-01-01 15:00:00           0  \n50014             4.0  2017-01-01 15:00:00           0  \n50015             4.0  2017-01-01 15:00:00           0  \n50016             4.0  2017-01-01 15:00:00           0  \n50017             4.0  2017-01-01 15:00:00           0  \n\n[50018 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_x</th>\n      <th>SongID</th>\n      <th>TimeStamp_Central</th>\n      <th>Performer_x</th>\n      <th>Album</th>\n      <th>Song_x</th>\n      <th>TimeStamp_UTC</th>\n      <th>index_y</th>\n      <th>Performer_y</th>\n      <th>Song_y</th>\n      <th>...</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>time_signature</th>\n      <th>session_3_hour</th>\n      <th>session_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Twenty Five MilesEdwin Starr</td>\n      <td>5/25/2021 5:18:00 PM</td>\n      <td>Edwin Starr</td>\n      <td>25 Miles</td>\n      <td>Twenty Five Miles</td>\n      <td>2021-05-25 23:18:00</td>\n      <td>9761</td>\n      <td>Edwin Starr</td>\n      <td>Twenty Five Miles</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0607</td>\n      <td>0.0595</td>\n      <td>0.000015</td>\n      <td>0.2240</td>\n      <td>0.964</td>\n      <td>124.567</td>\n      <td>4.0</td>\n      <td>2021-05-25 21:00:00</td>\n      <td>4332</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Devil's EyesGreyhounds</td>\n      <td>5/25/2021 5:15:00 PM</td>\n      <td>Greyhounds</td>\n      <td>Change of Pace</td>\n      <td>Devil's Eyes</td>\n      <td>2021-05-25 23:15:00</td>\n      <td>206</td>\n      <td>Greyhounds</td>\n      <td>Devil's Eyes</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0456</td>\n      <td>0.3540</td>\n      <td>0.000414</td>\n      <td>0.0974</td>\n      <td>0.858</td>\n      <td>113.236</td>\n      <td>4.0</td>\n      <td>2021-05-25 21:00:00</td>\n      <td>4332</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Pussy and PizzaMurs</td>\n      <td>5/25/2021 5:12:00 PM</td>\n      <td>Murs</td>\n      <td>Have a Nice Life</td>\n      <td>Pussy and Pizza</td>\n      <td>2021-05-25 23:12:00</td>\n      <td>6404</td>\n      <td>Murs</td>\n      <td>Pussy and Pizza</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0659</td>\n      <td>0.0708</td>\n      <td>0.000004</td>\n      <td>0.0780</td>\n      <td>0.381</td>\n      <td>93.991</td>\n      <td>4.0</td>\n      <td>2021-05-25 21:00:00</td>\n      <td>4332</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>Our Special PlaceThe Heavy</td>\n      <td>5/25/2021 4:46:00 PM</td>\n      <td>The Heavy</td>\n      <td>Great Vengeance and Furious Fire</td>\n      <td>Our Special Place</td>\n      <td>2021-05-25 22:46:00</td>\n      <td>6205</td>\n      <td>The Heavy</td>\n      <td>Our Special Place</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0386</td>\n      <td>0.2720</td>\n      <td>0.003610</td>\n      <td>0.0991</td>\n      <td>0.939</td>\n      <td>193.996</td>\n      <td>4.0</td>\n      <td>2021-05-25 21:00:00</td>\n      <td>4332</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>Make Peace and be FreePerfect Confusion</td>\n      <td>5/25/2021 4:39:00 PM</td>\n      <td>Perfect Confusion</td>\n      <td>Perfect Confusion</td>\n      <td>Make Peace and be Free</td>\n      <td>2021-05-25 22:39:00</td>\n      <td>6051</td>\n      <td>Perfect Confusion</td>\n      <td>Make Peace and be Free</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0315</td>\n      <td>0.0138</td>\n      <td>0.000017</td>\n      <td>0.0649</td>\n      <td>0.431</td>\n      <td>78.037</td>\n      <td>4.0</td>\n      <td>2021-05-25 21:00:00</td>\n      <td>4332</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>50013</th>\n      <td>62902</td>\n      <td>From Me To You - Remastered 2009The Beatles</td>\n      <td>1/1/2017 10:04:00 AM</td>\n      <td>The Beatles</td>\n      <td>Past Masters (Vols. 1 &amp; 2 / Remastered)</td>\n      <td>From Me To You - Remastered 2009</td>\n      <td>2017-01-01 16:04:00</td>\n      <td>5693</td>\n      <td>The Beatles</td>\n      <td>From Me To You - Remastered 2009</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0309</td>\n      <td>0.6130</td>\n      <td>0.000000</td>\n      <td>0.2690</td>\n      <td>0.966</td>\n      <td>136.125</td>\n      <td>4.0</td>\n      <td>2017-01-01 15:00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>50014</th>\n      <td>62903</td>\n      <td>And I Love Her - Remastered 2009The Beatles</td>\n      <td>1/1/2017 10:01:00 AM</td>\n      <td>The Beatles</td>\n      <td>A Hard Day's Night (Remastered)</td>\n      <td>And I Love Her - Remastered 2009</td>\n      <td>2017-01-01 16:01:00</td>\n      <td>360</td>\n      <td>The Beatles</td>\n      <td>And I Love Her - Remastered 2009</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0337</td>\n      <td>0.6400</td>\n      <td>0.000000</td>\n      <td>0.0681</td>\n      <td>0.636</td>\n      <td>113.312</td>\n      <td>4.0</td>\n      <td>2017-01-01 15:00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>50015</th>\n      <td>62904</td>\n      <td>Ticket To Ride - Remastered 2009The Beatles</td>\n      <td>1/1/2017 9:58:00 AM</td>\n      <td>The Beatles</td>\n      <td>Help! (Remastered)</td>\n      <td>Ticket To Ride - Remastered 2009</td>\n      <td>2017-01-01 15:58:00</td>\n      <td>9715</td>\n      <td>The Beatles</td>\n      <td>Ticket To Ride - Remastered 2009</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0678</td>\n      <td>0.0457</td>\n      <td>0.000000</td>\n      <td>0.2330</td>\n      <td>0.749</td>\n      <td>123.419</td>\n      <td>4.0</td>\n      <td>2017-01-01 15:00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>50016</th>\n      <td>62905</td>\n      <td>Come Together - Remastered 2009The Beatles</td>\n      <td>1/1/2017 9:54:00 AM</td>\n      <td>The Beatles</td>\n      <td>Abbey Road (Remastered)</td>\n      <td>Come Together - Remastered 2009</td>\n      <td>2017-01-01 15:54:00</td>\n      <td>7425</td>\n      <td>The Beatles</td>\n      <td>Come Together - Remastered 2009</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0393</td>\n      <td>0.0302</td>\n      <td>0.248000</td>\n      <td>0.0926</td>\n      <td>0.187</td>\n      <td>165.007</td>\n      <td>4.0</td>\n      <td>2017-01-01 15:00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>50017</th>\n      <td>62906</td>\n      <td>Penny Lane - Remastered 2009The Beatles</td>\n      <td>1/1/2017 9:51:00 AM</td>\n      <td>The Beatles</td>\n      <td>Magical Mystery Tour (Remastered)</td>\n      <td>Penny Lane - Remastered 2009</td>\n      <td>2017-01-01 15:51:00</td>\n      <td>4401</td>\n      <td>The Beatles</td>\n      <td>Penny Lane - Remastered 2009</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0316</td>\n      <td>0.2120</td>\n      <td>0.026000</td>\n      <td>0.1360</td>\n      <td>0.490</td>\n      <td>113.038</td>\n      <td>4.0</td>\n      <td>2017-01-01 15:00:00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>50018 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.info()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkOi7VaWau9m","outputId":"885e777a-404e-4294-de1f-08d2497b2b8d","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:07:57.592929Z","iopub.execute_input":"2024-12-12T12:07:57.593644Z","iopub.status.idle":"2024-12-12T12:07:57.640324Z","shell.execute_reply.started":"2024-12-12T12:07:57.593610Z","shell.execute_reply":"2024-12-12T12:07:57.639457Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50018 entries, 0 to 50017\nData columns (total 30 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   index_x                    50018 non-null  int64  \n 1   SongID                     50018 non-null  object \n 2   TimeStamp_Central          50018 non-null  object \n 3   Performer_x                50018 non-null  object \n 4   Album                      47890 non-null  object \n 5   Song_x                     50018 non-null  object \n 6   TimeStamp_UTC              50018 non-null  object \n 7   index_y                    50018 non-null  int64  \n 8   Performer_y                50018 non-null  object \n 9   Song_y                     50018 non-null  object \n 10  spotify_genre              50018 non-null  object \n 11  spotify_track_id           50018 non-null  object \n 12  spotify_track_preview_url  36001 non-null  object \n 13  spotify_track_duration_ms  50018 non-null  float64\n 14  spotify_track_popularity   50018 non-null  float64\n 15  spotify_track_explicit     50018 non-null  bool   \n 16  danceability               50018 non-null  float64\n 17  energy                     50018 non-null  float64\n 18  key                        50018 non-null  float64\n 19  loudness                   50018 non-null  float64\n 20  mode                       50018 non-null  float64\n 21  speechiness                50018 non-null  float64\n 22  acousticness               50018 non-null  float64\n 23  instrumentalness           50018 non-null  float64\n 24  liveness                   50018 non-null  float64\n 25  valence                    50018 non-null  float64\n 26  tempo                      50018 non-null  float64\n 27  time_signature             50018 non-null  float64\n 28  session_3_hour             50018 non-null  object \n 29  session_id                 50018 non-null  int64  \ndtypes: bool(1), float64(14), int64(3), object(12)\nmemory usage: 11.1+ MB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"test_col_name = 'mode'\n\ndf.loc[:, test_col_name]","metadata":{"id":"AUBP-uxUau9m","outputId":"7b495386-3f88-4c64-cca4-2d26ecae8f88"},"outputs":[{"data":{"text/plain":["0        1.0\n","1        0.0\n","2        1.0\n","3        1.0\n","4        1.0\n","        ... \n","50013    1.0\n","50014    0.0\n","50015    1.0\n","50016    0.0\n","50017    1.0\n","Name: mode, Length: 50018, dtype: float64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"markdown","source":"## Remove N.A.N data","metadata":{"id":"ENdbzTAEau9n"}},{"cell_type":"code","source":"# df_filtered = df[~df['danceability'].isna()]\n\n# df_filtered.info()","metadata":{"id":"YxvNRVFeau9n"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare Tensorflow Datasets","metadata":{"id":"LQ3vqtcvau9o"}},{"cell_type":"code","source":"import time\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom keras.api.preprocessing.sequence import pad_sequences\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Feature columns (as provided)\n\nfeature_columns = [\n\n    'spotify_genre',\n\n]\n\n\n\n# Define the DataPreprocessor class\n\nclass DataPreprocessor:\n\n    def __init__(self, df, feature_columns, batch_size=16, fixed_genre_size=15, train_size=0.8):\n\n        \"\"\"\n\n        Initializes the data preprocessor with necessary parameters and preprocessing layers.\n\n\n\n        Args:\n\n            df (DataFrame): The input DataFrame containing session data.\n\n            feature_columns (list): List of feature column names.\n\n            batch_size (int): The batch size for dataset creation.\n\n            fixed_genre_size (int): The fixed size for genre vectorization.\n\n            train_size (float): Proportion of the data to use for training (between 0 and 1).\n\n        \"\"\"\n\n        self.df = df\n\n        self.feature_columns = feature_columns\n\n        self.batch_size = batch_size\n\n        self.fixed_genre_size = fixed_genre_size\n\n        self.train_size = train_size\n\n\n\n        # Split the dataset into training and testing datasets\n\n        self.train_df, self.test_df = train_test_split(self.df, train_size=self.train_size, random_state=42)\n\n\n\n        # Numeric feature preprocessing\n\n        self.numeric_data = self.df[feature_columns[1:]].apply(pd.to_numeric, errors='coerce')\n\n        self.mean_values = self.numeric_data.mean()\n\n        self.std_values = self.numeric_data.std()\n\n\n\n        # Initialize LabelEncoder for SongID and spotify_genre\n\n        self.song_id_encoder = LabelEncoder()\n\n        self.genre_encoder = LabelEncoder()\n\n\n\n        # Extract unique SongIDs and genres\n\n        unique_song_ids = self.df['SongID'].unique()\n\n        all_genres = []\n\n        for genre_str in self.df['spotify_genre']:\n\n            try:\n\n                genre_list = ast.literal_eval(genre_str)  # Safely parse the string into a list\n\n                if isinstance(genre_list, list):\n\n                    all_genres.extend(genre_list)\n\n            except Exception as e:\n\n                print(f\"Error parsing genre: {e}\")\n\n\n\n        unique_genres = list(set(all_genres))\n\n\n\n        # Fit the LabelEncoders on the data\n\n        self.song_id_encoder.fit(unique_song_ids)\n\n        self.genre_encoder.fit(unique_genres)\n\n\n\n        self.items_size = len(self.song_id_encoder.classes_)  # Number of unique SongIDs\n\n        self.genres_size = len(self.genre_encoder.classes_)\n\n\n\n        self.dataset = None\n\n\n\n    def preprocess_song_id(self, song_id):\n\n        \"\"\"\n\n        Encode the SongID using LabelEncoder.\n\n        \"\"\"\n\n        return self.song_id_encoder.transform([song_id])[0]\n\n\n\n    def clean_genre(self, value, default_value=0, dtype=tf.int32):\n\n        \"\"\"\n\n        Clean and process the 'spotify_genre' feature.\n\n        \"\"\"\n\n        if value is None or (isinstance(value, str) and not value.strip()):\n\n            return np.full((self.fixed_genre_size,), default_value, dtype=dtype.as_numpy_dtype)\n\n\n\n        try:\n\n            genre_list = eval(value) if isinstance(value, str) else value\n\n            if isinstance(genre_list, list):\n\n                genre_encoded = self.genre_encoder.transform(genre_list)\n\n            else:\n\n                genre_encoded = self.genre_encoder.transform([value])\n\n        except Exception:\n\n            genre_encoded = self.genre_encoder.transform([value])\n\n\n\n        # Pad or truncate to fixed size\n\n        return np.pad(genre_encoded, (0, max(0, self.fixed_genre_size - len(genre_encoded))),\n\n                      mode='constant')[:self.fixed_genre_size].astype(dtype.as_numpy_dtype)\n\n\n\n    def clean_numeric_feature(self, value, default_value=0.0, feature_name=\"feature\", mean=None, std=None):\n\n        \"\"\"\n\n        Clean, process, and normalize numerical features using Z-score normalization.\n\n        \"\"\"\n\n        if value is None or (isinstance(value, float) and np.isnan(value)):\n\n            return default_value\n\n\n\n        try:\n\n            value = float(value)\n\n            # Apply Z-score normalization if mean and std are provided\n\n            if mean is not None and std is not None and std != 0:\n\n                z_score_value = (value - mean) / std\n\n                return z_score_value\n\n            return value  # Return raw value if no normalization\n\n        except ValueError:\n\n            return default_value\n\n\n\n    def create_session_dataset(self, session_df):\n\n        \"\"\"\n\n        Create session dataset as a list of dictionaries for each session.\n\n        \"\"\"\n\n        session_df = session_df.sort_values(by=['session_id', 'TimeStamp_UTC'])\n\n        grouped = session_df.groupby('session_id')\n\n        sessions_data = []\n\n        for session_id, group in grouped:\n\n            session_data = group.to_dict(orient='records')\n\n            sessions_data.append(session_data)\n\n        return sessions_data\n\n\n\n    def preprocess_data(self, sessions, k=1):\n\n        \"\"\"\n\n        Preprocess session data into TensorFlow dataset with split genre and features,\n\n        filtering out sequences where the next item sequence length is not greater than 10.\n\n        \"\"\"\n\n        item_sequences = []\n\n        next_item_sequences = []\n\n        genre_sequences = []\n\n        next_genre_sequences = []\n\n        feature_sequences = []\n\n        processed_item_count = 0\n\n\n\n        for idx, session in enumerate(sessions):\n\n            # Filter the session that has length less than k\n\n            if len(session) < k:\n\n                continue\n\n            session_item_sequences = []\n\n            session_next_item_sequences = []\n\n            session_genre_sequences = []\n\n            session_next_genre_sequences= []\n\n            session_feature_sequences = []\n\n            session_id = session[0]['session_id']\n\n\n\n            for i in range(len(session) - 1):\n\n                # Process items\n\n                session_item_encoded = self.preprocess_song_id(session[i]['SongID'])\n\n                next_session_item_encoded = self.preprocess_song_id(session[i + 1]['SongID'])\n\n                session_item_sequences.append(session_item_encoded)\n\n                session_next_item_sequences.append(next_session_item_encoded)\n\n\n\n                # Process genre\n\n                genre_cleaned = self.clean_genre(session[i].get('spotify_genre', None))\n\n                next_genre_cleaned = self.clean_genre(session[i+1].get('spotify_genre', None))\n\n                session_genre_sequences.append(genre_cleaned)\n\n                session_next_genre_sequences.append(next_genre_cleaned)\n\n\n\n                # Process numerical features\n\n                numeric_features = []\n\n                for col in self.feature_columns:\n\n                    if col != 'spotify_genre':\n\n                        mean = self.mean_values.get(col, None)\n\n                        std = self.std_values.get(col, None)\n\n                        cleaned_feature = self.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n\n                        numeric_features.append(cleaned_feature)\n\n\n\n                session_feature_sequences.append(numeric_features)\n\n\n\n            # Filter out sessions where the next item sequence length is not greater than 10\n\n            # Extend sequences only if the next item sequence length is greater than 10\n\n            print(\"session item sequences:\", session_item_sequences)\n\n            print(\"session next item sequences:\", session_next_item_sequences)\n\n\n\n            # Filter the item that have session length less than k\n\n            item_sequences.append(session_item_sequences)\n\n            next_item_sequences.append(session_next_item_sequences)\n\n            genre_sequences.append(session_genre_sequences)\n\n            next_genre_sequences.append(session_next_genre_sequences)\n\n            feature_sequences.append(session_feature_sequences)\n\n            processed_item_count += len(session_item_sequences)\n\n\n\n            #     # print(f\"Session {idx + 1} processed with {len(session_item_sequences)} items.\")\n\n            # else:\n\n            #     print(f\"Session {idx + 1} skipped because next item sequence length is {len(session_next_item_sequences)}.\")\n\n\n\n        print(f\"Total processed items: {processed_item_count}\")\n\n\n\n        # Pad sequences\n\n        item_sequences = pad_sequences(item_sequences, padding='pre', value=0)\n\n        next_item_sequences = pad_sequences(next_item_sequences, padding='pre', value=0)\n\n        genre_sequences = pad_sequences(genre_sequences, padding='pre', value=0)\n\n        next_genre_sequences = pad_sequences(next_genre_sequences, padding='pre', value=0)\n\n        feature_sequences = pad_sequences(feature_sequences, padding='pre', dtype='float32', value=0.0)\n\n        # print(f\"item_sequences shape: {item_sequences.shape}\")\n\n        # print(f\"next_item_sequences shape: {next_item_sequences.shape}\")\n\n        # print(f\"genre_sequences shape: {genre_sequences.shape}\")\n\n        # print(f\"next_genre_sequences shape: {next_genre_sequences.shape}\")\n\n        # print(f\"feature_sequences shape: {feature_sequences.shape}\")\n\n        # print(\"item sequence padded:\", item_sequences)\n\n        # print(\"next item sequence padded:\", next_item_sequences)\n\n        # print(\"genre sequence padded:\", genre_sequences)\n\n        # print(\"next genre sequence padded:\", next_genre_sequences)\n\n        # print(\"feature sequence padded:\", feature_sequences)\n\n\n\n        # Create TensorFlow dataset\n\n        sequence_length = item_sequences.shape[1]  # Assuming all sequences have the same length after padding\n\n        print(f\"Sequence length after padding: {sequence_length}\")\n    \n        # Create TensorFlow dataset\n        dataset = tf.data.Dataset.from_tensor_slices({\n            'item': item_sequences,\n            'genre': genre_sequences,\n            'features': feature_sequences,\n            'next_item': next_item_sequences,\n            'next_genre': next_genre_sequences\n        })\n    \n        return dataset, sequence_length\n\n\n\n    def create_session_dataset_tensor(self, k=1):\n\n        \"\"\"\n\n        Main function to create session dataset as tensors and return the dataset.\n\n        \"\"\"\n\n        if self.dataset is not None:\n\n            print(\"Dataset already created\")\n\n            return\n\n\n\n        print(\"Creating session dataset\")\n\n        sessions_data = self.create_session_dataset(self.df)\n\n        dataset, sequence_length = self.preprocess_data(sessions_data, k=k)\n\n\n\n        # Shuffle and batch the training data\n\n        dataset = (\n\n            dataset.batch(self.batch_size, drop_remainder=True)\n\n                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n\n        )\n\n\n\n        self.dataset = dataset\n\n        return dataset, sequence_length\n\n\n\n    def create_train_dataset(self, k=1):\n\n        \"\"\"\n\n        Main function to create session dataset as tensors and return the dataset.\n\n        \"\"\"\n\n        if self.dataset is not None:\n\n            print(\"Dataset already created\")\n\n            return\n\n\n\n        print(\"Creating session dataset\")\n\n        sessions_data = self.create_session_dataset(self.train_df)  # Use train data for training\n\n        print(\"Creating tensor dataset\")\n\n        dataset = self.preprocess_data(sessions_data, k=k)\n\n\n\n        # Shuffle and batch the training data\n\n        dataset = (\n\n            dataset.shuffle(buffer_size=1024)\n\n                   .batch(self.batch_size, drop_remainder=True)\n\n                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n\n        )\n\n        return dataset\n\n\n\n    def get_test_data(self, k):\n\n        \"\"\"\n\n        Return preprocessed test dataset without shuffling.\n\n        \"\"\"\n\n        sessions_data = self.create_session_dataset(self.test_df)\n\n        dataset = self.preprocess_data(sessions_data, k)\n\n\n\n        # Batch the test data without shuffling\n\n        dataset = (\n\n            dataset.batch(self.batch_size, drop_remainder=True)\n\n                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n\n        )\n\n\n\n        return dataset\n\n\n\n    def batch_timer(self, dataset):\n\n        \"\"\"\n\n        Timer function to track the time taken for batch processing.\n\n        \"\"\"\n\n        for batch in dataset:\n\n            start_time = time.time()\n\n\n\n            # Simulate processing (e.g., model training or data transformation)\n\n            end_time = time.time()\n\n            batch_time = end_time - start_time\n\n            print(f\"Batch processing time: {batch_time:.4f} seconds\")\n\n\n","metadata":{"id":"RP5P9y7yau9o","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:09:24.136760Z","iopub.execute_input":"2024-12-12T12:09:24.137142Z","iopub.status.idle":"2024-12-12T12:09:24.385260Z","shell.execute_reply.started":"2024-12-12T12:09:24.137107Z","shell.execute_reply":"2024-12-12T12:09:24.384358Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"<p style=\"font-size:36px;\">PENTING: Baca Notes Comment di setiap Sel!!!<p>","metadata":{"id":"5Ug642Mp4nKt"}},{"cell_type":"code","source":"# preprocessor = DataPreprocessor(df, feature_columns)  # <---- pakai ini untuk bener-bener ngetrain\n\npreprocessor = DataPreprocessor(df[:300], feature_columns, batch_size=8) # <------ pakai ini buat ngetes apakah bisa ditrain, bisa ekspor model, bisa import modelnya dengan lancar\n\n\n\n# Create the session dataset tensor\n\ndataset, sequence_length = preprocessor.create_session_dataset_tensor(k=10)  # k = 4 artinya mengambil sesi dengan panjang sesi item selanjutnya lebih dari 4\n\nprint(f\"Sequence Length: {sequence_length}\")\n# for batch in dataset.take(1):\n\n#     print(\"Items (SongID):\", batch['item'].numpy())\n\n#     print(\"Genre:\", batch['genre'].numpy())\n\n#     print(\"Features:\", batch['features'].numpy())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"vqP4prfnau9q","outputId":"db4e8a66-b078-460a-ff06-050e67d7cf91","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:11:40.666067Z","iopub.execute_input":"2024-12-12T12:11:40.666808Z","iopub.status.idle":"2024-12-12T12:11:40.911984Z","shell.execute_reply.started":"2024-12-12T12:11:40.666775Z","shell.execute_reply":"2024-12-12T12:11:40.911120Z"}},"outputs":[{"name":"stdout","text":"Creating session dataset\nsession item sequences: [159, 203, 130, 30, 231, 83, 228, 218, 129, 20, 88, 131, 9, 186]\nsession next item sequences: [203, 130, 30, 231, 83, 228, 218, 129, 20, 88, 131, 9, 186, 201]\nsession item sequences: [126, 37, 15, 196, 54, 176, 143, 78, 10, 206, 60, 128, 154, 94, 165, 4]\nsession next item sequences: [37, 15, 196, 54, 176, 143, 78, 10, 206, 60, 128, 154, 94, 165, 4, 194]\nsession item sequences: [194, 208, 94, 23, 4, 217, 29, 98, 3, 223, 208, 29, 194]\nsession next item sequences: [208, 94, 23, 4, 217, 29, 98, 3, 223, 208, 29, 194, 4]\nsession item sequences: [51, 155, 152, 209, 105, 43, 36, 181, 14, 12, 63, 203, 106, 193, 41, 192]\nsession next item sequences: [155, 152, 209, 105, 43, 36, 181, 14, 12, 63, 203, 106, 193, 41, 192, 185]\nsession item sequences: [192, 168, 109, 185, 145, 183, 204, 52, 123, 234, 38, 1]\nsession next item sequences: [168, 109, 185, 145, 183, 204, 52, 123, 234, 38, 1, 172]\nsession item sequences: [171, 180, 170, 182, 65, 221, 149, 33, 210, 99, 81, 16, 229, 153, 226]\nsession next item sequences: [180, 170, 182, 65, 221, 149, 33, 210, 99, 81, 16, 229, 153, 226, 226]\nsession item sequences: [122, 122, 200, 107, 127, 225, 179, 77, 30, 119, 85, 91, 86, 25, 113, 146, 72, 136, 231, 8, 163, 7, 6, 227, 61]\nsession next item sequences: [122, 200, 107, 127, 225, 179, 77, 30, 119, 85, 91, 86, 25, 113, 146, 72, 136, 231, 8, 163, 7, 6, 227, 61, 184]\nsession item sequences: [220, 68, 82, 227, 217, 80, 80, 4, 23]\nsession next item sequences: [68, 82, 227, 217, 80, 80, 4, 23, 194]\nsession item sequences: [100, 100, 98, 112, 112, 2, 188, 233, 162, 92, 53, 46, 69, 95, 95, 222]\nsession next item sequences: [100, 98, 112, 112, 2, 188, 233, 162, 92, 53, 46, 69, 95, 95, 222, 74]\nsession item sequences: [167, 18, 24, 170, 170, 58, 5, 75, 40, 164, 11, 47, 39, 144, 103, 195, 141]\nsession next item sequences: [18, 24, 170, 170, 58, 5, 75, 40, 164, 11, 47, 39, 144, 103, 195, 141, 57]\nsession item sequences: [115, 101, 157, 124, 207, 224, 117, 135, 151, 175, 26, 232]\nsession next item sequences: [101, 157, 124, 207, 224, 117, 135, 151, 175, 26, 232, 198]\nsession item sequences: [133, 73, 191, 110, 53, 106, 166, 70, 199, 160, 104, 176, 139, 142, 158, 56]\nsession next item sequences: [73, 191, 110, 53, 106, 166, 70, 199, 160, 104, 176, 139, 142, 158, 56, 53]\nTotal processed items: 181\nSequence length after padding: 25\nSequence Length: 25\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"test_dataset = preprocessor.get_test_data(k=1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5nPLPszcfaHv","outputId":"d0688b74-7cfa-461b-f2b0-62253af0b9c6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(dataset))\n\nfor batch in dataset.take(1):\n\n    print(\"Items (SongID):\", batch['item'].numpy())\n\n    print(\"Genre:\", batch['genre'].numpy())\n\n    print(\"Features:\", batch['features'].numpy())\n\n    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())\n\n    print(\"Next Genre:\", batch['next_genre'].numpy())\n\n    # print(batch.type())\n\n\n\nprint(dataset)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaFqUvobau9r","outputId":"737d98d4-0530-411a-c356-35e294c71045","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:11:45.448884Z","iopub.execute_input":"2024-12-12T12:11:45.449687Z","iopub.status.idle":"2024-12-12T12:11:45.467784Z","shell.execute_reply.started":"2024-12-12T12:11:45.449651Z","shell.execute_reply":"2024-12-12T12:11:45.466910Z"}},"outputs":[{"name":"stdout","text":"1\nItems (SongID): [[  0   0   0   0   0   0   0   0   0   0   0 159 203 130  30 231  83 228\n  218 129  20  88 131   9 186]\n [  0   0   0   0   0   0   0   0   0 126  37  15 196  54 176 143  78  10\n  206  60 128 154  94 165   4]\n [  0   0   0   0   0   0   0   0   0   0   0   0 194 208  94  23   4 217\n   29  98   3 223 208  29 194]\n [  0   0   0   0   0   0   0   0   0  51 155 152 209 105  43  36 181  14\n   12  63 203 106 193  41 192]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 192 168 109 185 145\n  183 204  52 123 234  38   1]\n [  0   0   0   0   0   0   0   0   0   0 171 180 170 182  65 221 149  33\n  210  99  81  16 229 153 226]\n [122 122 200 107 127 225 179  77  30 119  85  91  86  25 113 146  72 136\n  231   8 163   7   6 227  61]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 220  68\n   82 227 217  80  80   4  23]]\nGenre: [[[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [  7  19  62 ...   0   0   0]\n  [143   0   0 ...   0   0   0]\n  [  7  48 131 ...   0   0   0]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 49  50  52 ...   0   0   0]\n  [ 96  97 137 ...   0   0   0]\n  [ 42  50  51 ...   0   0   0]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 54 162 167 ...   0   0   0]\n  [ 34  54  70 ...   0   0   0]\n  [ 54 162   0 ...   0   0   0]]\n\n ...\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]]\n\n [[ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]\n  ...\n  [182   0   0 ...   0   0   0]\n  [182   0   0 ...   0   0   0]\n  [182   0   0 ...   0   0   0]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 96  97 113 ...   0   0   0]\n  [ 42  50  51 ...   0   0   0]\n  [ 54 162   0 ...   0   0   0]]]\nFeatures: []\nNext Items (Next SongID): [[  0   0   0   0   0   0   0   0   0   0   0 203 130  30 231  83 228 218\n  129  20  88 131   9 186 201]\n [  0   0   0   0   0   0   0   0   0  37  15 196  54 176 143  78  10 206\n   60 128 154  94 165   4 194]\n [  0   0   0   0   0   0   0   0   0   0   0   0 208  94  23   4 217  29\n   98   3 223 208  29 194   4]\n [  0   0   0   0   0   0   0   0   0 155 152 209 105  43  36 181  14  12\n   63 203 106 193  41 192 185]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 168 109 185 145 183\n  204  52 123 234  38   1 172]\n [  0   0   0   0   0   0   0   0   0   0 180 170 182  65 221 149  33 210\n   99  81  16 229 153 226 226]\n [122 200 107 127 225 179  77  30 119  85  91  86  25 113 146  72 136 231\n    8 163   7   6 227  61 184]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  68  82\n  227 217  80  80   4  23 194]]\nNext Genre: [[[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [143   0   0 ...   0   0   0]\n  [  7  48 131 ...   0   0   0]\n  [ 54  93  94 ...   0   0   0]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 96  97 137 ...   0   0   0]\n  [ 42  50  51 ...   0   0   0]\n  [ 54 162   0 ...   0   0   0]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 34  54  70 ...   0   0   0]\n  [ 54 162   0 ...   0   0   0]\n  [ 42  50  51 ...   0   0   0]]\n\n ...\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]]\n\n [[ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]\n  [ 78  84  95 ...   0   0   0]\n  ...\n  [182   0   0 ...   0   0   0]\n  [182   0   0 ...   0   0   0]\n  [182   0   0 ...   0   0   0]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [ 42  50  51 ...   0   0   0]\n  [ 54 162   0 ...   0   0   0]\n  [ 54 162   0 ...   0   0   0]]]\n<_PrefetchDataset element_spec={'item': TensorSpec(shape=(8, 25), dtype=tf.int32, name=None), 'genre': TensorSpec(shape=(8, 25, 15), dtype=tf.int32, name=None), 'features': TensorSpec(shape=(8, 25, 0), dtype=tf.float32, name=None), 'next_item': TensorSpec(shape=(8, 25), dtype=tf.int32, name=None), 'next_genre': TensorSpec(shape=(8, 25, 15), dtype=tf.int32, name=None)}>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"DATASET_SIZE = 37728\ntrain_size = int(0.8*DATASET_SIZE)\ntrain_dataset = dataset.take(train_size)\nval_dataset = dataset.take(train_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:59:03.983080Z","iopub.execute_input":"2024-12-11T15:59:03.983415Z","iopub.status.idle":"2024-12-11T15:59:03.991750Z","shell.execute_reply.started":"2024-12-11T15:59:03.983386Z","shell.execute_reply":"2024-12-11T15:59:03.991106Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"for batch in train_dataset.take(1):\n    print(\"Items (SongID):\", batch['item'].numpy())\n\n    print(\"Genre:\", batch['genre'].numpy())\n\n    print(\"Features:\", batch['features'].numpy())\n\n    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())\n\nfor batch in val_dataset.take(1):\n    print(\"Items (SongID):\", batch['item'].numpy())\n\n    print(\"Genre:\", batch['genre'].numpy())\n\n    print(\"Features:\", batch['features'].numpy())\n\n    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:49:59.974095Z","iopub.execute_input":"2024-12-11T17:49:59.974435Z","iopub.status.idle":"2024-12-11T17:50:00.006028Z","shell.execute_reply.started":"2024-12-11T17:49:59.974410Z","shell.execute_reply":"2024-12-11T17:50:00.005143Z"}},"outputs":[{"name":"stdout","text":"Items (SongID): [[    0     0     0 ...  8655   354  6331]\n [    0     0     0 ...  2245  4268 10534]\n [    0     0     0 ...  3905  3362  8022]\n ...\n [    0     0     0 ...   332  4091  2570]\n [    0     0     0 ...  1872   674   493]\n [    0     0     0 ...  2646  8927  7362]]\nGenre: [[[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 157  711  934 ...    0    0    0]\n  [ 157  711  934 ...    0    0    0]\n  [ 157  711  934 ...    0    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [  15   52  255 ... 1031    0    0]\n  [  15   52  157 ...  931  971  980]\n  [   9   15  255 ... 1195    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 336  560  724 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [ 320  474  783 ...    0    0    0]]\n\n ...\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [  15  157  255 ...    0    0    0]\n  [  15  157  255 ...    0    0    0]\n  [  15  157  255 ...    0    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 769    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 959    0    0 ...    0    0    0]\n  [  65  336  560 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]]]\nFeatures: []\nNext Items (Next SongID): [[    0     0     0 ...   354  6331  1984]\n [    0     0     0 ...  4268 10534  4906]\n [    0     0     0 ...  3362  8022  2635]\n ...\n [    0     0     0 ...  4091  2570  5020]\n [    0     0     0 ...   674   493  1493]\n [    0     0     0 ...  8927  7362  8602]]\nItems (SongID): [[    0     0     0 ...  8655   354  6331]\n [    0     0     0 ...  2245  4268 10534]\n [    0     0     0 ...  3905  3362  8022]\n ...\n [    0     0     0 ...   332  4091  2570]\n [    0     0     0 ...  1872   674   493]\n [    0     0     0 ...  2646  8927  7362]]\nGenre: [[[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 157  711  934 ...    0    0    0]\n  [ 157  711  934 ...    0    0    0]\n  [ 157  711  934 ...    0    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [  15   52  255 ... 1031    0    0]\n  [  15   52  157 ...  931  971  980]\n  [   9   15  255 ... 1195    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 336  560  724 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [ 320  474  783 ...    0    0    0]]\n\n ...\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [  15  157  255 ...    0    0    0]\n  [  15  157  255 ...    0    0    0]\n  [  15  157  255 ...    0    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 769    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]]\n\n [[   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]\n  ...\n  [ 959    0    0 ...    0    0    0]\n  [  65  336  560 ...    0    0    0]\n  [   0    0    0 ...    0    0    0]]]\nFeatures: []\nNext Items (Next SongID): [[    0     0     0 ...   354  6331  1984]\n [    0     0     0 ...  4268 10534  4906]\n [    0     0     0 ...  3362  8022  2635]\n ...\n [    0     0     0 ...  4091  2570  5020]\n [    0     0     0 ...   674   493  1493]\n [    0     0     0 ...  8927  7362  8602]]\n","output_type":"stream"}],"execution_count":183},{"cell_type":"code","source":"for batch in test_dataset.take(1):\n\n    print(\"Items (SongID):\", batch['item'].numpy())\n\n    print(\"Genre:\", batch['genre'].numpy())\n\n    print(\"Features:\", batch['features'].numpy())\n\n    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())","metadata":{"id":"MalfdduKbjlB","colab":{"base_uri":"https://localhost:8080/","height":219},"outputId":"462296f9-acfe-441d-ba46-a4e7f6e13d3a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Model class","metadata":{"id":"Fo1s8zV6au9r"}},{"cell_type":"code","source":"@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\nclass ItemEmbedding(Layer):\n\n    def __init__(self, num_items, item_embed_dim, **kwargs):\n\n        super(ItemEmbedding, self).__init__(**kwargs)\n\n\n\n        self.item_embedding = Embedding(input_dim=num_items, output_dim=item_embed_dim, mask_zero=True)\n\n\n\n    def call(self, items):\n\n        # Embed items\n\n        items_embedded = self.item_embedding(items)\n\n        return items_embedded\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"num_items\": self.num_items,\n            \"item_embed_dim\": self.item_embed_dim\n        })\n        return config\n\n\n@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\nclass GRU4REC(Model):\n\n    def __init__(self, rnn_params, genre_embed_dim, item_embed_dim, ffn1_units, feature_dense_units,  items_size, genres_size, *args, **kwargs):\n\n        super(GRU4REC, self).__init__(*args, **kwargs)\n\n        self.rnn_params = rnn_params\n        self.genre_embed_dim = genre_embed_dim\n        self.item_embed_dim = item_embed_dim\n        self.ffn1_units = ffn1_units\n        self.feature_dense_units = feature_dense_units\n        self.items_size = items_size\n        self.genres_size = genres_size\n        self.input_shape = None\n\n        print(f\"items size: {items_size}\")\n\n        print(f\"genres size: {genres_size}\")\n        self.embedding = Embedding(input_dim=items_size, output_dim=item_embed_dim, mask_zero=True)\n\n        \n\n        # Genre embedding (only for genre, which is categorical and a string)\n\n        self.genre_embedding = Embedding(input_dim=genres_size, output_dim=genre_embed_dim, mask_zero=True, name='genre_embedding')\n\n\n\n        # RNN layers\n\n        self.rnn_layers = []\n\n        self.rnn_layers.append(GRU(**rnn_params[0], return_sequences=True))\n\n        for i in range(1, len(rnn_params) - 1):\n\n            self.rnn_layers.append(GRU(**rnn_params[i], return_sequences=True))\n\n        self.rnn_layers.append(GRU(**rnn_params[-1], return_sequences=True))\n\n\n\n        self.concat = Concatenate(axis=-1, name='concat_1')\n\n        self.batch_norm = BatchNormalization(name='batchnorm')\n\n\n\n        # Dropout layer\n\n        self.dropout = Dropout(0.2, name='dropout')\n\n\n\n        # Feed-forward layers\n\n        self.feature_dense = Dense(feature_dense_units, activation='relu', name='feature_dense')  # Dense layer for features (if required)\n\n        self.ffn1 = Dense(ffn1_units, name='ffn_1')\n\n        self.activation1 = LeakyReLU(negative_slope=0.2, name='freaky_relu')\n\n        self.item_output = Dense(items_size, name='item_output')\n\n        # self.genre_output = Dense(preprocessed_data.genres_size, activation='softmax', name='genre_output')\n\n\n\n        self.attention = Attention(use_scale=False, dropout=0.2, name='attention')\n\n\n\n    def call(self, inputs, training=False):\n\n        \"\"\"\n\n        Forward pass for the GRU4REC model.\n\n        :param inputs: Tuple (item_sequences, item_features, item_genres)\n\n        :param training: Boolean indicating if the model is in training mode\n\n        \"\"\"\n\n        item_sequences, item_features, item_genres = inputs\n\n        # Update input shape dynamically\n        if self.input_shape is None:\n            # Set the input shape based on the first batch of inputs\n            self.input_shape = item_sequences.shape\n\n        encoding_padding_mask = tf.math.logical_not(tf.math.equal(item_sequences, 0))\n\n\n\n        # print(\"Item Sequence Shape:\", item_sequences.shape)\n\n        # print(\"Item Genres Shape:\", item_genres.shape)\n\n        # Embed items\n\n        item_embedded = self.embedding(item_sequences)\n\n        item_embedded = tf.expand_dims(item_embedded, axis=2)\n\n        # Genre embedding\n\n        genre_embedded = self.genre_embedding(item_genres)\n\n\n\n        # print(\"Item Embedded Shape:\", item_embedded.shape)\n\n        # print(\"Genre Embedded Shape:\", genre_embedded.shape)\n\n        genre_embedded = tf.reduce_mean(genre_embedded, axis=2)\n\n        genre_embedded = tf.expand_dims(genre_embedded, axis=2)\n\n\n\n        # Feature transformation (features are passed directly as floats, so no embedding is needed)\n\n        # feature_transformed = self.feature_dense(item_features)\n\n        # feature_transformed = tf.expand_dims(feature_transformed, axis=1)\n\n\n\n        # combined_input = tf.concat([item_embedded, feature_transformed, genre_embedded], axis=-1)\n\n        combined_input = tf.concat([item_embedded, genre_embedded], axis=-1)\n\n        combined_input = self.batch_norm(combined_input)\n\n        # print(\"Combined input shape:\",combined_input.shape)\n\n        # Pass through RNN layers\n        combined_input = tf.reduce_mean(combined_input, axis=-2)\n\n        # print(\"Reduced input shape:\", combined_input.shape)\n\n        x = combined_input\n\n        x = self.rnn_layers[0](x)\n\n        x = self.dropout(x, training=training)\n\n        for i in range(1, len(self.rnn_layers)):\n\n            x = self.concat([combined_input, x])  # Concatenate item embeddings with RNN outputs\n\n            x = self.rnn_layers[i](x)\n\n            x = self.dropout(x, training=training)\n\n\n\n        # x = self.batch_norm(x)\n\n\n\n        # # Give attention\n\n        # encodding_padding_mask = tf.expand_dims(encoding_padding_mask, axis=1)\n\n        # # x = tf.expand_dims(x, axis=1)\n        # print(f\"Shape before attention: {x.shape}\")\n\n        # x = self.attention(inputs=[x,x], mask=[encodding_padding_mask, encodding_padding_mask], use_causal_mask=True)\n\n\n\n        # Feed-forward layers\n\n        # print(f\"Shape before squeeze: {x.shape}\")\n\n        # x = tf.squeeze(x, axis=1)\n\n        # print(f\"Shape before softmax: {x.shape}\")\n\n        x = self.ffn1(x)\n\n        x = self.dropout(x, training=training)\n\n        x = self.activation1(x)\n\n        # print(f\"Shape after activation: {x.shape}\")\n\n        item_logits = self.item_output(x)  # Item prediction\n\n        # print(f\"Output shape: {item_logits.shape}\")\n\n\n\n        return item_logits\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"rnn_params\": self.rnn_params,\n            \"genre_embed_dim\": self.genre_embed_dim,\n            \"item_embed_dim\": self.item_embed_dim,\n            \"ffn1_units\": self.ffn1_units,\n            \"feature_dense_units\": self.feature_dense_units,\n            \"items_size\": self.items_size,\n            \"genres_size\": self.genres_size,\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{"id":"urRmM_jCau9r","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:58:52.734133Z","iopub.execute_input":"2024-12-12T12:58:52.734634Z","iopub.status.idle":"2024-12-12T12:58:52.753627Z","shell.execute_reply.started":"2024-12-12T12:58:52.734603Z","shell.execute_reply":"2024-12-12T12:58:52.752837Z"},"_kg_hide-input":true},"outputs":[],"execution_count":86},{"cell_type":"code","source":"def build_gru4rec_model(rnn_params, genre_embed_dim, item_embed_dim, ffn1_units, feature_dense_units,  items_size, genres_size, seq_length, loss_fn, optimizer):\n    items_sequence = keras.api.Input(shape=[seq_length], name=\"item_sequence\")\n    genres_sequence = keras.api.Input(shape=[seq_length, None], name=\"genre_sequence\")\n    \n    # encoding_padding_mask = tf.math.logical_not(tf.math.equal(items_sequence, 0))\n    \n    item_embedded = Embedding(input_dim=items_size, output_dim=item_embed_dim, mask_zero=True, name=\"item_embedding\")(items_sequence)\n    item_embedded = Lambda(lambda x: tf.expand_dims(x, axis=2), output_shape=(seq_length, 1, item_embed_dim))(item_embedded)\n\n    # Genre embedding\n    genre_embedded = Embedding(input_dim=genres_size, output_dim=genre_embed_dim, mask_zero=True, name='genre_embedding')(genres_sequence)\n\n    # print(\"Item Embedded Shape:\", item_embedded.shape)\n    # print(\"Genre Embedded Shape:\", genre_embedded.shape)\n    genre_embedded = Lambda(lambda x:tf.reduce_mean(x, axis=2),output_shape=(seq_length, genre_embed_dim))(genre_embedded)\n    genre_embedded = Lambda(lambda x:tf.expand_dims(x, axis=2),output_shape=(seq_length, 1, genre_embed_dim))(genre_embedded)\n    \n    combined_input = Concatenate(axis=-1)([item_embedded, genre_embedded])\n\n    combined_input = BatchNormalization(name='batchnorm')(combined_input)\n\n    # print(\"Combined input shape:\",combined_input.shape)\n\n    # Pass through RNN layers\n    combined_input = Lambda(lambda x:tf.reduce_mean(x, axis=-2), output_shape=(seq_length, genre_embed_dim + item_embed_dim))(combined_input)\n\n    x = combined_input\n    x = GRU(**rnn_params[0], return_sequences=True, name='gru_0')(x)\n    x = Dropout(0.2, name='dropout_0')(x)\n    for i in range(1, len(rnn_params)-1):\n        x = Concatenate(axis=-1)([combined_input, x])\n        x = GRU(**rnn_params[i], return_sequences=True, name=f'gru_{i}')(x)\n        x = Dropout(0.2, name=f'dropout_{i}')(x)\n\n    # Final GRU layer without return_sequences\n    x = GRU(**rnn_params[-1], return_sequences=True, name=f'gru_{len(rnn_params) - 1}')(x)\n    x = Dropout(0.2, name='dropout_final')(x)\n\n    # Final Feed-forward layer\n    x = Dense(ffn1_units, activation='relu', name='feature_dense')(x)\n    x = Dropout(0.2, name='feature_dropout')(x)\n\n    # Output layer for item predictions\n    output = Dense(items_size, activation='linear', name='output_layer')(x)\n\n    # Build the model\n    model = keras.Model(inputs=[items_sequence, genres_sequence], outputs=output, name='GRU4Rec_Model')\n\n    model.compile(\n        loss=loss_fn,\n        optimizer = optimizer\n    )\n\n    return model\n\ndef train_model(model, train_dataset,epochs, val_dataset=None):\n    # train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    def preprocess(data):\n        return {\n            \"item_sequence\": data[\"item\"], \n            \"genre_sequence\": data[\"genre\"]\n        }, data[\"next_item\"]  # Assuming \"next_item\" is the target\n\n    train_dataset = train_dataset.map(preprocess)\n    # val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n        tf.distribute.experimental.CollectiveCommunication.AUTO)\n    \n    with mirrored_strategy.scope():\n        mirrored_model = model\n\n    history = mirrored_model.fit(train_dataset,\n                                 steps_per_epoch=5,\n                                 epochs=epochs, verbose=2)\n\n    return mirrored_model, history\n    \n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                                 reduction='none')\n    loss_ = loss_object_(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_mean(loss_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:56:27.036452Z","iopub.execute_input":"2024-12-12T12:56:27.036878Z","iopub.status.idle":"2024-12-12T12:56:27.057554Z","shell.execute_reply.started":"2024-12-12T12:56:27.036839Z","shell.execute_reply":"2024-12-12T12:56:27.056612Z"}},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":"## Training Loop","metadata":{"id":"k0qU_Hpsau9s"}},{"cell_type":"code","source":"import keras.api\n\nfrom keras.api.metrics import Recall\n\n\n\n@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\n\nclass RecallAtK(tf.keras.metrics.Metric):\n\n    def __init__(self, k=10, name=\"recall_at_k\", **kwargs):\n\n        super(RecallAtK, self).__init__(name=name, **kwargs)\n\n        self.k = k\n\n        self.recall_at_k = Recall(top_k=self.k)\n\n\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n\n        \"\"\"\n\n        Update the state of the metric.\n\n        \"\"\"\n\n        # Since y_true is a list of true items and y_pred are the predicted scores,\n\n        # we need to calculate recall for top-k predicted items\n\n        y_true = tf.cast(y_true, tf.int32)\n\n\n\n        # Calculate the top-k predicted items\n\n        top_k_preds = tf.argsort(y_pred, axis=-1, direction='DESCENDING')[:, :self.k]\n\n\n\n        # Calculate recall by comparing true labels with the top-k predictions\n\n        recall = tf.reduce_mean(tf.cast(tf.equal(y_true, top_k_preds), tf.float32), axis=-1)\n\n        return recall\n\n\n\n    def result(self):\n\n        return self.recall_at_k.result()\n\n\n\n    def reset_state(self):\n\n        self.recall_at_k.reset_state()\n\n\n\n@keras.saving.register_keras_serializable(package=\"gru4rec_with_attention\")\n\nclass GRU4RECLoss(tf.keras.losses.Loss):\n\n    def __init__(self, item_loss_weight=1.0, genre_loss_weight=1.0, name=\"gru4rec_loss\"):\n\n        super(GRU4RECLoss, self).__init__(name=name)\n\n        self.item_loss_weight = item_loss_weight\n\n        self.genre_loss_weight = genre_loss_weight\n\n        self.categorical_crossentropy = keras.api.losses.CategoricalCrossentropy()\n\n        self.sparse_categorical_crossentropy = keras.api.losses.SparseCategoricalCrossentropy()\n\n        self.binary_crossentropy = tf.keras.losses.BinaryCrossentropy()\n\n\n\n    def sparse_to_multi_hot(self, true_genres, num_genres):\n\n        \"\"\"\n\n        Converts sparse label encoded genres into multi-hot encoded vectors.\n\n\n\n        :param true_genres: Tensor of shape (batch_size, num_labels) with sparse integer labels\n\n        :param num_genres: Total number of genres (the size of the multi-hot vector)\n\n        :return: Multi-hot encoded tensor of shape (batch_size, num_genres)\n\n        \"\"\"\n\n        ## Create a tensor of zeros with shape (batch_size, num_genres)\n\n        batch_size = tf.shape(true_genres)[0]\n\n        multi_hot = tf.zeros((batch_size, num_genres), dtype=tf.float32)\n\n\n\n        # Flatten the batch for indexing\n\n        indices = tf.reshape(true_genres, [-1])  # Flatten true_genres to a 1D tensor\n\n        updates = tf.ones_like(indices, dtype=tf.float32)  # Create a 1D tensor of ones\n\n        batch_indices = tf.repeat(tf.range(batch_size), tf.shape(true_genres)[1])  # Batch indices for each label\n\n\n\n        # Combine batch and label indices\n\n        scatter_indices = tf.stack([batch_indices, indices], axis=1)\n\n\n\n        # Update the multi-hot tensor\n\n        multi_hot = tf.tensor_scatter_nd_update(multi_hot, scatter_indices, updates)\n\n\n\n        return multi_hot\n\n\n\n    def call(self, y_true, y_pred):\n\n        \"\"\"\n\n        Compute the total loss.\n\n        :param y_true: A tuple (true_items, true_genres)\n\n        :param y_pred: A tuple (predicted_items, predicted_genres)\n\n        \"\"\"\n\n        true_items, true_genres = y_true\n\n        pred_items, pred_genres = y_pred\n\n\n\n        # Calculate item loss\n\n        item_loss = self.sparse_categorical_crossentropy(true_items, pred_items)\n\n\n\n        true_genres = tf.cast(true_genres, tf.int32)\n\n        num_genres = pred_genres.shape[1]\n\n\n\n        true_genres_multi_hot = self.sparse_to_multi_hot(true_genres, num_genres)\n\n\n\n\n\n        # Calculate genre loss\n\n        genre_loss = self.binary_crossentropy(true_genres_multi_hot, pred_genres)\n\n\n\n        total_loss = self.item_loss_weight * item_loss + self.genre_loss_weight * genre_loss\n\n        return total_loss\n\n\n\n# @tf.function\n\ndef train_step(batch, loss_fn, model, optimizer):\n\n    with tf.GradientTape() as tape:\n\n        item_logits = model((batch['item'], batch['features'], batch['genre']), training=True)\n\n        loss = loss_fn(batch['next_item'], item_logits)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    return loss\n\n\n\ndef train_gru4rec(model, train_dataset, optimizer, epochs, k, val_dataset=None, loss_fn = keras.api.losses.SparseCategoricalCrossentropy(from_logits=True)):\n\n    # metric = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n\n\n    loss_history = []\n\n    val_loss_history = []\n\n    val_metric_history = []\n\n    metric_history = []\n\n    for epoch in range(epochs):\n\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n\n        epoch_loss = 0.0\n\n\n\n        for step, batch in enumerate(train_dataset):\n\n            loss = train_step(batch, loss_fn, model, optimizer)\n\n            if loss is None:\n\n              print(f\"Warning: train_step returned None at step {step}\")\n\n              continue\n\n            epoch_loss += loss.numpy()\n\n\n\n            # Update metric\n\n            # metric.update_state(batch['next_item'], logits)\n\n\n\n        print(f\"Training Loss: {epoch_loss / (step + 1):.4f}\")\n\n        loss_history.append(epoch_loss / (step + 1))\n\n        # metric_history.append(metric.result().numpy())\n\n        # metric.reset_state()\n\n\n\n        if val_dataset:\n\n            val_loss = 0.0\n\n            for step, batch in enumerate(val_dataset):\n\n                item_logits, genre_logits = model((batch['item'], batch['features'], batch['genre']), training=False)\n\n                val_loss += loss_fn((batch['next_item'], batch['next_genre']), (item_logits, genre_logits)).numpy()\n\n            print(f\"Validation Loss: {val_loss / (step + 1):.4f}\")\n\n            val_loss_history.append(val_loss / (step + 1))\n\n    return {\n\n        'loss_history': loss_history,\n\n        'metric_history': metric_history,\n\n        'val_loss_history': val_loss_history,\n\n        'val_metric_history': val_metric_history\n\n    }\n\n\n","metadata":{"id":"2_6LBqBlau9s","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:59:13.945933Z","iopub.execute_input":"2024-12-12T12:59:13.946295Z","iopub.status.idle":"2024-12-12T12:59:13.965980Z","shell.execute_reply.started":"2024-12-12T12:59:13.946266Z","shell.execute_reply":"2024-12-12T12:59:13.965073Z"}},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":"## Run with strategy","metadata":{"id":"8vN2c_coYWs2"}},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\nitems_size = preprocessor.items_size\ngenres_size = preprocessor.genres_size\n\nwith strategy.scope():\n\n    model = GRU4REC(\n\n        rnn_params=[\n\n          {\"units\": 128},\n\n          {\"units\": 128},\n\n          {\"units\": 64}\n\n        ],\n\n        item_embed_dim=32,\n\n        genre_embed_dim=32,\n\n        ffn1_units=256,\n\n        feature_dense_units=16,\n\n        items_size=items_size,\n        genres_size=genres_size\n\n    )\n\n    optimizer = keras.api.optimizers.Adam(learning_rate=0.001)\n\n    loss_fn = keras.api.losses.SparseCategoricalCrossentropy()","metadata":{"id":"Qd0w08zYC9oJ","outputId":"324f1cc5-8b00-49d9-b775-0d5612ebebb5","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:59:20.456791Z","iopub.execute_input":"2024-12-12T12:59:20.457464Z","iopub.status.idle":"2024-12-12T12:59:20.506624Z","shell.execute_reply.started":"2024-12-12T12:59:20.457430Z","shell.execute_reply":"2024-12-12T12:59:20.505841Z"}},"outputs":[{"name":"stdout","text":"items size: 237\ngenres size: 226\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"\n\n\n\n@tf.function\n\ndef distributed_train_step(batch, loss_fn, model, optimizer):\n\n    def step_fn(batch):\n\n      with tf.GradientTape() as tape:\n\n          item_logits = model((batch['item'], batch['features'], batch['genre']), training=True)\n\n          loss = loss_fn(batch['next_item'], item_logits)\n\n      gradients = tape.gradient(loss, model.trainable_variables)\n\n      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n      return loss\n\n\n\n    per_replica_losses = strategy.run(step_fn, args=(batch,))\n\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n\n\n\n@tf.function\n\ndef distributed_val_step(batch, loss_fn, model):\n\n    def step_fn(batch):\n\n        item_logits, genre_logits = model((batch['item'], batch['features'], batch['genre']), training=False)\n\n        loss = loss_fn((batch['next_item'], batch['next_genre']), (item_logits, genre_logits))\n\n        return loss\n\n\n\n    per_replica_losses = strategy.run(step_fn, args=(batch,))\n\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n\n\n\ndef train_gru4rec_with_strategy(model, train_dataset, optimizer, epochs, k, loss_fn, val_dataset=None, early_stopping=None):\n\n    loss_history = []\n\n    val_loss_history = []\n\n    val_metric_history = []\n\n    metric_history = []\n\n    for epoch in range(epochs):\n\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n\n        epoch_loss = 0.0\n\n        for step, batch in enumerate(train_dataset):\n\n            loss = distributed_train_step(batch, loss_fn, model, optimizer)\n\n            if loss is None:\n\n              print(f\"Warning: train_step returned None at step {step}\")\n\n              continue\n\n            epoch_loss += loss.numpy()\n\n\n\n        print(f\"Training Loss: {epoch_loss / (step + 1):.4f}\")\n\n        loss_history.append(epoch_loss / (step + 1))\n\n        if val_dataset:\n\n            val_loss = 0.0\n\n            for step, batch in enumerate(val_dataset):\n\n                val_loss += distributed_val_step(batch, loss_fn, model).numpy()\n\n            print(f\"Validation Loss: {val_loss / (step + 1):.4f}\")\n\n            val_loss_history.append(val_loss / (step + 1))\n\n\n\n    return {\n\n        'loss_history': loss_history,\n\n        'metric_history': metric_history,\n\n        'val_loss_history': val_loss_history,\n\n        'val_metric_history': val_metric_history\n\n    }\n","metadata":{"id":"mfG75HwPYYI8","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:59:24.868477Z","iopub.execute_input":"2024-12-12T12:59:24.869046Z","iopub.status.idle":"2024-12-12T12:59:24.880105Z","shell.execute_reply.started":"2024-12-12T12:59:24.869013Z","shell.execute_reply":"2024-12-12T12:59:24.879203Z"}},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":"## Train with distributed Training","metadata":{"id":"nfj1AZ1KaFxg"}},{"cell_type":"code","source":"from keras.api.callbacks import EarlyStopping\nimport time\n\n# from keras.api.optimizers import Adam\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                                 reduction='none')\n    loss_ = loss_object_(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_mean(loss_)\n\nstart_time = time.time()\n# history = train_gru4rec_with_strategy(model, train_dataset.take(10), optimizer, epochs=50, loss_fn=loss_function, k=8)\nhistory = train_gru4rec_with_strategy(model, dataset, optimizer, epochs=50, loss_fn=loss_function, k=8)\nprint(\"Execution time: {}\".format(time.time() - start_time))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Q_1BbWZIaLfQ","outputId":"de3bebd4-74ea-4845-d6db-3c0bc2618964","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:00:04.785502Z","iopub.execute_input":"2024-12-12T13:00:04.785943Z","iopub.status.idle":"2024-12-12T13:00:16.352260Z","shell.execute_reply.started":"2024-12-12T13:00:04.785886Z","shell.execute_reply":"2024-12-12T13:00:16.351253Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'gru4rec', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 6.5689\nEpoch 2/50\nTraining Loss: 6.5041\nEpoch 3/50\nTraining Loss: 6.4422\nEpoch 4/50\nTraining Loss: 6.3711\nEpoch 5/50\nTraining Loss: 6.2894\nEpoch 6/50\nTraining Loss: 6.2135\nEpoch 7/50\nTraining Loss: 6.1250\nEpoch 8/50\nTraining Loss: 6.0495\nEpoch 9/50\nTraining Loss: 5.9571\nEpoch 10/50\nTraining Loss: 5.8174\nEpoch 11/50\nTraining Loss: 5.7309\nEpoch 12/50\nTraining Loss: 5.5980\nEpoch 13/50\nTraining Loss: 5.4648\nEpoch 14/50\nTraining Loss: 5.3371\nEpoch 15/50\nTraining Loss: 5.2469\nEpoch 16/50\nTraining Loss: 5.1162\nEpoch 17/50\nTraining Loss: 4.9323\nEpoch 18/50\nTraining Loss: 4.8021\nEpoch 19/50\nTraining Loss: 4.6443\nEpoch 20/50\nTraining Loss: 4.5212\nEpoch 21/50\nTraining Loss: 4.3890\nEpoch 22/50\nTraining Loss: 4.2585\nEpoch 23/50\nTraining Loss: 4.0739\nEpoch 24/50\nTraining Loss: 3.9856\nEpoch 25/50\nTraining Loss: 3.8730\nEpoch 26/50\nTraining Loss: 3.6894\nEpoch 27/50\nTraining Loss: 3.6881\nEpoch 28/50\nTraining Loss: 3.5621\nEpoch 29/50\nTraining Loss: 3.4101\nEpoch 30/50\nTraining Loss: 3.2481\nEpoch 31/50\nTraining Loss: 3.2490\nEpoch 32/50\nTraining Loss: 3.0499\nEpoch 33/50\nTraining Loss: 2.9621\nEpoch 34/50\nTraining Loss: 2.9257\nEpoch 35/50\nTraining Loss: 2.7583\nEpoch 36/50\nTraining Loss: 2.6077\nEpoch 37/50\nTraining Loss: 2.5320\nEpoch 38/50\nTraining Loss: 2.4578\nEpoch 39/50\nTraining Loss: 2.3210\nEpoch 40/50\nTraining Loss: 2.2156\nEpoch 41/50\nTraining Loss: 2.1890\nEpoch 42/50\nTraining Loss: 2.0773\nEpoch 43/50\nTraining Loss: 2.0468\nEpoch 44/50\nTraining Loss: 1.9188\nEpoch 45/50\nTraining Loss: 1.8770\nEpoch 46/50\nTraining Loss: 1.7603\nEpoch 47/50\nTraining Loss: 1.6086\nEpoch 48/50\nTraining Loss: 1.5983\nEpoch 49/50\nTraining Loss: 1.4364\nEpoch 50/50\nTraining Loss: 1.3946\nExecution time: 11.560771942138672\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"## Run the training process","metadata":{"id":"RASvcPZbau9t"}},{"cell_type":"markdown","source":"Note: Don't run this if you have run using the distributed training","metadata":{"id":"Haly4qb9uuzW"}},{"cell_type":"code","source":"# Define the model\n\n\n\n\n\nmodel = GRU4REC(\n\n    rnn_params=[\n\n        {\"units\": 128},\n\n        {\"units\": 128},\n\n        {\"units\": 64}\n\n    ],\n\n    item_embed_dim=32,\n\n    genre_embed_dim=32,\n\n    ffn1_units=256,\n\n    feature_dense_units=16,\n\n    preprocessed_data=preprocessor\n\n)\n\n","metadata":{"id":"PSkPj14Jau9u","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T15:08:18.271770Z","iopub.execute_input":"2024-12-11T15:08:18.272453Z","iopub.status.idle":"2024-12-11T15:08:18.298290Z","shell.execute_reply.started":"2024-12-11T15:08:18.272419Z","shell.execute_reply":"2024-12-11T15:08:18.297457Z"}},"outputs":[{"name":"stdout","text":"items size: 10676\ngenres size: 1200\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# Compile the model\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                                 reduction='none')\n    loss_ = loss_object_(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n    \n# Train the model\n\ntrain_gru4rec(model=model, train_dataset=dataset,optimizer=optimizer, epochs=10, k=8, loss_fn=loss_function)","metadata":{"id":"GlczKLIHau9v","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_recall_at_k(predicted_items, targets, k):\n    \"\"\"\n    Compute the recall@k for the given predictions and targets.\n    \n    Args:\n    - predicted_items: The predicted items (batch_size, seq_length, num_of_unique_items).\n    - targets: The actual next items (batch_size, seq_length).\n    - k: The top-k value for recall calculation.\n    \n    Returns:\n    - recall_at_k: The recall at k metric.\n    \"\"\"\n    # Mask to ignore padding (0 value) in target\n    valid_mask = targets != 0  # Ignoring padding value (0)\n\n    # Create a tensor of top-k predictions for the entire batch\n    top_k_predictions = tf.argsort(predicted_items, axis=-1, direction='DESCENDING')[..., :k]\n\n    # Compute true positives\n    # Reshape targets to match top_k_predictions shape for comparison\n    targets_expanded = tf.expand_dims(targets, axis=-1)\n    \n    # Check if targets are in top-k predictions\n    matches = tf.reduce_any(tf.equal(top_k_predictions, targets_expanded), axis=-1)\n    \n    # Apply the valid mask to focus on non-padding items\n    matches = tf.boolean_mask(matches, valid_mask)\n    \n    # Calculate recall\n    total_true_positives = tf.reduce_sum(tf.cast(matches, tf.float32))\n    total_relevant_items = tf.reduce_sum(tf.cast(valid_mask, tf.float32))\n    \n    # Compute recall\n    recall_at_k = total_true_positives / (total_relevant_items + tf.keras.backend.epsilon())\n    \n    return recall_at_k\n\n# Initialize variables to calculate overall recall\nmean_recall_at_k = 0\n\n# Loop through training dataset and predict the most probable item\nk = 10\nfor step, batch in enumerate(val_dataset):  # Ensure correct syntax for take()\n\n    item_sequences = batch['item']\n    item_genres = batch['genre']\n    item_features = batch['features']\n    targets = batch['next_item']\n\n    # Get the predicted items (predicted items should be a sequence)\n    predicted_items = model((item_sequences, item_features, item_genres), training=False)  # Assuming predict function is implemented\n    # print(\"Predicted Item Shape:\", predicted_items.shape)\n\n    batch_recall_at_k = compute_recall_at_k(predicted_items, targets, k)\n\n    print(f\"Batch {step + 1} Recall at {k}: {batch_recall_at_k:.4f}\")\n    mean_recall_at_k = (mean_recall_at_k*(step) + batch_recall_at_k) / (step+1)\n\nprint(f\"Average Recall@{k} = {mean_recall_at_k}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dmy4kH_bau9v","outputId":"a43ccf20-8f5c-458a-df54-8df1827b206f","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T16:28:29.664410Z","iopub.execute_input":"2024-12-11T16:28:29.664762Z","iopub.status.idle":"2024-12-11T16:28:48.225031Z","shell.execute_reply.started":"2024-12-11T16:28:29.664731Z","shell.execute_reply":"2024-12-11T16:28:48.224129Z"}},"outputs":[{"name":"stdout","text":"Batch 1 Recall at 10: 0.9920\nBatch 2 Recall at 10: 1.0000\nBatch 3 Recall at 10: 0.9930\nBatch 4 Recall at 10: 0.9960\nBatch 5 Recall at 10: 0.9679\nBatch 6 Recall at 10: 0.9786\nBatch 7 Recall at 10: 0.9847\nBatch 8 Recall at 10: 1.0000\nBatch 9 Recall at 10: 0.9928\nBatch 10 Recall at 10: 0.9731\nBatch 11 Recall at 10: 0.9904\nBatch 12 Recall at 10: 0.9888\nBatch 13 Recall at 10: 0.9796\nBatch 14 Recall at 10: 0.9975\nBatch 15 Recall at 10: 0.9969\nBatch 16 Recall at 10: 0.9964\nBatch 17 Recall at 10: 0.9815\nBatch 18 Recall at 10: 0.9967\nBatch 19 Recall at 10: 0.9815\nBatch 20 Recall at 10: 0.9907\nBatch 21 Recall at 10: 0.9770\nBatch 22 Recall at 10: 0.9910\nBatch 23 Recall at 10: 0.9879\nBatch 24 Recall at 10: 0.9739\nBatch 25 Recall at 10: 0.9719\nBatch 26 Recall at 10: 0.9740\nBatch 27 Recall at 10: 0.9610\nBatch 28 Recall at 10: 0.9403\nBatch 29 Recall at 10: 0.9768\nBatch 30 Recall at 10: 0.9732\nBatch 31 Recall at 10: 0.9919\nBatch 32 Recall at 10: 0.9776\nBatch 33 Recall at 10: 0.9750\nBatch 34 Recall at 10: 0.9847\nBatch 35 Recall at 10: 0.9895\nBatch 36 Recall at 10: 0.9773\nBatch 37 Recall at 10: 0.9834\nBatch 38 Recall at 10: 0.9967\nBatch 39 Recall at 10: 0.9883\nBatch 40 Recall at 10: 0.9782\nBatch 41 Recall at 10: 0.9865\nBatch 42 Recall at 10: 0.9812\nBatch 43 Recall at 10: 0.9741\nBatch 44 Recall at 10: 0.9789\nBatch 45 Recall at 10: 0.9593\nBatch 46 Recall at 10: 0.9808\nBatch 47 Recall at 10: 0.9664\nBatch 48 Recall at 10: 0.9654\nBatch 49 Recall at 10: 0.9738\nBatch 50 Recall at 10: 0.9817\nBatch 51 Recall at 10: 0.9967\nBatch 52 Recall at 10: 0.9874\nBatch 53 Recall at 10: 0.9692\nBatch 54 Recall at 10: 0.9915\nBatch 55 Recall at 10: 0.9918\nBatch 56 Recall at 10: 0.9838\nBatch 57 Recall at 10: 0.9732\nBatch 58 Recall at 10: 0.9679\nBatch 59 Recall at 10: 0.9595\nBatch 60 Recall at 10: 0.9516\nBatch 61 Recall at 10: 0.9803\nBatch 62 Recall at 10: 0.9971\nBatch 63 Recall at 10: 1.0000\nBatch 64 Recall at 10: 0.9680\nBatch 65 Recall at 10: 0.9804\nBatch 66 Recall at 10: 0.9662\nBatch 67 Recall at 10: 0.9818\nBatch 68 Recall at 10: 0.9430\nBatch 69 Recall at 10: 0.9796\nBatch 70 Recall at 10: 0.9804\nBatch 71 Recall at 10: 0.9938\nBatch 72 Recall at 10: 0.9848\nBatch 73 Recall at 10: 0.9857\nBatch 74 Recall at 10: 0.9611\nBatch 75 Recall at 10: 0.9649\nBatch 76 Recall at 10: 1.0000\nBatch 77 Recall at 10: 0.9844\nBatch 78 Recall at 10: 0.9910\nBatch 79 Recall at 10: 0.9968\nBatch 80 Recall at 10: 0.9863\nBatch 81 Recall at 10: 0.9784\nBatch 82 Recall at 10: 0.9747\nBatch 83 Recall at 10: 0.9889\nBatch 84 Recall at 10: 1.0000\nBatch 85 Recall at 10: 0.9867\nBatch 86 Recall at 10: 0.9771\nBatch 87 Recall at 10: 1.0000\nBatch 88 Recall at 10: 0.9860\nBatch 89 Recall at 10: 0.9819\nBatch 90 Recall at 10: 0.9966\nBatch 91 Recall at 10: 0.9972\nBatch 92 Recall at 10: 0.9760\nBatch 93 Recall at 10: 1.0000\nBatch 94 Recall at 10: 0.9933\nBatch 95 Recall at 10: 0.9965\nBatch 96 Recall at 10: 0.9792\nBatch 97 Recall at 10: 0.9891\nBatch 98 Recall at 10: 0.9872\nBatch 99 Recall at 10: 0.9691\nBatch 100 Recall at 10: 0.9793\nBatch 101 Recall at 10: 0.9815\nBatch 102 Recall at 10: 0.9865\nBatch 103 Recall at 10: 0.9821\nBatch 104 Recall at 10: 0.9878\nBatch 105 Recall at 10: 0.9943\nBatch 106 Recall at 10: 1.0000\nBatch 107 Recall at 10: 0.9860\nBatch 108 Recall at 10: 0.9922\nBatch 109 Recall at 10: 0.9835\nBatch 110 Recall at 10: 0.9908\nBatch 111 Recall at 10: 0.9918\nBatch 112 Recall at 10: 0.9877\nBatch 113 Recall at 10: 0.9885\nBatch 114 Recall at 10: 0.9958\nBatch 115 Recall at 10: 0.9953\nBatch 116 Recall at 10: 0.9879\nBatch 117 Recall at 10: 0.9838\nBatch 118 Recall at 10: 0.9953\nBatch 119 Recall at 10: 0.9764\nBatch 120 Recall at 10: 0.9851\nBatch 121 Recall at 10: 0.9761\nBatch 122 Recall at 10: 0.9904\nBatch 123 Recall at 10: 0.9880\nBatch 124 Recall at 10: 0.9848\nBatch 125 Recall at 10: 0.9874\nBatch 126 Recall at 10: 0.9953\nBatch 127 Recall at 10: 0.9756\nBatch 128 Recall at 10: 0.9864\nBatch 129 Recall at 10: 0.9871\nBatch 130 Recall at 10: 0.9648\nAverage Recall@10 = 0.9833801984786987\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"def preprocess_data_single_session(\n\n    session,\n\n    feature_columns,\n\n    k=1\n\n):\n\n    \"\"\"\n\n    Preprocess a single session into TensorFlow dataset with split genre and features.\n\n\n\n    Args:\n\n    - session (list): A list of dictionaries containing session data.\n\n    - feature_columns (list): List of numerical feature column names.\n\n    - mean_values (dict): Mean values for numerical features for normalization.\n\n    - std_values (dict): Std values for numerical features for normalization.\n\n    - k (int): Minimum length of `next_item_sequences`.\n\n\n\n    Returns:\n\n    - tf.data.Dataset: TensorFlow dataset containing preprocessed data.\n\n    \"\"\"\n\n    item_sequences = []\n\n    next_item_sequences = []\n\n    genre_sequences = []\n\n    feature_sequences = []\n\n\n\n    for i in range(len(session) - 1):\n\n        # Process items\n\n        session_item_encoded = preprocessor.preprocess_song_id(session[i]['SongID'])\n\n        next_session_item_encoded = preprocessor.preprocess_song_id(session[i + 1]['SongID'])\n\n        item_sequences.append(session_item_encoded)\n\n        next_item_sequences.append(next_session_item_encoded)\n\n\n\n        # Process genre\n\n        genre_cleaned = preprocessor.clean_genre(session[i].get('spotify_genre', None))\n\n        genre_sequences.append(genre_cleaned)\n\n\n\n        # Process numerical features\n\n        numeric_features = []\n\n        for col in feature_columns:\n\n            if col != 'spotify_genre':\n\n                mean = preprocessor.mean_values.get(col, None)\n\n                std = preprocessor.std_values.get(col, None)\n\n                cleaned_feature = preprocessor.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n\n                numeric_features.append(cleaned_feature)\n\n\n\n        feature_sequences.append(numeric_features)\n\n\n\n    # Filter session if next_item_sequences length is not greater than k\n\n    if len(next_item_sequences) <= k:\n\n        print(f\"Session skipped because next item sequence length is {len(next_item_sequences)}.\")\n\n        return\n\n\n\n    print(f\"Processed session with {len(item_sequences)} items.\")\n\n\n\n    # Convert to tensors\n\n    item_sequences = tf.stack(item_sequences, axis=0)\n\n    next_item_sequences = tf.stack(next_item_sequences, axis=0)\n\n    genre_sequences_tensor = tf.constant(genre_sequences, dtype=tf.int32)\n\n    feature_sequences_tensor = tf.constant(feature_sequences, dtype=tf.float32)\n\n\n\n    # Create TensorFlow dataset\n\n    dataset = tf.data.Dataset.from_tensor_slices({\n\n        'item': item_sequences,\n\n        'genre': genre_sequences_tensor,\n\n        'features': feature_sequences_tensor,\n\n        'next_item': next_item_sequences\n\n    })\n\n\n\n    return dataset","metadata":{"id":"KyEfocm_yBRR"},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Note: Pakai Cara 3 (itu yang bener)","metadata":{"id":"r37Q_7lRu5_O"}},{"cell_type":"markdown","source":"## Cara 1","metadata":{"id":"vzF4yZg47oey"}},{"cell_type":"code","source":"def predict_next(model, item_sequences, item_features, item_genres):\n\n    \"\"\"\n\n    Predict the next item for a given input sequence.\n\n\n\n    Args:\n\n    - model: The trained model.\n\n    - item_sequences: Input item sequences (batch_size, seq_length).\n\n    - item_features: Input item features (batch_size, feature_length).\n\n    - item_genres: Input item genres (batch_size, genre_length).\n\n\n\n    Returns:\n\n    - predicted_items: Predicted next item (batch_size,).\n\n    \"\"\"\n\n    # Run inference\n\n    _, logits = model((item_sequences, item_features, item_genres), training=False)\n\n    # print(\"Logits:\", logits)\n\n\n\n    # Apply softmax to logits to get probabilities\n\n    probabilities = tf.nn.softmax(logits, axis=-1)\n\n\n\n    # Select the item with the highest probability\n\n    predicted_items = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n\n\n\n    return predicted_items.numpy()\n\n\n\n\n\ndef compute_recall_at_k(predicted_sequence, target_sequence, k):\n\n    \"\"\"\n\n    Compute Recall@k for a given session.\n\n\n\n    Args:\n\n    - predicted_sequence: The predicted sequence of items.\n\n    - target_sequence: The actual target sequence of items.\n\n    - k: The number of items in the predicted sequence.\n\n\n\n    Returns:\n\n    - recall_at_k: Recall@k value.\n\n    \"\"\"\n\n    # Count how many of the target items appear in the predicted sequence\n\n    predicted_ids = [item['SongID'] if isinstance(item, dict) else item for item in predicted_sequence]\n\n    target_ids = [item['SongID'] if isinstance(item, dict) else item for item in target_sequence]\n\n\n\n    # Count how many of the target items appear in the predicted sequence\n\n    hits = len(set(predicted_ids[:k]) & set(target_ids[:k]))\n\n    return hits / len(target_ids[:k]) if target_ids[:k] else 0.0\n\n    return hits / len(target_sequence[:k])\n\n\n\n\n\n# Initialize overall metrics\n\ntotal_recall = 0\n\nsession_count = 0\n\nk = 10  # Set the value of k\n\nprint(\"Creating session dataset\")\n\nsessions_data = preprocessor.create_session_dataset(preprocessor.train_df)  # Use train data for training\n\nprint(\"Creating tensor dataset\")\n\n# Process each session in the dataset\n\nfor session in sessions_data[:100]:  # Assume train_sessions is your preprocessed session data\n\n    if len(session) <= k:\n\n      continue\n\n    # print(session)\n\n    context_length = len(session) - k\n\n    context = session[:context_length]\n\n    target = session[context_length:]\n\n    dataset = preprocess_data_single_session(session, feature_columns, k=k)\n\n    if dataset is None:\n\n        continue\n\n    dataset = dataset.batch(1)\n\n    predicted_sequence = []\n\n    current_sequence = context\n\n\n\n    # Generate k predictions iteratively\n\n    for batch in dataset:\n\n        # Prepare input features for the current sequence\n\n        item_sequences = batch['item']  # Batch of size 1\n\n        item_features = batch['features']  # Extract corresponding features\n\n        item_genres = batch['genre']  # Extract corresponding genres\n\n\n\n        # Predict the next item\n\n        predicted_item = predict_next(model, item_sequences, item_features, item_genres)\n\n        predicted_sequence.append(predicted_item[0])  # Append the prediction\n\n\n\n        # Update the current sequence\n\n        current_sequence = current_sequence[1:] + [predicted_item[0]]\n\n\n\n        if len(predicted_sequence) >= k:\n\n            break\n\n    ori_pred_seq = preprocessor.song_id_encoder.inverse_transform(predicted_sequence)\n\n    print(f\"Predicted Sequence: {ori_pred_seq}\")\n\n    # target_ids = preprocessor.song_id_encoder.transform([item['SongID'] if isinstance(item, dict) else item for item in target])\n\n    print(f\"True Sequence: {[item['SongID'] for item in target]}\")\n\n\n\n    # check genre\n\n    original_item_genres = preprocessor.train_df.loc[preprocessor.train_df['SongID'].isin([item['SongID'] if isinstance(item, dict) else item for item in target]), 'spotify_genre'].values\n\n    # print(f\"Original Item Genres: {original_item_genres}\")\n\n    predicted_item_genres = preprocessor.train_df.loc[preprocessor.train_df['SongID'].isin([item['SongID'] if isinstance(item, dict) else item for item in ori_pred_seq]), 'spotify_genre'].values\n\n    # print(f\"Predicted Item Genres: {predicted_item_genres}\")\n\n    # Calculate Recall@k for the current session\n\n    session_recall = compute_recall_at_k(predicted_sequence, target, k)\n\n    total_recall += session_recall\n\n    session_count += 1\n\n\n\n    print(f\"Session recall: {session_recall:.4f}\")\n\n\n\n# Calculate overall Recall@k\n\noverall_recall_at_k = total_recall / session_count if session_count > 0 else 0\n\nprint(f\"Overall Recall@{k}: {overall_recall_at_k:.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZAs7Kq0au9w","outputId":"d2c3c328-2726-4dde-ee9b-933c4fb89af7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cara 2","metadata":{"id":"tbOSEngl7lMn"}},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\ndef predict_next(model, item_sequences, item_features, item_genres, k=5):\n\n    \"\"\"\n\n    Predict the next item for a given input sequence.\n\n\n\n    Args:\n\n    - model: The trained model.\n\n    - item_sequences: Input item sequences (batch_size, seq_length).\n\n    - item_features: Input item features (batch_size, feature_length).\n\n    - item_genres: Input item genres (batch_size, genre_length).\n\n    - k: Number of top predictions to consider.\n\n\n\n    Returns:\n\n    - predicted_items: Top k predicted next items (batch_size, k).\n\n    \"\"\"\n\n    # Run inference\n\n    _, logits = model((item_sequences, item_features, item_genres), training=False)\n\n\n\n    # Apply softmax to logits to get probabilities\n\n    probabilities = tf.nn.softmax(logits, axis=-1)\n\n\n\n    # Get the top k predictions\n\n    top_k_values, top_k_indices = tf.nn.top_k(probabilities, k=k, sorted=True)\n\n\n\n    return top_k_indices.numpy()  # Return top k item indices\n\n\n\ndef compute_recall_at_k(predicted_sequence, target_sequence, k):\n\n    \"\"\"\n\n    Compute Recall@k for a given session.\n\n\n\n    Args:\n\n    - predicted_sequence: The predicted sequence of items.\n\n    - target_sequence: The actual target sequence of items.\n\n    - k: The number of top predictions to consider.\n\n\n\n    Returns:\n\n    - recall_at_k: Recall@k value.\n\n    \"\"\"\n\n    # Extract 'SongID' from dictionaries if needed\n\n    predicted_ids = [item['SongID'] if isinstance(item, dict) else item for item in predicted_sequence]\n\n    target_ids = [item['SongID'] if isinstance(item, dict) else item for item in target_sequence]\n\n\n\n    # Select the top k items from predicted sequence\n\n    top_k_predicted = predicted_ids[:k]\n\n\n\n    # Count how many of the target items appear in the top k predictions\n\n    hits = len(set(top_k_predicted) & set(target_ids[:k]))\n\n    return hits / len(target_ids[:k]) if target_ids[:k] else 0.0\n\n\n\n\n\n# Initialize overall metrics\n\ntotal_recall = 0\n\nsession_count = 0\n\nk = 5  # Set the value of k\n\n\n\nprint(\"Creating session dataset\")\n\nsessions_data = preprocessor.create_session_dataset(preprocessor.train_df)  # Use train data for training\n\nprint(\"Creating tensor dataset\")\n\n\n\n# Process each session in the dataset\n\nfor session in sessions_data:  # Assume sessions_data is your preprocessed session data\n\n    if len(session) <= k:\n\n        continue\n\n    print(session)\n\n    context_length = len(session) - k\n\n    context = session[:context_length]\n\n    target = session[context_length:]\n\n    dataset = preprocess_data_single_session(session, feature_columns, k=k)\n\n    if dataset is None:\n\n        continue\n\n    dataset = dataset.batch(1)\n\n    predicted_sequence = []\n\n    current_sequence = context\n\n\n\n    # Generate k predictions iteratively\n\n    for batch in dataset:\n\n        # Prepare input features for the current sequence\n\n        item_sequences = batch['item']  # Batch of size 1\n\n        item_features = batch['features']  # Extract corresponding features\n\n        item_genres = batch['genre']  # Extract corresponding genres\n\n\n\n        # Predict the top k items\n\n        top_k_predictions = predict_next(model, item_sequences, item_features, item_genres, k=k)\n\n\n\n        # Append the top k predictions to the sequence (only append first item from the top k)\n\n        predicted_sequence.extend(top_k_predictions[0])  # Assuming batch size is 1\n\n\n\n        # Update the current sequence\n\n        current_sequence = current_sequence[1:] + [top_k_predictions[0][0]]  # Only use first predicted item for the next context\n\n\n\n        if len(predicted_sequence) >= k:\n\n            break\n\n\n\n    # Calculate Recall@k for the current session\n\n    session_recall = compute_recall_at_k(predicted_sequence, target, k)\n\n    total_recall += session_recall\n\n    session_count += 1\n\n\n\n    print(f\"Session recall: {session_recall:.4f}\")\n\n\n\n# Calculate overall Recall@k\n\noverall_recall_at_k = total_recall / session_count if session_count > 0 else 0\n\nprint(f\"Overall Recall@{k}: {overall_recall_at_k:.4f}\")\n","metadata":{"id":"KRbYXzqWwHJa"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cara 3","metadata":{"id":"MVjdRzAd7th2"}},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\ndef predict_next(model, item_sequences, item_features, item_genres, k=5):\n\n    \"\"\"\n\n    Predict the next item for a given input sequence.\n\n\n\n    Args:\n\n    - model: The trained model.\n\n    - item_sequences: Input item sequences (batch_size, seq_length).\n\n    - item_features: Input item features (batch_size, feature_length).\n\n    - item_genres: Input item genres (batch_size, genre_length).\n\n    - k: The number of top predictions to return.\n\n\n\n    Returns:\n\n    - predicted_items: Top-k predicted items (batch_size, k).\n\n    \"\"\"\n\n    # Run inference\n\n    pred_sequence = model((item_sequences, item_features, item_genres), training=False)\n\n    \n\n    # Apply softmax to logits to get probabilities\n\n    probabilities = tf.nn.softmax(logits, axis=-1)\n\n\n\n    # Get the top-k predicted item indices based on probabilities\n\n    top_k_indices = tf.argsort(probabilities, axis=-1, direction='DESCENDING')[:, :k]\n\n\n\n    return top_k_indices.numpy()\n\n\n\n\n\n# Initialize overall metrics\n\ntotal_recall = 0\n\nsession_count = 0\n\nk = 10 # Set the value of k\n\nprint(\"Creating session dataset\")\n\nsessions_data = preprocessor.create_session_dataset(preprocessor.train_df)  # Use train data for training\n\nprint(\"Creating tensor dataset\")\n\n\n\n# Process each session in the dataset\n\nfor session in sessions_data:  # Assume train_sessions is your preprocessed session data\n\n    if len(session) <= k:\n\n        continue\n\n\n\n    context_length = len(session) - k\n\n    context = session[:context_length]\n\n    target = session[context_length:]\n\n    dataset_for_prediction = preprocess_data_single_session(session, feature_columns, k=k)\n\n    if dataset_for_prediction is None:\n\n        continue\n\n    dataset_for_prediction = dataset_for_prediction.batch(1)\n\n    predicted_sequence = []\n\n\n\n    # Generate k predictions iteratively\n\n    for batch in dataset_for_prediction:\n\n        # Prepare input features for the current sequence\n\n        item_sequences = batch['item']  # Batch of size 1\n\n        item_features = batch['features']  # Extract corresponding features\n\n        item_genres = batch['genre']  # Extract corresponding genres\n\n\n\n        # Predict the top k items\n\n        top_k_predictions = predict_next(model, item_sequences, item_features, item_genres, k)\n\n\n\n        # Add the top-k predictions to the predicted sequence\n\n        predicted_sequence.extend(top_k_predictions[0])  # Extend by top-k predicted items\n\n\n\n        if len(predicted_sequence) >= k:\n\n            break\n\n\n\n    ori_pred_seq = preprocessor.song_id_encoder.inverse_transform(predicted_sequence)\n\n    print(ori_pred_seq)\n\n    predicted_song_genre = [\n\n      {'SongID': item, 'Genre': preprocessor.train_df.loc[preprocessor.train_df['SongID'] == item, 'spotify_genre'].values[0]}\n\n      for item in ori_pred_seq\n\n    ]\n\n\n\n    # Extract SongID and genre for the true sequence\n\n    true_song_genre = [\n\n        {'SongID': item['SongID'], 'Genre': preprocessor.train_df.loc[preprocessor.train_df['SongID'] == item['SongID'], 'spotify_genre'].values[0]}\n\n        for item in target\n\n    ]\n\n\n\n    for true, predicted in zip(true_song_genre, predicted_song_genre):\n\n        print(f\"Predicted SongID: {predicted['SongID']}; True SongID: {true['SongID']}\")\n\n        print(f\"Predicted Genre: {predicted['Genre']}; True Genre: {true['Genre']}\")\n\n        print('=====================================================================')\n\n\n\n    # Print the predicted and true sequences with SongID and Genre\n\n    print(f\"Predicted Sequence: {predicted_song_genre}\")\n\n    print(f\"True Sequence: {true_song_genre}\")\n\n    # Calculate Recall@k for the current session\n\n    session_recall = compute_recall_at_k(predicted_sequence, target, k)\n\n    total_recall += session_recall\n\n    session_count += 1\n\n\n\n    print(f\"Session recall: {session_recall:.4f}\")\n\n\n\n# Calculate overall Recall@k\n\noverall_recall_at_k = total_recall / session_count if session_count > 0 else 0\n\nprint(f\"Overall Recall@{k}: {overall_recall_at_k:.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mdr6XBLu6sCq","outputId":"1863021a-582d-4fc7-8581-427a2026707d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## display training history","metadata":{"id":"zDXnvTfLpp-c"}},{"cell_type":"code","source":"def plot_training_history(loss_history, metric_name=\"Metric\", metric_history=None):\n\n    \"\"\"Plot the training loss and specified metric.\"\"\"\n\n    epochs = range(1, len(loss_history) + 1)\n\n\n\n    # Create subplots\n\n    if metric_history is not None:\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n    else:\n\n        fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n\n\n\n    # Plot the training loss\n\n    ax1.plot(epochs, loss_history, label='Loss', color='blue', linestyle='-', marker='o')\n\n    ax1.set_title('Training Loss')\n\n    ax1.set_xlabel('Epochs')\n\n    ax1.set_ylabel('Loss')\n\n    ax1.legend()\n\n    ax1.grid(True)\n\n\n\n    # Plot the specified metric if provided\n\n    if metric_history is not None:\n\n        ax2.plot(epochs, metric_history, label=metric_name, color='green', linestyle='-', marker='o')\n\n        ax2.set_title(f'Training {metric_name}')\n\n        ax2.set_xlabel('Epochs')\n\n        ax2.set_ylabel(metric_name)\n\n        ax2.legend()\n\n        ax2.grid(True)\n\n\n\n    plt.tight_layout()\n\n    plt.show()","metadata":{"id":"d5VUMhUapxyL","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:49:25.867929Z","iopub.execute_input":"2024-12-12T12:49:25.868849Z","iopub.status.idle":"2024-12-12T12:49:25.876207Z","shell.execute_reply.started":"2024-12-12T12:49:25.868812Z","shell.execute_reply":"2024-12-12T12:49:25.875157Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"plot_training_history(history['loss_history'])","metadata":{"id":"DEDRp9VQ676h","colab":{"base_uri":"https://localhost:8080/","height":607},"outputId":"5bdbc2e5-54bb-4702-f00a-7b0ef7f4ef37","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:49:27.311413Z","iopub.execute_input":"2024-12-12T12:49:27.311898Z","iopub.status.idle":"2024-12-12T12:49:27.336328Z","shell.execute_reply.started":"2024-12-12T12:49:27.311868Z","shell.execute_reply":"2024-12-12T12:49:27.335299Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_training_history(\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n","\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"],"ename":"TypeError","evalue":"'History' object is not subscriptable","output_type":"error"}],"execution_count":71},{"cell_type":"markdown","source":"# Save model","metadata":{"id":"Ye4bsSrJrlW_"}},{"cell_type":"code","source":"# Save the trained model to an H5 file\n\n# mirrored_model.save(\"gru4rec_model.keras\")\nmodel.save(\"gru4rec_model.keras\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soidj3f9qNgh","outputId":"4becb233-e371-4115-bb5f-7b9a5ceb2a1d","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:00:57.577238Z","iopub.execute_input":"2024-12-12T13:00:57.577589Z","iopub.status.idle":"2024-12-12T13:00:57.613211Z","shell.execute_reply.started":"2024-12-12T13:00:57.577560Z","shell.execute_reply":"2024-12-12T13:00:57.612269Z"}},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":"# Test Load model","metadata":{"id":"WSiZRK4I3nGG"}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n\n\nloaded_model = load_model(\n\n    \"gru4rec_model.keras\",\n    safe_mode=False,\n    custom_objects = {\n        'GRU4REC': GRU4REC\n    }\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"0WgtMdwfrnDW","outputId":"ddcf75fd-cefe-41d0-ba43-7c3ac2cf3f5e","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:01:28.139235Z","iopub.execute_input":"2024-12-12T13:01:28.139958Z","iopub.status.idle":"2024-12-12T13:01:28.460400Z","shell.execute_reply.started":"2024-12-12T13:01:28.139923Z","shell.execute_reply":"2024-12-12T13:01:28.459456Z"}},"outputs":[{"name":"stdout","text":"items size: 237\ngenres size: 226\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"weights = loaded_model.get_weights()\n\n# Check if weights are non-zero\nis_trained = any(np.any(w != 0) for w in weights)\n\nif is_trained:\n    print(\"The model appears to be trained (non-zero weights).\")\nelse:\n    print(\"The model is not trained (weights are all zero or uninitialized).\")","metadata":{"id":"8hZiuO9Lrvrn","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:01:31.145324Z","iopub.execute_input":"2024-12-12T13:01:31.146000Z","iopub.status.idle":"2024-12-12T13:01:31.158280Z","shell.execute_reply.started":"2024-12-12T13:01:31.145970Z","shell.execute_reply":"2024-12-12T13:01:31.157199Z"}},"outputs":[{"name":"stdout","text":"The model appears to be trained (non-zero weights).\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"# sequence_length = loaded_model.layers[0].input_shape[1]  # Assuming the input shape is (batch_size, sequence_length, features)\n# print(f\"Model expects input sequences of length: {sequence_length}\")\n\ninput_shape = loaded_model.input\nprint(f\"Model expects input shape: {input_shape}\")\n\nfor i, layer in enumerate(loaded_model.layers):\n    print(f\"Layer {i}: {layer.name}\")\n    print(f\"Type: {type(layer)}\")\n    print(f\"Config: {layer.get_config()}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:01:37.892815Z","iopub.execute_input":"2024-12-12T13:01:37.893459Z","iopub.status.idle":"2024-12-12T13:01:37.944392Z","shell.execute_reply.started":"2024-12-12T13:01:37.893427Z","shell.execute_reply":"2024-12-12T13:01:37.943103Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sequence_length = loaded_model.layers[0].input_shape[1]  # Assuming the input shape is (batch_size, sequence_length, features)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(f\"Model expects input sequences of length: {sequence_length}\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel expects input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loaded_model\u001b[38;5;241m.\u001b[39mlayers):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py:228\u001b[0m, in \u001b[0;36mOperation.input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py:259\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: The layer gru4rec has never been called and thus has no defined input."],"ename":"ValueError","evalue":"The layer gru4rec has never been called and thus has no defined input.","output_type":"error"}],"execution_count":95},{"cell_type":"code","source":"import numpy as np\n\n# Input data\nitems_sequence = np.array([0, 0, 0, 0, 5, 37, 999, 165, 24, 37])\ngenres_sequence = np.array([\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],\n])\n\n# Loaded model target sequence length\ntarget_sequence_length = 20\nextra_dimension = 5\n\n# Step 1: Pad items_sequence to target length\npadded_items_sequence = np.pad(\n    items_sequence, \n    (target_sequence_length - len(items_sequence), 0), \n    mode='constant', \n    constant_values=0\n)\n\n# Step 2: Process genres_sequence\n# Initialize an array of zeros with the new shape\npadded_genres_sequence = np.zeros((target_sequence_length, genres_sequence.shape[1] + extra_dimension))\n\nfor i in range(target_sequence_length):\n    if i < len(items_sequence):\n        padded_row = np.pad(genres_sequence[i], (0, extra_dimension), mode='constant', constant_values=0)\n    else:\n        # Padding beyond original sequence length, keep zeros\n        padded_row = np.pad(np.zeros(genres_sequence.shape[1]), (0, extra_dimension), mode='constant', constant_values=0)\n    padded_genres_sequence[i-len(items_sequence)] = padded_row\n\n# Step 3: Add batch dimension for both sequences\npadded_items_sequence = np.expand_dims(padded_items_sequence, axis=0)  # Shape: (1, sequence_length)\npadded_genres_sequence = np.expand_dims(padded_genres_sequence, axis=0)  # Shape: (1, sequence_length, num_features)\n\n# Step 4: Handle features_sequence (if any)\nfeatures_sequence = np.array([])  # Placeholder\nfeatures_sequence = np.expand_dims(features_sequence, axis=0) if features_sequence.size > 0 else np.empty((1, 0))\n\n# Output results\nprint(\"Padded Items Sequence:\")\nprint(padded_items_sequence)\nprint(\"\\nPadded Genres Sequence:\")\nprint(padded_genres_sequence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:43:27.352581Z","iopub.execute_input":"2024-12-12T13:43:27.353240Z","iopub.status.idle":"2024-12-12T13:43:27.366406Z","shell.execute_reply.started":"2024-12-12T13:43:27.353206Z","shell.execute_reply":"2024-12-12T13:43:27.365594Z"}},"outputs":[{"name":"stdout","text":"Padded Items Sequence:\n[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   5  37 999 165\n   24  37]]\n\nPadded Genres Sequence:\n[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  [1. 2. 3. 4. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"\n\n# Now the input is ready to be passed to the model\npredicted_sequence = loaded_model(\n    (padded_items_sequence, features_sequence, padded_genres_sequence), \n    training=False\n)\n\n\n\n# Output prediction\nprint(predicted_sequence)\n\nargmax_indices = tf.argmax(predicted_sequence, axis=2)\n\n# Print the resulting indices\nprint(\"Argmax Indices (Recommendations per Time Step):\")\nprint(argmax_indices.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T13:48:07.802260Z","iopub.execute_input":"2024-12-12T13:48:07.802600Z","iopub.status.idle":"2024-12-12T13:48:07.852986Z","shell.execute_reply.started":"2024-12-12T13:48:07.802571Z","shell.execute_reply":"2024-12-12T13:48:07.851792Z"}},"outputs":[{"name":"stdout","text":"tf.Tensor(\n[[[-0.43405047  0.10213776 -0.5212281  ...  0.0335298  -0.45646548\n   -0.49395168]\n  [-0.66223687  0.13763948 -0.806867   ...  0.04974967 -0.69908124\n   -0.7849773 ]\n  [-0.9345697   0.11506276 -1.134389   ...  0.04852194 -0.97862065\n   -1.1271734 ]\n  ...\n  [-3.8346498  -0.38739395 -3.8176863  ...  0.39466196 -3.7565904\n   -3.9223816 ]\n  [-3.8000398  -0.16819523 -3.81531    ...  0.5464753  -3.7458878\n   -3.9035866 ]\n  [-3.7534513  -0.04196454 -3.755143   ...  0.5992688  -3.7021155\n   -3.8517566 ]]], shape=(1, 20, 237), dtype=float32)\nArgmax Indices (Recommendations per Time Step):\n[[185 185 185 185 185 109 109 109 109 109 109 109 109 109 109 109 109 109\n  185  83]]\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}