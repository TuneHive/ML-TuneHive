{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.api.layers import Dense, Embedding, GRU, LeakyReLU, Concatenate, Masking, Layer, StringLookup, Normalization, BatchNormalization\n",
    "from keras.api import Input\n",
    "from keras.api.models import Model\n",
    "from keras.api.losses import SparseCategoricalCrossentropy\n",
    "from keras.api.metrics import SparseCategoricalAccuracy, Mean, TopKCategoricalAccuracy\n",
    "# from transformers.models.bert import TFBertTokenizer, TFBertEmbeddings  # embedding and tokenizer for description/nlp related stufff\n",
    "from keras.api.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>SongID</th>\n",
       "      <th>TimeStamp_Central</th>\n",
       "      <th>Performer_x</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song_x</th>\n",
       "      <th>TimeStamp_UTC</th>\n",
       "      <th>index_y</th>\n",
       "      <th>Performer_y</th>\n",
       "      <th>Song_y</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>session_3_hour</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Twenty Five MilesEdwin Starr</td>\n",
       "      <td>5/25/2021 5:18:00 PM</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>25 Miles</td>\n",
       "      <td>Twenty Five Miles</td>\n",
       "      <td>2021-05-25 23:18:00</td>\n",
       "      <td>9761</td>\n",
       "      <td>Edwin Starr</td>\n",
       "      <td>Twenty Five Miles</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.964</td>\n",
       "      <td>124.567</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Devil's EyesGreyhounds</td>\n",
       "      <td>5/25/2021 5:15:00 PM</td>\n",
       "      <td>Greyhounds</td>\n",
       "      <td>Change of Pace</td>\n",
       "      <td>Devil's Eyes</td>\n",
       "      <td>2021-05-25 23:15:00</td>\n",
       "      <td>206</td>\n",
       "      <td>Greyhounds</td>\n",
       "      <td>Devil's Eyes</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.858</td>\n",
       "      <td>113.236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pussy and PizzaMurs</td>\n",
       "      <td>5/25/2021 5:12:00 PM</td>\n",
       "      <td>Murs</td>\n",
       "      <td>Have a Nice Life</td>\n",
       "      <td>Pussy and Pizza</td>\n",
       "      <td>2021-05-25 23:12:00</td>\n",
       "      <td>6404</td>\n",
       "      <td>Murs</td>\n",
       "      <td>Pussy and Pizza</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.381</td>\n",
       "      <td>93.991</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Our Special PlaceThe Heavy</td>\n",
       "      <td>5/25/2021 4:46:00 PM</td>\n",
       "      <td>The Heavy</td>\n",
       "      <td>Great Vengeance and Furious Fire</td>\n",
       "      <td>Our Special Place</td>\n",
       "      <td>2021-05-25 22:46:00</td>\n",
       "      <td>6205</td>\n",
       "      <td>The Heavy</td>\n",
       "      <td>Our Special Place</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.939</td>\n",
       "      <td>193.996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Make Peace and be FreePerfect Confusion</td>\n",
       "      <td>5/25/2021 4:39:00 PM</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Make Peace and be Free</td>\n",
       "      <td>2021-05-25 22:39:00</td>\n",
       "      <td>6051</td>\n",
       "      <td>Perfect Confusion</td>\n",
       "      <td>Make Peace and be Free</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.431</td>\n",
       "      <td>78.037</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-05-25 21:00:00</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50013</th>\n",
       "      <td>62902</td>\n",
       "      <td>From Me To You - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 10:04:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Past Masters (Vols. 1 &amp; 2 / Remastered)</td>\n",
       "      <td>From Me To You - Remastered 2009</td>\n",
       "      <td>2017-01-01 16:04:00</td>\n",
       "      <td>5693</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>From Me To You - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.6130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.966</td>\n",
       "      <td>136.125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50014</th>\n",
       "      <td>62903</td>\n",
       "      <td>And I Love Her - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 10:01:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>A Hard Day's Night (Remastered)</td>\n",
       "      <td>And I Love Her - Remastered 2009</td>\n",
       "      <td>2017-01-01 16:01:00</td>\n",
       "      <td>360</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>And I Love Her - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.636</td>\n",
       "      <td>113.312</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50015</th>\n",
       "      <td>62904</td>\n",
       "      <td>Ticket To Ride - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:58:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Help! (Remastered)</td>\n",
       "      <td>Ticket To Ride - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:58:00</td>\n",
       "      <td>9715</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Ticket To Ride - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.749</td>\n",
       "      <td>123.419</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50016</th>\n",
       "      <td>62905</td>\n",
       "      <td>Come Together - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:54:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Abbey Road (Remastered)</td>\n",
       "      <td>Come Together - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:54:00</td>\n",
       "      <td>7425</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Come Together - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.187</td>\n",
       "      <td>165.007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50017</th>\n",
       "      <td>62906</td>\n",
       "      <td>Penny Lane - Remastered 2009The Beatles</td>\n",
       "      <td>1/1/2017 9:51:00 AM</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Magical Mystery Tour (Remastered)</td>\n",
       "      <td>Penny Lane - Remastered 2009</td>\n",
       "      <td>2017-01-01 15:51:00</td>\n",
       "      <td>4401</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Penny Lane - Remastered 2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.490</td>\n",
       "      <td>113.038</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50018 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index_x                                       SongID  \\\n",
       "0            0                 Twenty Five MilesEdwin Starr   \n",
       "1            1                       Devil's EyesGreyhounds   \n",
       "2            2                          Pussy and PizzaMurs   \n",
       "3            8                   Our Special PlaceThe Heavy   \n",
       "4           10      Make Peace and be FreePerfect Confusion   \n",
       "...        ...                                          ...   \n",
       "50013    62902  From Me To You - Remastered 2009The Beatles   \n",
       "50014    62903  And I Love Her - Remastered 2009The Beatles   \n",
       "50015    62904  Ticket To Ride - Remastered 2009The Beatles   \n",
       "50016    62905   Come Together - Remastered 2009The Beatles   \n",
       "50017    62906      Penny Lane - Remastered 2009The Beatles   \n",
       "\n",
       "          TimeStamp_Central        Performer_x  \\\n",
       "0      5/25/2021 5:18:00 PM        Edwin Starr   \n",
       "1      5/25/2021 5:15:00 PM         Greyhounds   \n",
       "2      5/25/2021 5:12:00 PM               Murs   \n",
       "3      5/25/2021 4:46:00 PM          The Heavy   \n",
       "4      5/25/2021 4:39:00 PM  Perfect Confusion   \n",
       "...                     ...                ...   \n",
       "50013  1/1/2017 10:04:00 AM        The Beatles   \n",
       "50014  1/1/2017 10:01:00 AM        The Beatles   \n",
       "50015   1/1/2017 9:58:00 AM        The Beatles   \n",
       "50016   1/1/2017 9:54:00 AM        The Beatles   \n",
       "50017   1/1/2017 9:51:00 AM        The Beatles   \n",
       "\n",
       "                                         Album  \\\n",
       "0                                     25 Miles   \n",
       "1                               Change of Pace   \n",
       "2                             Have a Nice Life   \n",
       "3             Great Vengeance and Furious Fire   \n",
       "4                            Perfect Confusion   \n",
       "...                                        ...   \n",
       "50013  Past Masters (Vols. 1 & 2 / Remastered)   \n",
       "50014          A Hard Day's Night (Remastered)   \n",
       "50015                       Help! (Remastered)   \n",
       "50016                  Abbey Road (Remastered)   \n",
       "50017        Magical Mystery Tour (Remastered)   \n",
       "\n",
       "                                 Song_x        TimeStamp_UTC  index_y  \\\n",
       "0                     Twenty Five Miles  2021-05-25 23:18:00     9761   \n",
       "1                          Devil's Eyes  2021-05-25 23:15:00      206   \n",
       "2                       Pussy and Pizza  2021-05-25 23:12:00     6404   \n",
       "3                     Our Special Place  2021-05-25 22:46:00     6205   \n",
       "4                Make Peace and be Free  2021-05-25 22:39:00     6051   \n",
       "...                                 ...                  ...      ...   \n",
       "50013  From Me To You - Remastered 2009  2017-01-01 16:04:00     5693   \n",
       "50014  And I Love Her - Remastered 2009  2017-01-01 16:01:00      360   \n",
       "50015  Ticket To Ride - Remastered 2009  2017-01-01 15:58:00     9715   \n",
       "50016   Come Together - Remastered 2009  2017-01-01 15:54:00     7425   \n",
       "50017      Penny Lane - Remastered 2009  2017-01-01 15:51:00     4401   \n",
       "\n",
       "             Performer_y                            Song_y  ... mode  \\\n",
       "0            Edwin Starr                 Twenty Five Miles  ...  1.0   \n",
       "1             Greyhounds                      Devil's Eyes  ...  0.0   \n",
       "2                   Murs                   Pussy and Pizza  ...  1.0   \n",
       "3              The Heavy                 Our Special Place  ...  1.0   \n",
       "4      Perfect Confusion            Make Peace and be Free  ...  1.0   \n",
       "...                  ...                               ...  ...  ...   \n",
       "50013        The Beatles  From Me To You - Remastered 2009  ...  1.0   \n",
       "50014        The Beatles  And I Love Her - Remastered 2009  ...  0.0   \n",
       "50015        The Beatles  Ticket To Ride - Remastered 2009  ...  1.0   \n",
       "50016        The Beatles   Come Together - Remastered 2009  ...  0.0   \n",
       "50017        The Beatles      Penny Lane - Remastered 2009  ...  1.0   \n",
       "\n",
       "      speechiness acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0          0.0607       0.0595          0.000015    0.2240    0.964  124.567   \n",
       "1          0.0456       0.3540          0.000414    0.0974    0.858  113.236   \n",
       "2          0.0659       0.0708          0.000004    0.0780    0.381   93.991   \n",
       "3          0.0386       0.2720          0.003610    0.0991    0.939  193.996   \n",
       "4          0.0315       0.0138          0.000017    0.0649    0.431   78.037   \n",
       "...           ...          ...               ...       ...      ...      ...   \n",
       "50013      0.0309       0.6130          0.000000    0.2690    0.966  136.125   \n",
       "50014      0.0337       0.6400          0.000000    0.0681    0.636  113.312   \n",
       "50015      0.0678       0.0457          0.000000    0.2330    0.749  123.419   \n",
       "50016      0.0393       0.0302          0.248000    0.0926    0.187  165.007   \n",
       "50017      0.0316       0.2120          0.026000    0.1360    0.490  113.038   \n",
       "\n",
       "       time_signature       session_3_hour  session_id  \n",
       "0                 4.0  2021-05-25 21:00:00        4332  \n",
       "1                 4.0  2021-05-25 21:00:00        4332  \n",
       "2                 4.0  2021-05-25 21:00:00        4332  \n",
       "3                 4.0  2021-05-25 21:00:00        4332  \n",
       "4                 4.0  2021-05-25 21:00:00        4332  \n",
       "...               ...                  ...         ...  \n",
       "50013             4.0  2017-01-01 15:00:00           0  \n",
       "50014             4.0  2017-01-01 15:00:00           0  \n",
       "50015             4.0  2017-01-01 15:00:00           0  \n",
       "50016             4.0  2017-01-01 15:00:00           0  \n",
       "50017             4.0  2017-01-01 15:00:00           0  \n",
       "\n",
       "[50018 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "df = pd.read_csv(\"data/session-data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50018 entries, 0 to 50017\n",
      "Data columns (total 30 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index_x                    50018 non-null  int64  \n",
      " 1   SongID                     50018 non-null  object \n",
      " 2   TimeStamp_Central          50018 non-null  object \n",
      " 3   Performer_x                50018 non-null  object \n",
      " 4   Album                      47890 non-null  object \n",
      " 5   Song_x                     50018 non-null  object \n",
      " 6   TimeStamp_UTC              50018 non-null  object \n",
      " 7   index_y                    50018 non-null  int64  \n",
      " 8   Performer_y                50018 non-null  object \n",
      " 9   Song_y                     50018 non-null  object \n",
      " 10  spotify_genre              50018 non-null  object \n",
      " 11  spotify_track_id           50018 non-null  object \n",
      " 12  spotify_track_preview_url  36001 non-null  object \n",
      " 13  spotify_track_duration_ms  50018 non-null  float64\n",
      " 14  spotify_track_popularity   50018 non-null  float64\n",
      " 15  spotify_track_explicit     50018 non-null  bool   \n",
      " 16  danceability               50018 non-null  float64\n",
      " 17  energy                     50018 non-null  float64\n",
      " 18  key                        50018 non-null  float64\n",
      " 19  loudness                   50018 non-null  float64\n",
      " 20  mode                       50018 non-null  float64\n",
      " 21  speechiness                50018 non-null  float64\n",
      " 22  acousticness               50018 non-null  float64\n",
      " 23  instrumentalness           50018 non-null  float64\n",
      " 24  liveness                   50018 non-null  float64\n",
      " 25  valence                    50018 non-null  float64\n",
      " 26  tempo                      50018 non-null  float64\n",
      " 27  time_signature             50018 non-null  float64\n",
      " 28  session_3_hour             50018 non-null  object \n",
      " 29  session_id                 50018 non-null  int64  \n",
      "dtypes: bool(1), float64(14), int64(3), object(12)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        0.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "50013    1.0\n",
       "50014    0.0\n",
       "50015    1.0\n",
       "50016    0.0\n",
       "50017    1.0\n",
       "Name: mode, Length: 50018, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_col_name = 'mode'\n",
    "df.loc[:, test_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove N.A.N data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered = df[~df['danceability'].isna()]\n",
    "# df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature columns (as provided)\n",
    "feature_columns = [\n",
    "    'spotify_genre',\n",
    "    'spotify_track_popularity',\n",
    "    'danceability',\n",
    "    'loudness',\n",
    "    'acousticness',\n",
    "    'instrumentalness',\n",
    "    'tempo',\n",
    "]\n",
    "\n",
    "# Define the DataPreprocessor class\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, df, feature_columns, batch_size=16, fixed_genre_size=10, train_size=0.8):\n",
    "        \"\"\"\n",
    "        Initializes the data preprocessor with necessary parameters and preprocessing layers.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame containing session data.\n",
    "            feature_columns (list): List of feature column names.\n",
    "            batch_size (int): The batch size for dataset creation.\n",
    "            fixed_genre_size (int): The fixed size for genre vectorization.\n",
    "            train_size (float): Proportion of the data to use for training (between 0 and 1).\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_columns = feature_columns\n",
    "        self.batch_size = batch_size\n",
    "        self.fixed_genre_size = fixed_genre_size\n",
    "        self.train_size = train_size\n",
    "\n",
    "        # Split the dataset into training and testing datasets\n",
    "        self.train_df, self.test_df = train_test_split(self.df, train_size=self.train_size, random_state=42)\n",
    "        \n",
    "        # Numeric feature preprocessing\n",
    "        self.numeric_data = self.df[feature_columns[1:]].apply(pd.to_numeric, errors='coerce')\n",
    "        self.mean_values = self.numeric_data.mean()\n",
    "        self.std_values = self.numeric_data.std()\n",
    "\n",
    "        # Initialize LabelEncoder for SongID and spotify_genre\n",
    "        self.song_id_encoder = LabelEncoder()\n",
    "        self.genre_encoder = LabelEncoder()\n",
    "\n",
    "        # Extract unique SongIDs and genres\n",
    "        unique_song_ids = self.df['SongID'].unique()\n",
    "        all_genres = []\n",
    "        for genre_str in self.df['spotify_genre']:\n",
    "            try:\n",
    "                genre_list = ast.literal_eval(genre_str)  # Safely parse the string into a list\n",
    "                if isinstance(genre_list, list):\n",
    "                    all_genres.extend(genre_list)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing genre: {e}\")\n",
    "\n",
    "        unique_genres = list(set(all_genres))\n",
    "\n",
    "        # Fit the LabelEncoders on the data\n",
    "        self.song_id_encoder.fit(unique_song_ids)\n",
    "        self.genre_encoder.fit(unique_genres)\n",
    "\n",
    "        self.items_size = len(self.song_id_encoder.classes_)  # Number of unique SongIDs\n",
    "        self.genres_size = len(self.genre_encoder.classes_)  \n",
    "        \n",
    "        self.dataset = None\n",
    "\n",
    "    def preprocess_song_id(self, song_id):\n",
    "        \"\"\"\n",
    "        Encode the SongID using LabelEncoder.\n",
    "        \"\"\"\n",
    "        return self.song_id_encoder.transform([song_id])[0]\n",
    "\n",
    "    def clean_genre(self, value, default_value=0, dtype=tf.int32):\n",
    "        \"\"\"\n",
    "        Clean and process the 'spotify_genre' feature.\n",
    "        \"\"\"\n",
    "        if value is None or (isinstance(value, str) and not value.strip()):\n",
    "            return np.full((self.fixed_genre_size,), default_value, dtype=dtype.as_numpy_dtype)\n",
    "\n",
    "        try:\n",
    "            genre_list = eval(value) if isinstance(value, str) else value\n",
    "            if isinstance(genre_list, list):\n",
    "                genre_encoded = self.genre_encoder.transform(genre_list)\n",
    "            else:\n",
    "                genre_encoded = self.genre_encoder.transform([value])\n",
    "        except Exception:\n",
    "            genre_encoded = self.genre_encoder.transform([value])\n",
    "\n",
    "        # Pad or truncate to fixed size\n",
    "        return np.pad(genre_encoded, (0, max(0, self.fixed_genre_size - len(genre_encoded))),\n",
    "                      mode='constant')[:self.fixed_genre_size].astype(dtype.as_numpy_dtype)\n",
    "\n",
    "    def clean_numeric_feature(self, value, default_value=0.0, feature_name=\"feature\", mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Clean, process, and normalize numerical features using Z-score normalization.\n",
    "        \"\"\"\n",
    "        if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "            return default_value\n",
    "\n",
    "        try:\n",
    "            value = float(value)\n",
    "            # Apply Z-score normalization if mean and std are provided\n",
    "            if mean is not None and std is not None and std != 0:\n",
    "                z_score_value = (value - mean) / std\n",
    "                return z_score_value\n",
    "            return value  # Return raw value if no normalization\n",
    "        except ValueError:\n",
    "            return default_value\n",
    "\n",
    "    def create_session_dataset(self, session_df):\n",
    "        \"\"\"\n",
    "        Create session dataset as a list of dictionaries for each session.\n",
    "        \"\"\"\n",
    "        session_df = session_df.sort_values(by=['session_id', 'TimeStamp_UTC'])\n",
    "        grouped = session_df.groupby('session_id')\n",
    "        sessions_data = []\n",
    "        for session_id, group in grouped:\n",
    "            session_data = group.to_dict(orient='records')\n",
    "            sessions_data.append(session_data)\n",
    "        return sessions_data\n",
    "\n",
    "    def preprocess_data(self, sessions, k=1):\n",
    "        \"\"\"\n",
    "        Preprocess session data into TensorFlow dataset with split genre and features,\n",
    "        filtering out sequences where the next item sequence length is not greater than 10.\n",
    "        \"\"\"\n",
    "        item_sequences = []\n",
    "        next_item_sequences = []\n",
    "        genre_sequences = []\n",
    "        feature_sequences = []\n",
    "        processed_item_count = 0\n",
    "\n",
    "        for idx, session in enumerate(sessions):\n",
    "            session_item_sequences = []\n",
    "            session_next_item_sequences = []\n",
    "            session_genre_sequences = []\n",
    "            session_feature_sequences = []\n",
    "\n",
    "            for i in range(len(session) - 1):\n",
    "                # Process items\n",
    "                session_item_encoded = self.preprocess_song_id(session[i]['SongID'])\n",
    "                next_session_item_encoded = self.preprocess_song_id(session[i + 1]['SongID'])\n",
    "                session_item_sequences.append(session_item_encoded)\n",
    "                session_next_item_sequences.append(next_session_item_encoded)\n",
    "\n",
    "                # Process genre\n",
    "                genre_cleaned = self.clean_genre(session[i].get('spotify_genre', None))\n",
    "                session_genre_sequences.append(genre_cleaned)\n",
    "\n",
    "                # Process numerical features\n",
    "                numeric_features = []\n",
    "                for col in self.feature_columns:\n",
    "                    if col != 'spotify_genre':\n",
    "                        mean = self.mean_values.get(col, None)\n",
    "                        std = self.std_values.get(col, None)\n",
    "                        cleaned_feature = self.clean_numeric_feature(session[i].get(col, None), mean=mean, std=std)\n",
    "                        numeric_features.append(cleaned_feature)\n",
    "\n",
    "                session_feature_sequences.append(numeric_features)\n",
    "\n",
    "            # Filter out sessions where the next item sequence length is not greater than 10\n",
    "            if len(session_next_item_sequences) > k:\n",
    "                # Extend sequences only if the next item sequence length is greater than 10\n",
    "                item_sequences.extend(session_item_sequences)\n",
    "                next_item_sequences.extend(session_next_item_sequences)\n",
    "                genre_sequences.extend(session_genre_sequences)\n",
    "                feature_sequences.extend(session_feature_sequences)\n",
    "                processed_item_count += len(session_item_sequences)\n",
    "\n",
    "                print(f\"Session {idx + 1} processed with {len(session_item_sequences)} items.\")\n",
    "            else:\n",
    "                print(f\"Session {idx + 1} skipped because next item sequence length is {len(session_next_item_sequences)}.\")\n",
    "\n",
    "        print(f\"Total processed items: {processed_item_count}\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        item_sequences = tf.stack(item_sequences, axis=-1)\n",
    "        next_item_sequences = tf.stack(next_item_sequences, axis=-1)\n",
    "        genre_sequences_tensor = tf.constant(genre_sequences, dtype=tf.int32)\n",
    "        feature_sequences_tensor = tf.constant(feature_sequences, dtype=tf.float32)\n",
    "\n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'item': item_sequences,\n",
    "            'genre': genre_sequences_tensor,\n",
    "            'features': feature_sequences_tensor,\n",
    "            'next_item': next_item_sequences\n",
    "        })\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_session_dataset_tensor(self, k):\n",
    "        \"\"\"\n",
    "        Main function to create session dataset as tensors and return the dataset.\n",
    "        \"\"\"\n",
    "        if self.dataset is not None:\n",
    "            print(\"Dataset already created\")\n",
    "            return\n",
    "        \n",
    "        print(\"Creating session dataset\")\n",
    "        sessions_data = self.create_session_dataset(self.train_df)  # Use train data for training\n",
    "        print(\"Creating tensor dataset\")\n",
    "        dataset = self.preprocess_data(sessions_data, k=k)\n",
    "\n",
    "        # Shuffle and batch the training data\n",
    "        dataset = dataset.shuffle(buffer_size=1024).batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        return dataset\n",
    "\n",
    "    def get_test_data(self, k):\n",
    "        \"\"\"\n",
    "        Return preprocessed test dataset without shuffling.\n",
    "        \"\"\"\n",
    "        sessions_data = self.create_session_dataset(self.test_df)\n",
    "        dataset = self.preprocess_data(sessions_data, k)\n",
    "\n",
    "        # Batch the test data without shuffling\n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def batch_timer(self, dataset):\n",
    "        \"\"\"\n",
    "        Timer function to track the time taken for batch processing.\n",
    "        \"\"\"\n",
    "        for batch in dataset:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Simulate processing (e.g., model training or data transformation)\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            print(f\"Batch processing time: {batch_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating session dataset\n",
      "Creating tensor dataset\n",
      "Session 1 processed with 4 items.\n",
      "Session 2 processed with 6 items.\n",
      "Session 3 processed with 18 items.\n",
      "Session 4 skipped because next item sequence length is 1.\n",
      "Session 5 processed with 3 items.\n",
      "Session 6 processed with 10 items.\n",
      "Session 7 processed with 22 items.\n",
      "Session 8 processed with 9 items.\n",
      "Session 9 skipped because next item sequence length is 2.\n",
      "Session 10 processed with 9 items.\n",
      "Session 11 processed with 8 items.\n",
      "Session 12 processed with 9 items.\n",
      "Session 13 processed with 12 items.\n",
      "Session 14 processed with 11 items.\n",
      "Session 15 processed with 15 items.\n",
      "Session 16 skipped because next item sequence length is 2.\n",
      "Session 17 processed with 6 items.\n",
      "Session 18 processed with 14 items.\n",
      "Session 19 skipped because next item sequence length is 2.\n",
      "Session 20 processed with 8 items.\n",
      "Session 21 processed with 9 items.\n",
      "Session 22 processed with 15 items.\n",
      "Session 23 processed with 6 items.\n",
      "Session 24 processed with 6 items.\n",
      "Session 25 processed with 5 items.\n",
      "Session 26 skipped because next item sequence length is 1.\n",
      "Session 27 processed with 8 items.\n",
      "Session 28 processed with 3 items.\n",
      "Session 29 processed with 6 items.\n",
      "Session 30 processed with 6 items.\n",
      "Session 31 processed with 18 items.\n",
      "Session 32 processed with 13 items.\n",
      "Session 33 processed with 3 items.\n",
      "Session 34 skipped because next item sequence length is 0.\n",
      "Session 35 skipped because next item sequence length is 1.\n",
      "Session 36 processed with 5 items.\n",
      "Session 37 processed with 5 items.\n",
      "Session 38 processed with 6 items.\n",
      "Session 39 processed with 3 items.\n",
      "Session 40 processed with 6 items.\n",
      "Session 41 processed with 16 items.\n",
      "Session 42 processed with 9 items.\n",
      "Session 43 processed with 5 items.\n",
      "Session 44 processed with 3 items.\n",
      "Session 45 processed with 22 items.\n",
      "Session 46 processed with 11 items.\n",
      "Session 47 skipped because next item sequence length is 2.\n",
      "Session 48 processed with 6 items.\n",
      "Session 49 processed with 9 items.\n",
      "Session 50 skipped because next item sequence length is 2.\n",
      "Session 51 processed with 3 items.\n",
      "Session 52 processed with 4 items.\n",
      "Session 53 skipped because next item sequence length is 2.\n",
      "Session 54 processed with 5 items.\n",
      "Session 55 processed with 5 items.\n",
      "Session 56 processed with 3 items.\n",
      "Session 57 processed with 10 items.\n",
      "Session 58 processed with 9 items.\n",
      "Session 59 processed with 5 items.\n",
      "Session 60 processed with 4 items.\n",
      "Session 61 processed with 10 items.\n",
      "Session 62 processed with 4 items.\n",
      "Session 63 processed with 13 items.\n",
      "Session 64 processed with 6 items.\n",
      "Session 65 processed with 12 items.\n",
      "Session 66 processed with 5 items.\n",
      "Session 67 processed with 6 items.\n",
      "Session 68 processed with 14 items.\n",
      "Session 69 skipped because next item sequence length is 2.\n",
      "Session 70 skipped because next item sequence length is 2.\n",
      "Session 71 processed with 5 items.\n",
      "Session 72 processed with 5 items.\n",
      "Session 73 skipped because next item sequence length is 2.\n",
      "Session 74 processed with 3 items.\n",
      "Session 75 processed with 12 items.\n",
      "Session 76 processed with 12 items.\n",
      "Session 77 processed with 10 items.\n",
      "Session 78 skipped because next item sequence length is 1.\n",
      "Session 79 skipped because next item sequence length is 1.\n",
      "Session 80 skipped because next item sequence length is 2.\n",
      "Session 81 processed with 11 items.\n",
      "Session 82 processed with 7 items.\n",
      "Session 83 skipped because next item sequence length is 1.\n",
      "Session 84 skipped because next item sequence length is 2.\n",
      "Session 85 processed with 14 items.\n",
      "Session 86 processed with 22 items.\n",
      "Session 87 processed with 6 items.\n",
      "Session 88 skipped because next item sequence length is 2.\n",
      "Session 89 processed with 7 items.\n",
      "Session 90 processed with 5 items.\n",
      "Session 91 skipped because next item sequence length is 1.\n",
      "Session 92 skipped because next item sequence length is 2.\n",
      "Session 93 processed with 12 items.\n",
      "Session 94 skipped because next item sequence length is 1.\n",
      "Session 95 processed with 12 items.\n",
      "Session 96 skipped because next item sequence length is 2.\n",
      "Session 97 processed with 10 items.\n",
      "Session 98 processed with 13 items.\n",
      "Session 99 processed with 6 items.\n",
      "Session 100 processed with 6 items.\n",
      "Total processed items: 664\n",
      "Session 1 skipped because next item sequence length is 1.\n",
      "Session 2 skipped because next item sequence length is 2.\n",
      "Session 3 processed with 3 items.\n",
      "Session 4 skipped because next item sequence length is 0.\n",
      "Session 5 processed with 5 items.\n",
      "Session 6 skipped because next item sequence length is 2.\n",
      "Session 7 skipped because next item sequence length is 2.\n",
      "Session 8 processed with 3 items.\n",
      "Session 9 skipped because next item sequence length is 0.\n",
      "Session 10 processed with 3 items.\n",
      "Session 11 skipped because next item sequence length is 1.\n",
      "Session 12 skipped because next item sequence length is 1.\n",
      "Session 13 processed with 6 items.\n",
      "Session 14 skipped because next item sequence length is 1.\n",
      "Session 15 skipped because next item sequence length is 0.\n",
      "Session 16 skipped because next item sequence length is 2.\n",
      "Session 17 skipped because next item sequence length is 1.\n",
      "Session 18 skipped because next item sequence length is 2.\n",
      "Session 19 processed with 3 items.\n",
      "Session 20 skipped because next item sequence length is 0.\n",
      "Session 21 skipped because next item sequence length is 2.\n",
      "Session 22 skipped because next item sequence length is 0.\n",
      "Session 23 skipped because next item sequence length is 2.\n",
      "Session 24 processed with 8 items.\n",
      "Session 25 skipped because next item sequence length is 2.\n",
      "Session 26 skipped because next item sequence length is 0.\n",
      "Session 27 skipped because next item sequence length is 0.\n",
      "Session 28 skipped because next item sequence length is 0.\n",
      "Session 29 skipped because next item sequence length is 2.\n",
      "Session 30 skipped because next item sequence length is 1.\n",
      "Session 31 skipped because next item sequence length is 2.\n",
      "Session 32 skipped because next item sequence length is 0.\n",
      "Session 33 skipped because next item sequence length is 1.\n",
      "Session 34 processed with 4 items.\n",
      "Session 35 skipped because next item sequence length is 0.\n",
      "Session 36 processed with 3 items.\n",
      "Session 37 processed with 3 items.\n",
      "Session 38 skipped because next item sequence length is 2.\n",
      "Session 39 skipped because next item sequence length is 0.\n",
      "Session 40 skipped because next item sequence length is 0.\n",
      "Session 41 skipped because next item sequence length is 0.\n",
      "Session 42 skipped because next item sequence length is 0.\n",
      "Session 43 skipped because next item sequence length is 0.\n",
      "Session 44 skipped because next item sequence length is 1.\n",
      "Session 45 skipped because next item sequence length is 1.\n",
      "Session 46 skipped because next item sequence length is 1.\n",
      "Session 47 skipped because next item sequence length is 0.\n",
      "Session 48 skipped because next item sequence length is 2.\n",
      "Session 49 processed with 4 items.\n",
      "Session 50 skipped because next item sequence length is 0.\n",
      "Session 51 skipped because next item sequence length is 0.\n",
      "Session 52 processed with 3 items.\n",
      "Session 53 processed with 5 items.\n",
      "Session 54 processed with 5 items.\n",
      "Session 55 skipped because next item sequence length is 1.\n",
      "Session 56 skipped because next item sequence length is 1.\n",
      "Session 57 skipped because next item sequence length is 0.\n",
      "Session 58 skipped because next item sequence length is 1.\n",
      "Session 59 skipped because next item sequence length is 1.\n",
      "Session 60 skipped because next item sequence length is 0.\n",
      "Session 61 processed with 3 items.\n",
      "Session 62 skipped because next item sequence length is 0.\n",
      "Session 63 processed with 5 items.\n",
      "Session 64 skipped because next item sequence length is 1.\n",
      "Session 65 skipped because next item sequence length is 0.\n",
      "Session 66 skipped because next item sequence length is 0.\n",
      "Session 67 skipped because next item sequence length is 0.\n",
      "Session 68 skipped because next item sequence length is 0.\n",
      "Session 69 skipped because next item sequence length is 2.\n",
      "Session 70 skipped because next item sequence length is 0.\n",
      "Session 71 skipped because next item sequence length is 1.\n",
      "Session 72 skipped because next item sequence length is 1.\n",
      "Session 73 skipped because next item sequence length is 0.\n",
      "Session 74 processed with 3 items.\n",
      "Session 75 skipped because next item sequence length is 2.\n",
      "Session 76 processed with 4 items.\n",
      "Session 77 skipped because next item sequence length is 1.\n",
      "Session 78 skipped because next item sequence length is 2.\n",
      "Session 79 skipped because next item sequence length is 0.\n",
      "Total processed items: 73\n"
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor(df[:1000], feature_columns)\n",
    "\n",
    "# Create the session dataset tensor\n",
    "train_dataset = preprocessor.create_session_dataset_tensor(k=2)\n",
    "test_dataset = preprocessor.get_test_data(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items (SongID): [391 220 149 368  74 455 139  24 148 473 230 451 156 218 486 626]\n",
      "Genre: [[ 12 227 247 262   0   0   0   0   0   0]\n",
      " [  4  92 186 207 259 290   0   0   0   0]\n",
      " [179   0   0   0   0   0   0   0   0   0]\n",
      " [ 28 174 176 268 314   0   0   0   0   0]\n",
      " [ 38 103 122 128 200 201 271 292 306   0]\n",
      " [148 150 205 246   0   0   0   0   0   0]\n",
      " [ 29  83 243 249 260 320   0   0   0   0]\n",
      " [  4  68 117 118 186 271 286 290 310   0]\n",
      " [  6  68 140 191 271   0   0   0   0   0]\n",
      " [179   0   0   0   0   0   0   0   0   0]\n",
      " [  6  38  68 140 191 256 271   0   0   0]\n",
      " [  6  21  68 140 166 254 256 271 303   0]\n",
      " [  6 142 186 271 290   0   0   0   0   0]\n",
      " [122 128 147 151 201 205 216 271 299   0]\n",
      " [122 128 147 151 201 205 216 271 299   0]\n",
      " [170   0   0   0   0   0   0   0   0   0]]\n",
      "Features: [[ 0.92993677 -0.38409698  1.0024786  -0.84509045 -0.3540921   0.97593945]\n",
      " [ 0.18184407 -0.62183243 -1.7416495   0.93507767 -0.3540921  -2.0438871 ]\n",
      " [-0.8539766   1.0358906  -0.9859964   0.24242198 -0.35262948 -1.0630704 ]\n",
      " [ 0.0092073  -0.4162234   0.7185658  -0.43646663 -0.3536296   0.0891747 ]\n",
      " [-0.0483383  -0.44192454  1.583604   -0.8371314  -0.3540756  -1.2827619 ]\n",
      " [ 1.5053928  -0.03713171  1.0540497  -0.81587845 -0.3540921   0.22535498]\n",
      " [ 1.2176647   1.614166    0.5714523  -0.7014827  -0.35363618  0.04335197]\n",
      " [ 1.045028   -0.33912    -1.0915816   2.1612072  -0.3540921   0.57824427]\n",
      " [ 0.06675289 -0.8274415  -0.99712497  0.1133557  -0.35407555  0.6749736 ]\n",
      " [-0.7388854  -0.12708567 -0.13832957 -0.3082608  -0.03785261  2.124897  ]\n",
      " [ 0.23938967 -1.836211    0.01095534 -0.7242844  -0.31627512  1.7742108 ]\n",
      " [-1.1417046  -2.0032682  -0.6494268  -0.69804096 -0.31765866 -0.9717978 ]\n",
      " [ 0.6997544   0.03997168  0.3819962   0.345675   -0.3540554  -0.34014228]\n",
      " [-0.39361185  0.10422451 -0.20265779 -0.8062415   1.5762864  -1.4797794 ]\n",
      " [-1.4294325  -0.6089819   0.15345457 -0.7832247   0.41674164 -0.51719695]\n",
      " [-1.3143413   0.25843132  0.44551015 -0.3168652  -0.3540921  -0.5841687 ]]\n",
      "Next Items (Next SongID): [294 225 203 114 300  83 372 119  54 214 382 478 143  47 202 356]\n",
      "Items (SongID): [526 110  35 434  13  15 521 400 409 445 580 501 191 458 379 450]\n",
      "Genre: [[ 11  76 198 245   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 23 121 122 128 151 201 205 213   0   0]\n",
      " [  6  68 118 142 186 246 271 276 286 290]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [  6 186 271 290 334   0   0   0   0   0]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [  6 123 133 140 271   0   0   0   0   0]\n",
      " [  6  38  68 140 256 271   0   0   0   0]\n",
      " [ 45 190 256 271   0   0   0   0   0   0]\n",
      " [  6  38  68 140 256 271   0   0   0   0]\n",
      " [ 12  13  73 137 138 140 155 156 191 227]\n",
      " [ 12 123 227 247 262 271   0   0   0   0]\n",
      " [  6  21  68 186 254 271 290 303   0   0]\n",
      " [266   0   0   0   0   0   0   0   0   0]\n",
      " [266   0   0   0   0   0   0   0   0   0]]\n",
      "Features: [[ 0.0092073   1.9161543   0.628452    0.69415396 -0.3540921  -0.830093  ]\n",
      " [-1.8322517   1.0294652  -0.07182993 -0.6657744  -0.347372   -1.0301946 ]\n",
      " [-0.68133986 -0.28129247  0.48323852 -0.5952182  -0.3540921  -1.1172308 ]\n",
      " [ 0.6997544  -0.724637    0.35729635 -0.84347284  3.7306678   1.640708  ]\n",
      " [-0.68133986  0.7596034  -0.42414233 -0.76214385  1.8134661   0.31594977]\n",
      " [ 1.6204839   0.5604196  -2.816501    0.25963083 -0.35356832 -0.9717639 ]\n",
      " [ 0.23938967  0.72747695 -0.06884423  0.11765791 -0.26317325 -1.1419723 ]\n",
      " [-0.50870305 -0.57043016 -0.32642856 -0.820826   -0.3449343  -0.6793051 ]\n",
      " [ 0.92993677 -0.32626945  0.6561376  -0.8083496   3.4539583  -0.42328072]\n",
      " [ 0.5271176  -0.8531426  -0.38749966 -0.47733763 -0.33926836 -0.87700033]\n",
      " [ 0.6422088  -2.3759346   0.27831104  1.2190235  -0.29881606  1.8360308 ]\n",
      " [-0.62379426 -1.1101539   0.7077087  -0.8459247   1.8200543   2.1206267 ]\n",
      " [ 0.98748237 -1.4828204   0.49138132 -0.845941    0.33768177 -0.61331624]\n",
      " [-2.0048885  -0.76318866  0.07121216  2.38062    -0.35408017 -1.3149599 ]\n",
      " [ 0.06675289 -1.1744068   0.39611042  1.1157705  -0.2986184  -1.0298219 ]\n",
      " [-0.10588389  1.1900973   0.45853865 -0.79225934 -0.06684123  1.6466392 ]]\n",
      "Next Items (Next SongID): [110  35  53  13  15 521 400 591 445 580 222 191 458 369 450  21]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "    print(\"Items (SongID):\", batch['item'].numpy())\n",
    "    print(\"Genre:\", batch['genre'].numpy())\n",
    "    print(\"Features:\", batch['features'].numpy())\n",
    "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())\n",
    "    \n",
    "for batch in test_dataset.take(1):\n",
    "    print(\"Items (SongID):\", batch['item'].numpy())\n",
    "    print(\"Genre:\", batch['genre'].numpy())\n",
    "    print(\"Features:\", batch['features'].numpy())\n",
    "    print(\"Next Items (Next SongID):\", batch['next_item'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemEmbedding(Layer):\n",
    "    def __init__(self, num_items, item_embed_dim):\n",
    "        super(ItemEmbedding, self).__init__()\n",
    "        \n",
    "        self.item_embedding = Embedding(input_dim=num_items, output_dim=item_embed_dim, mask_zero=True, name='item_embedding')\n",
    "\n",
    "    def call(self, items):\n",
    "        # Embed items\n",
    "        items_embedded = self.item_embedding(items)\n",
    "        return items_embedded\n",
    "\n",
    "class GRU4REC(Model):\n",
    "    def __init__(self, rnn_params, genre_embed_dim, item_embed_dim, ffn1_units, feature_dense_units,  preprocessed_data:DataPreprocessor):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        print(f\"items size: {preprocessed_data.items_size}\")\n",
    "        print(f\"genres size: {preprocessed_data.genres_size}\")\n",
    "        self.embedding = ItemEmbedding(preprocessed_data.items_size, item_embed_dim)\n",
    "        \n",
    "        # Genre embedding (only for genre, which is categorical and a string)\n",
    "        self.genre_embedding = Embedding(input_dim=preprocessed_data.genres_size, output_dim=genre_embed_dim, mask_zero=True, name='genre_embedding')\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn_layers = []\n",
    "        self.rnn_layers.append(GRU(**rnn_params[0], return_sequences=True, name='gru_0'))\n",
    "        for i in range(1, len(rnn_params) - 1):\n",
    "            self.rnn_layers.append(GRU(**rnn_params[i], return_sequences=True, name=f'gru_{i}'))\n",
    "        self.rnn_layers.append(GRU(**rnn_params[-1], return_sequences=False, name=f\"gru_{len(rnn_params)-1})\"))\n",
    "\n",
    "        self.concat = Concatenate(axis=-1, name='concat_1')\n",
    "        self.batch_norm = BatchNormalization(name='batchnorm')\n",
    "\n",
    "        # Feed-forward layers\n",
    "        self.feature_dense = Dense(feature_dense_units, activation='relu', name='feature_dense')  # Dense layer for features (if required)\n",
    "        self.ffn1 = Dense(ffn1_units, name='feed_forward_1')\n",
    "        self.activation1 = LeakyReLU(alpha=0.2, name='freaky_relu')\n",
    "        self.out = Dense(preprocessed_data.items_size, activation='softmax', name='output_layer')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass for the GRU4REC model.\n",
    "        :param inputs: Tuple (item_sequences, item_features, item_genres)\n",
    "        :param training: Boolean indicating if the model is in training mode\n",
    "        \"\"\"\n",
    "        \n",
    "        item_sequences, item_features, item_genres = inputs\n",
    "\n",
    "        # Embed items\n",
    "        item_embedded = self.embedding(item_sequences)\n",
    "        item_embedded = tf.expand_dims(item_embedded, axis=1)\n",
    "        # Genre embedding\n",
    "        genre_embedded = self.genre_embedding(item_genres)\n",
    "        genre_embedded = tf.reduce_mean(genre_embedded, axis=1)\n",
    "        genre_embedded = tf.expand_dims(genre_embedded, axis=1)\n",
    "        \n",
    "        # Feature transformation (features are passed directly as floats, so no embedding is needed)\n",
    "        feature_transformed = self.feature_dense(item_features)\n",
    "        feature_transformed = tf.expand_dims(feature_transformed, axis=1)\n",
    "\n",
    "        # Pass through RNN layers\n",
    "        x = item_embedded\n",
    "        x = self.rnn_layers[0](x)\n",
    "        for i in range(1, len(self.rnn_layers)):\n",
    "            x = self.concat([item_embedded, x])  # Concatenate item embeddings with RNN outputs\n",
    "            x = self.rnn_layers[i](x)\n",
    "\n",
    "        # Concatenate RNN output with feature embeddings and genre embeddings\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.concat([x, feature_transformed, genre_embedded])\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        # Feed-forward layers\n",
    "        x = self.ffn1(x)\n",
    "        x = self.activation1(x)\n",
    "        logits = self.out(x)  # Shape: (batch_size, num_items)\n",
    "\n",
    "        # Generate the sequence of items (choose the item with the highest probability using argmax)\n",
    "        predicted_items = tf.argmax(logits, axis=-1)  # (batch_size, sequence_length)\n",
    "\n",
    "        return predicted_items, logits  # Return both predicted item indices and logits (probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.metrics import Recall\n",
    "\n",
    "class RecallAtK(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k=10, name=\"recall_at_k\", **kwargs):\n",
    "        super(RecallAtK, self).__init__(name=name, **kwargs)\n",
    "        self.k = k\n",
    "        self.recall_at_k = Recall(top_k=self.k)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Update the state of the metric.\n",
    "        \"\"\"\n",
    "        # Since y_true is a list of true items and y_pred are the predicted scores,\n",
    "        # we need to calculate recall for top-k predicted items\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Calculate the top-k predicted items\n",
    "        top_k_preds = tf.argsort(y_pred, axis=-1, direction='DESCENDING')[:, :self.k]\n",
    "\n",
    "        # Calculate recall by comparing true labels with the top-k predictions\n",
    "        recall = tf.reduce_mean(tf.cast(tf.equal(y_true, top_k_preds), tf.float32), axis=-1)\n",
    "        return recall\n",
    "\n",
    "    def result(self):\n",
    "        return self.recall_at_k.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.recall_at_k.reset_state()\n",
    "\n",
    "def train_gru4rec(model, train_dataset, optimizer, loss_fn, epochs, k, val_dataset=None):\n",
    "    # Create the RecallAtK metric\n",
    "    recall_at_k = RecallAtK(k=k)\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Reset metrics for the epoch\n",
    "        # recall_at_k.reset_state()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Train the model using the dataset\n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            item_sequences = batch['item']\n",
    "            item_genres = batch['genre']\n",
    "            item_features = batch['features']\n",
    "            targets = batch['next_item']\n",
    "            # print(f\"item_sequences:\", item_sequences)\n",
    "            # print(f\"item_genres: \", item_genres)\n",
    "            # print(f\"item_features:\", item_features)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass: outputs is a tuple (predicted_items, logits)\n",
    "                predicted_items, logits = model((item_sequences, item_features, item_genres), training=True)\n",
    "                logits = tf.squeeze(logits, axis=1)\n",
    "                # print(logits)  # Log logits for debugging\n",
    "                \n",
    "                # Compute the loss using logits (not predicted_items)\n",
    "                loss = loss_fn(targets, logits)  # Use logits for loss calculation\n",
    "                print(f\"Epoch {epoch+1}, Step {step+1} Loss = {loss}\")\n",
    "                epoch_loss += loss.numpy()\n",
    "            \n",
    "            # Compute gradients and apply them\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Update Recall@k metric using logits\n",
    "            # recall_at_k.update_state(targets, logits)  # Use logits for metric calculation\n",
    "\n",
    "            # # Generate the predicted sequence (for Recall at k)\n",
    "            # input_sequence = item_sequences\n",
    "            # num_predictions = k  # Length of the sequence to predict\n",
    "\n",
    "            # predicted_sequence = []\n",
    "            # input_seq = input_sequence  # Start with the initial sequence\n",
    "\n",
    "            # for _ in range(num_predictions):\n",
    "            #     # Get the model's prediction (logits) for the next item\n",
    "            #     predicted_items, logits = model(input_seq, training=False)\n",
    "\n",
    "            #     # Add the predicted item (argmax) to the sequence\n",
    "            #     predicted_sequence.append(predicted_items)\n",
    "\n",
    "            #     # Update the input sequence by appending the predicted item\n",
    "            #     input_seq = tf.concat([input_seq, predicted_items[:, -1:]], axis=-1)  # Append the last predicted item\n",
    "\n",
    "            # predicted_sequence = tf.stack(predicted_sequence, axis=1)  # Shape: [batch_size, num_predictions]\n",
    "\n",
    "            # # Calculate recall by comparing predicted sequence with targets\n",
    "            # for batch_idx in range(predicted_sequence.shape[0]):\n",
    "            #     top_k_preds = predicted_sequence[batch_idx]  # Predicted top-k items for this batch item\n",
    "            #     true_item = targets[batch_idx]  # True next item for this batch item\n",
    "                \n",
    "            #     # Check if true item is in the top-k predictions\n",
    "            #     if true_item in top_k_preds.numpy():\n",
    "            #         recall_at_k.update_state(tf.convert_to_tensor([true_item]), predicted_sequence)  # Update recall metric\n",
    "        \n",
    "        # Calculate average loss and Recall@k for the training epoch\n",
    "        avg_loss = epoch_loss / (step + 1)\n",
    "        # train_recall_at_k = recall_at_k.result().numpy()\n",
    "        print(f\"Training loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if val_dataset is None:\n",
    "            continue\n",
    "        \n",
    "        # Validation step\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(val_dataset):\n",
    "            item_sequences = batch['item']\n",
    "            item_genres = batch['genre']\n",
    "            item_features = batch['features']\n",
    "            targets = batch['next_item']\n",
    "            # print(f\"item_sequences:\", item_sequences)\n",
    "            # print(f\"item_genres: \", item_genres)\n",
    "            # print(f\"item_features:\", item_features)\n",
    "            # Forward pass on validation set\n",
    "            predicted_items, logits = model((item_sequences, item_features, item_genres), training=False)\n",
    "            logits = tf.squeeze(logits, axis=1)\n",
    "            loss = loss_fn(targets, logits)\n",
    "            val_loss += loss.numpy()\n",
    "            \n",
    "            # # Update validation metrics\n",
    "            # recall_at_k.update_state(targets, logits)\n",
    "        \n",
    "        avg_val_loss = val_loss / (step + 1)\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "def plot_training_history(loss_history, metric_history, metric_name, top_k):\n",
    "    \"\"\"Plot the training loss and accuracy.\"\"\"\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    # Create subplots for loss and accuracy\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Plot the training loss\n",
    "    ax1.plot(epochs, loss_history, label='Loss', color='blue', linestyle='-', marker='o')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot the top-k accuracy\n",
    "    ax2.plot(epochs, metric_history, label=metric_name, color='green', linestyle='-', marker='o')\n",
    "    ax2.set_title(f'Training {metric_name}')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel(metric_name)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items size: 635\n",
      "genres size: 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# num_items = len()\n",
    "# feature_vocab_size = len(feature_columns)\n",
    "\n",
    "model = GRU4REC(\n",
    "    rnn_params=[\n",
    "        {\"units\": 128, \"dropout\": 0.3},\n",
    "        {\"units\": 64, \"dropout\": 0.3}\n",
    "    ],\n",
    "    item_embed_dim=16,\n",
    "    genre_embed_dim=16,\n",
    "    ffn1_units=128,\n",
    "    feature_dense_units=32,\n",
    "    preprocessed_data=preprocessor\n",
    ")\n",
    "\n",
    "# model = GRU4REC(\n",
    "#     rnn_params=[\n",
    "#         {\"units\": 128},\n",
    "#         {\"units\": 128},\n",
    "#         {\"units\": 64}\n",
    "#     ],\n",
    "#     item_embed_dim=32,\n",
    "#     genre_embed_dim=16,\n",
    "#     ffn1_units=128,\n",
    "#     feature_dense_units=64,\n",
    "#     preprocessed_data=preprocessor\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1381: UserWarning: Layer 'gru4rec' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: '''gru_1)' is not a valid root scope name. A root scope name has to match the following pattern: ^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$''\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'gru4rec', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1 Loss = 6.425483703613281\n",
      "Epoch 1, Step 2 Loss = 6.569718837738037\n",
      "Epoch 1, Step 3 Loss = 6.571135520935059\n",
      "Epoch 1, Step 4 Loss = 6.483668804168701\n",
      "Epoch 1, Step 5 Loss = 6.523014068603516\n",
      "Epoch 1, Step 6 Loss = 6.4677042961120605\n",
      "Epoch 1, Step 7 Loss = 6.403263092041016\n",
      "Epoch 1, Step 8 Loss = 6.519145488739014\n",
      "Epoch 1, Step 9 Loss = 6.466794967651367\n",
      "Epoch 1, Step 10 Loss = 6.403201103210449\n",
      "Epoch 1, Step 11 Loss = 6.408785820007324\n",
      "Epoch 1, Step 12 Loss = 6.4136457443237305\n",
      "Epoch 1, Step 13 Loss = 6.498261451721191\n",
      "Epoch 1, Step 14 Loss = 6.5286078453063965\n",
      "Epoch 1, Step 15 Loss = 6.523867607116699\n",
      "Epoch 1, Step 16 Loss = 6.506232261657715\n",
      "Epoch 1, Step 17 Loss = 6.59171199798584\n",
      "Epoch 1, Step 18 Loss = 6.595395088195801\n",
      "Epoch 1, Step 19 Loss = 6.615542411804199\n",
      "Epoch 1, Step 20 Loss = 6.568951606750488\n",
      "Epoch 1, Step 21 Loss = 6.4454545974731445\n",
      "Epoch 1, Step 22 Loss = 6.6328558921813965\n",
      "Epoch 1, Step 23 Loss = 6.5021257400512695\n",
      "Epoch 1, Step 24 Loss = 6.454003810882568\n",
      "Epoch 1, Step 25 Loss = 6.515568733215332\n",
      "Epoch 1, Step 26 Loss = 6.511696815490723\n",
      "Epoch 1, Step 27 Loss = 6.637048244476318\n",
      "Epoch 1, Step 28 Loss = 6.528192043304443\n",
      "Epoch 1, Step 29 Loss = 6.4818196296691895\n",
      "Epoch 1, Step 30 Loss = 6.563732624053955\n",
      "Epoch 1, Step 31 Loss = 6.542754173278809\n",
      "Epoch 1, Step 32 Loss = 6.5242390632629395\n",
      "Epoch 1, Step 33 Loss = 6.56916618347168\n",
      "Epoch 1, Step 34 Loss = 6.445756912231445\n",
      "Epoch 1, Step 35 Loss = 6.553766250610352\n",
      "Epoch 1, Step 36 Loss = 6.658884525299072\n",
      "Epoch 1, Step 37 Loss = 6.636011123657227\n",
      "Epoch 1, Step 38 Loss = 6.574582099914551\n",
      "Epoch 1, Step 39 Loss = 6.621335983276367\n",
      "Epoch 1, Step 40 Loss = 6.459820747375488\n",
      "Epoch 1, Step 41 Loss = 6.4587602615356445\n",
      "Training loss: 6.5220\n",
      "Epoch 2/50\n",
      "Epoch 2, Step 1 Loss = 6.177179336547852\n",
      "Epoch 2, Step 2 Loss = 6.217679500579834\n",
      "Epoch 2, Step 3 Loss = 6.105887413024902\n",
      "Epoch 2, Step 4 Loss = 6.183606147766113\n",
      "Epoch 2, Step 5 Loss = 6.148820877075195\n",
      "Epoch 2, Step 6 Loss = 6.12297248840332\n",
      "Epoch 2, Step 7 Loss = 6.189825057983398\n",
      "Epoch 2, Step 8 Loss = 6.023471355438232\n",
      "Epoch 2, Step 9 Loss = 5.971109390258789\n",
      "Epoch 2, Step 10 Loss = 5.965349197387695\n",
      "Epoch 2, Step 11 Loss = 6.108482360839844\n",
      "Epoch 2, Step 12 Loss = 6.060293197631836\n",
      "Epoch 2, Step 13 Loss = 5.838736534118652\n",
      "Epoch 2, Step 14 Loss = 5.995823383331299\n",
      "Epoch 2, Step 15 Loss = 6.056219100952148\n",
      "Epoch 2, Step 16 Loss = 6.0062174797058105\n",
      "Epoch 2, Step 17 Loss = 6.029858589172363\n",
      "Epoch 2, Step 18 Loss = 5.948909759521484\n",
      "Epoch 2, Step 19 Loss = 5.954117774963379\n",
      "Epoch 2, Step 20 Loss = 5.977595329284668\n",
      "Epoch 2, Step 21 Loss = 6.072651386260986\n",
      "Epoch 2, Step 22 Loss = 6.015695571899414\n",
      "Epoch 2, Step 23 Loss = 5.8982343673706055\n",
      "Epoch 2, Step 24 Loss = 5.893715858459473\n",
      "Epoch 2, Step 25 Loss = 5.938047409057617\n",
      "Epoch 2, Step 26 Loss = 5.620347499847412\n",
      "Epoch 2, Step 27 Loss = 5.841743469238281\n",
      "Epoch 2, Step 28 Loss = 6.000489234924316\n",
      "Epoch 2, Step 29 Loss = 6.0212249755859375\n",
      "Epoch 2, Step 30 Loss = 5.938815116882324\n",
      "Epoch 2, Step 31 Loss = 5.7200236320495605\n",
      "Epoch 2, Step 32 Loss = 5.849699974060059\n",
      "Epoch 2, Step 33 Loss = 5.950313091278076\n",
      "Epoch 2, Step 34 Loss = 5.761641502380371\n",
      "Epoch 2, Step 35 Loss = 5.873373031616211\n",
      "Epoch 2, Step 36 Loss = 5.859840393066406\n",
      "Epoch 2, Step 37 Loss = 5.911026954650879\n",
      "Epoch 2, Step 38 Loss = 5.930846214294434\n",
      "Epoch 2, Step 39 Loss = 5.856806755065918\n",
      "Epoch 2, Step 40 Loss = 5.738787651062012\n",
      "Epoch 2, Step 41 Loss = 6.143520355224609\n",
      "Training loss: 5.9736\n",
      "Epoch 3/50\n",
      "Epoch 3, Step 1 Loss = 5.580722808837891\n",
      "Epoch 3, Step 2 Loss = 5.48381233215332\n",
      "Epoch 3, Step 3 Loss = 5.541133403778076\n",
      "Epoch 3, Step 4 Loss = 5.62625789642334\n",
      "Epoch 3, Step 5 Loss = 5.497430801391602\n",
      "Epoch 3, Step 6 Loss = 5.618765830993652\n",
      "Epoch 3, Step 7 Loss = 5.489934921264648\n",
      "Epoch 3, Step 8 Loss = 5.387040138244629\n",
      "Epoch 3, Step 9 Loss = 5.334644317626953\n",
      "Epoch 3, Step 10 Loss = 5.419106483459473\n",
      "Epoch 3, Step 11 Loss = 5.371903419494629\n",
      "Epoch 3, Step 12 Loss = 5.462213516235352\n",
      "Epoch 3, Step 13 Loss = 5.514014720916748\n",
      "Epoch 3, Step 14 Loss = 5.302704334259033\n",
      "Epoch 3, Step 15 Loss = 5.26171875\n",
      "Epoch 3, Step 16 Loss = 5.4112324714660645\n",
      "Epoch 3, Step 17 Loss = 5.567954063415527\n",
      "Epoch 3, Step 18 Loss = 5.55686616897583\n",
      "Epoch 3, Step 19 Loss = 5.288393974304199\n",
      "Epoch 3, Step 20 Loss = 5.266399383544922\n",
      "Epoch 3, Step 21 Loss = 5.178093910217285\n",
      "Epoch 3, Step 22 Loss = 5.15110969543457\n",
      "Epoch 3, Step 23 Loss = 5.287278175354004\n",
      "Epoch 3, Step 24 Loss = 5.490897178649902\n",
      "Epoch 3, Step 25 Loss = 5.12977933883667\n",
      "Epoch 3, Step 26 Loss = 5.2148118019104\n",
      "Epoch 3, Step 27 Loss = 5.337489128112793\n",
      "Epoch 3, Step 28 Loss = 5.176506996154785\n",
      "Epoch 3, Step 29 Loss = 5.575408935546875\n",
      "Epoch 3, Step 30 Loss = 5.3369460105896\n",
      "Epoch 3, Step 31 Loss = 5.027225494384766\n",
      "Epoch 3, Step 32 Loss = 5.402499198913574\n",
      "Epoch 3, Step 33 Loss = 5.1688690185546875\n",
      "Epoch 3, Step 34 Loss = 5.226648807525635\n",
      "Epoch 3, Step 35 Loss = 5.2726335525512695\n",
      "Epoch 3, Step 36 Loss = 5.467578411102295\n",
      "Epoch 3, Step 37 Loss = 5.149919509887695\n",
      "Epoch 3, Step 38 Loss = 5.17131233215332\n",
      "Epoch 3, Step 39 Loss = 5.5003767013549805\n",
      "Epoch 3, Step 40 Loss = 5.2349090576171875\n",
      "Epoch 3, Step 41 Loss = 5.31276798248291\n",
      "Training loss: 5.3609\n",
      "Epoch 4/50\n",
      "Epoch 4, Step 1 Loss = 4.687941074371338\n",
      "Epoch 4, Step 2 Loss = 4.812714576721191\n",
      "Epoch 4, Step 3 Loss = 4.910275936126709\n",
      "Epoch 4, Step 4 Loss = 4.9122209548950195\n",
      "Epoch 4, Step 5 Loss = 4.691168785095215\n",
      "Epoch 4, Step 6 Loss = 4.9454264640808105\n",
      "Epoch 4, Step 7 Loss = 4.365732669830322\n",
      "Epoch 4, Step 8 Loss = 4.830074310302734\n",
      "Epoch 4, Step 9 Loss = 4.789150238037109\n",
      "Epoch 4, Step 10 Loss = 4.233921051025391\n",
      "Epoch 4, Step 11 Loss = 4.634232044219971\n",
      "Epoch 4, Step 12 Loss = 4.363041877746582\n",
      "Epoch 4, Step 13 Loss = 4.466707229614258\n",
      "Epoch 4, Step 14 Loss = 4.614407539367676\n",
      "Epoch 4, Step 15 Loss = 4.593133449554443\n",
      "Epoch 4, Step 16 Loss = 4.656096458435059\n",
      "Epoch 4, Step 17 Loss = 4.450329303741455\n",
      "Epoch 4, Step 18 Loss = 4.437397003173828\n",
      "Epoch 4, Step 19 Loss = 4.815513610839844\n",
      "Epoch 4, Step 20 Loss = 4.475083827972412\n",
      "Epoch 4, Step 21 Loss = 4.528456687927246\n",
      "Epoch 4, Step 22 Loss = 4.5582275390625\n",
      "Epoch 4, Step 23 Loss = 4.453864097595215\n",
      "Epoch 4, Step 24 Loss = 4.675571441650391\n",
      "Epoch 4, Step 25 Loss = 4.242621421813965\n",
      "Epoch 4, Step 26 Loss = 4.897268295288086\n",
      "Epoch 4, Step 27 Loss = 4.359323501586914\n",
      "Epoch 4, Step 28 Loss = 4.2207489013671875\n",
      "Epoch 4, Step 29 Loss = 4.69146728515625\n",
      "Epoch 4, Step 30 Loss = 4.550751686096191\n",
      "Epoch 4, Step 31 Loss = 4.4340105056762695\n",
      "Epoch 4, Step 32 Loss = 4.693392276763916\n",
      "Epoch 4, Step 33 Loss = 4.716360569000244\n",
      "Epoch 4, Step 34 Loss = 4.66943359375\n",
      "Epoch 4, Step 35 Loss = 4.933718681335449\n",
      "Epoch 4, Step 36 Loss = 5.139084815979004\n",
      "Epoch 4, Step 37 Loss = 4.340610504150391\n",
      "Epoch 4, Step 38 Loss = 4.421893119812012\n",
      "Epoch 4, Step 39 Loss = 4.35651159286499\n",
      "Epoch 4, Step 40 Loss = 4.4812445640563965\n",
      "Epoch 4, Step 41 Loss = 4.287163734436035\n",
      "Training loss: 4.5936\n",
      "Epoch 5/50\n",
      "Epoch 5, Step 1 Loss = 3.8407695293426514\n",
      "Epoch 5, Step 2 Loss = 3.717843532562256\n",
      "Epoch 5, Step 3 Loss = 4.111092567443848\n",
      "Epoch 5, Step 4 Loss = 3.6741600036621094\n",
      "Epoch 5, Step 5 Loss = 3.7181529998779297\n",
      "Epoch 5, Step 6 Loss = 3.666308879852295\n",
      "Epoch 5, Step 7 Loss = 3.688347101211548\n",
      "Epoch 5, Step 8 Loss = 3.8879523277282715\n",
      "Epoch 5, Step 9 Loss = 3.6196582317352295\n",
      "Epoch 5, Step 10 Loss = 3.8482980728149414\n",
      "Epoch 5, Step 11 Loss = 3.7940673828125\n",
      "Epoch 5, Step 12 Loss = 3.521547794342041\n",
      "Epoch 5, Step 13 Loss = 3.7505993843078613\n",
      "Epoch 5, Step 14 Loss = 4.0306925773620605\n",
      "Epoch 5, Step 15 Loss = 3.572841167449951\n",
      "Epoch 5, Step 16 Loss = 4.033883094787598\n",
      "Epoch 5, Step 17 Loss = 3.5632519721984863\n",
      "Epoch 5, Step 18 Loss = 3.473450183868408\n",
      "Epoch 5, Step 19 Loss = 3.621990203857422\n",
      "Epoch 5, Step 20 Loss = 3.5837903022766113\n",
      "Epoch 5, Step 21 Loss = 3.785766124725342\n",
      "Epoch 5, Step 22 Loss = 3.8822007179260254\n",
      "Epoch 5, Step 23 Loss = 3.597721815109253\n",
      "Epoch 5, Step 24 Loss = 4.030158996582031\n",
      "Epoch 5, Step 25 Loss = 3.8257088661193848\n",
      "Epoch 5, Step 26 Loss = 3.4682376384735107\n",
      "Epoch 5, Step 27 Loss = 3.460104465484619\n",
      "Epoch 5, Step 28 Loss = 3.6714303493499756\n",
      "Epoch 5, Step 29 Loss = 3.9481699466705322\n",
      "Epoch 5, Step 30 Loss = 3.8765952587127686\n",
      "Epoch 5, Step 31 Loss = 3.8911452293395996\n",
      "Epoch 5, Step 32 Loss = 3.514519691467285\n",
      "Epoch 5, Step 33 Loss = 3.708751916885376\n",
      "Epoch 5, Step 34 Loss = 3.8140709400177\n",
      "Epoch 5, Step 35 Loss = 3.763512134552002\n",
      "Epoch 5, Step 36 Loss = 3.892148017883301\n",
      "Epoch 5, Step 37 Loss = 3.7182812690734863\n",
      "Epoch 5, Step 38 Loss = 3.823594570159912\n",
      "Epoch 5, Step 39 Loss = 4.107199192047119\n",
      "Epoch 5, Step 40 Loss = 3.7700002193450928\n",
      "Epoch 5, Step 41 Loss = 3.8215603828430176\n",
      "Training loss: 3.7583\n",
      "Epoch 6/50\n",
      "Epoch 6, Step 1 Loss = 3.4748008251190186\n",
      "Epoch 6, Step 2 Loss = 2.9081921577453613\n",
      "Epoch 6, Step 3 Loss = 2.7635045051574707\n",
      "Epoch 6, Step 4 Loss = 2.472306489944458\n",
      "Epoch 6, Step 5 Loss = 3.295785903930664\n",
      "Epoch 6, Step 6 Loss = 2.9484663009643555\n",
      "Epoch 6, Step 7 Loss = 2.8003411293029785\n",
      "Epoch 6, Step 8 Loss = 2.6644022464752197\n",
      "Epoch 6, Step 9 Loss = 2.7958145141601562\n",
      "Epoch 6, Step 10 Loss = 3.0027670860290527\n",
      "Epoch 6, Step 11 Loss = 2.7429962158203125\n",
      "Epoch 6, Step 12 Loss = 2.77671480178833\n",
      "Epoch 6, Step 13 Loss = 2.821791410446167\n",
      "Epoch 6, Step 14 Loss = 2.695852756500244\n",
      "Epoch 6, Step 15 Loss = 2.835033416748047\n",
      "Epoch 6, Step 16 Loss = 2.5305228233337402\n",
      "Epoch 6, Step 17 Loss = 2.5360867977142334\n",
      "Epoch 6, Step 18 Loss = 2.9654226303100586\n",
      "Epoch 6, Step 19 Loss = 3.7767586708068848\n",
      "Epoch 6, Step 20 Loss = 2.6760666370391846\n",
      "Epoch 6, Step 21 Loss = 2.764902114868164\n",
      "Epoch 6, Step 22 Loss = 3.0129270553588867\n",
      "Epoch 6, Step 23 Loss = 2.7998085021972656\n",
      "Epoch 6, Step 24 Loss = 2.7496109008789062\n",
      "Epoch 6, Step 25 Loss = 3.003488540649414\n",
      "Epoch 6, Step 26 Loss = 2.950024127960205\n",
      "Epoch 6, Step 27 Loss = 2.840040445327759\n",
      "Epoch 6, Step 28 Loss = 2.8647310733795166\n",
      "Epoch 6, Step 29 Loss = 2.525892734527588\n",
      "Epoch 6, Step 30 Loss = 2.4967100620269775\n",
      "Epoch 6, Step 31 Loss = 2.5413455963134766\n",
      "Epoch 6, Step 32 Loss = 2.8850977420806885\n",
      "Epoch 6, Step 33 Loss = 3.0576577186584473\n",
      "Epoch 6, Step 34 Loss = 2.7010111808776855\n",
      "Epoch 6, Step 35 Loss = 2.88082218170166\n",
      "Epoch 6, Step 36 Loss = 2.9245662689208984\n",
      "Epoch 6, Step 37 Loss = 2.7604432106018066\n",
      "Epoch 6, Step 38 Loss = 3.125720262527466\n",
      "Epoch 6, Step 39 Loss = 3.223294973373413\n",
      "Epoch 6, Step 40 Loss = 2.5580005645751953\n",
      "Epoch 6, Step 41 Loss = 2.5382485389709473\n",
      "Training loss: 2.8460\n",
      "Epoch 7/50\n",
      "Epoch 7, Step 1 Loss = 2.1209704875946045\n",
      "Epoch 7, Step 2 Loss = 2.5379223823547363\n",
      "Epoch 7, Step 3 Loss = 2.4095258712768555\n",
      "Epoch 7, Step 4 Loss = 1.675389051437378\n",
      "Epoch 7, Step 5 Loss = 2.2378106117248535\n",
      "Epoch 7, Step 6 Loss = 2.0209412574768066\n",
      "Epoch 7, Step 7 Loss = 1.960405945777893\n",
      "Epoch 7, Step 8 Loss = 2.136162757873535\n",
      "Epoch 7, Step 9 Loss = 1.9035329818725586\n",
      "Epoch 7, Step 10 Loss = 1.6598162651062012\n",
      "Epoch 7, Step 11 Loss = 2.341599941253662\n",
      "Epoch 7, Step 12 Loss = 1.6264185905456543\n",
      "Epoch 7, Step 13 Loss = 1.8645069599151611\n",
      "Epoch 7, Step 14 Loss = 2.2136340141296387\n",
      "Epoch 7, Step 15 Loss = 2.031625270843506\n",
      "Epoch 7, Step 16 Loss = 2.0255985260009766\n",
      "Epoch 7, Step 17 Loss = 2.0652332305908203\n",
      "Epoch 7, Step 18 Loss = 2.047106981277466\n",
      "Epoch 7, Step 19 Loss = 1.7661464214324951\n",
      "Epoch 7, Step 20 Loss = 2.443152666091919\n",
      "Epoch 7, Step 21 Loss = 2.318528652191162\n",
      "Epoch 7, Step 22 Loss = 2.2254416942596436\n",
      "Epoch 7, Step 23 Loss = 2.1634199619293213\n",
      "Epoch 7, Step 24 Loss = 2.3474411964416504\n",
      "Epoch 7, Step 25 Loss = 1.821929931640625\n",
      "Epoch 7, Step 26 Loss = 2.163892984390259\n",
      "Epoch 7, Step 27 Loss = 2.5317633152008057\n",
      "Epoch 7, Step 28 Loss = 2.1162915229797363\n",
      "Epoch 7, Step 29 Loss = 2.855715751647949\n",
      "Epoch 7, Step 30 Loss = 1.992331624031067\n",
      "Epoch 7, Step 31 Loss = 2.1042253971099854\n",
      "Epoch 7, Step 32 Loss = 1.8491321802139282\n",
      "Epoch 7, Step 33 Loss = 2.2708849906921387\n",
      "Epoch 7, Step 34 Loss = 1.719030737876892\n",
      "Epoch 7, Step 35 Loss = 2.0054097175598145\n",
      "Epoch 7, Step 36 Loss = 1.9935470819473267\n",
      "Epoch 7, Step 37 Loss = 2.2887990474700928\n",
      "Epoch 7, Step 38 Loss = 2.2585878372192383\n",
      "Epoch 7, Step 39 Loss = 1.8690228462219238\n",
      "Epoch 7, Step 40 Loss = 2.244814157485962\n",
      "Epoch 7, Step 41 Loss = 2.2061922550201416\n",
      "Training loss: 2.1081\n",
      "Epoch 8/50\n",
      "Epoch 8, Step 1 Loss = 1.4555461406707764\n",
      "Epoch 8, Step 2 Loss = 1.2325382232666016\n",
      "Epoch 8, Step 3 Loss = 1.309069037437439\n",
      "Epoch 8, Step 4 Loss = 1.4902567863464355\n",
      "Epoch 8, Step 5 Loss = 1.4599542617797852\n",
      "Epoch 8, Step 6 Loss = 1.1759202480316162\n",
      "Epoch 8, Step 7 Loss = 1.849876880645752\n",
      "Epoch 8, Step 8 Loss = 1.3772451877593994\n",
      "Epoch 8, Step 9 Loss = 1.3054389953613281\n",
      "Epoch 8, Step 10 Loss = 1.8544257879257202\n",
      "Epoch 8, Step 11 Loss = 1.5735846757888794\n",
      "Epoch 8, Step 12 Loss = 1.5806094408035278\n",
      "Epoch 8, Step 13 Loss = 1.600611686706543\n",
      "Epoch 8, Step 14 Loss = 1.3100926876068115\n",
      "Epoch 8, Step 15 Loss = 1.3416913747787476\n",
      "Epoch 8, Step 16 Loss = 1.6935007572174072\n",
      "Epoch 8, Step 17 Loss = 1.6352185010910034\n",
      "Epoch 8, Step 18 Loss = 1.5935451984405518\n",
      "Epoch 8, Step 19 Loss = 1.5236546993255615\n",
      "Epoch 8, Step 20 Loss = 1.3388521671295166\n",
      "Epoch 8, Step 21 Loss = 1.6370494365692139\n",
      "Epoch 8, Step 22 Loss = 1.6612756252288818\n",
      "Epoch 8, Step 23 Loss = 1.5331242084503174\n",
      "Epoch 8, Step 24 Loss = 1.7292816638946533\n",
      "Epoch 8, Step 25 Loss = 1.2550075054168701\n",
      "Epoch 8, Step 26 Loss = 1.8269282579421997\n",
      "Epoch 8, Step 27 Loss = 1.4147789478302002\n",
      "Epoch 8, Step 28 Loss = 1.629219651222229\n",
      "Epoch 8, Step 29 Loss = 1.4386043548583984\n",
      "Epoch 8, Step 30 Loss = 1.6481142044067383\n",
      "Epoch 8, Step 31 Loss = 1.978520154953003\n",
      "Epoch 8, Step 32 Loss = 1.6818302869796753\n",
      "Epoch 8, Step 33 Loss = 2.3170084953308105\n",
      "Epoch 8, Step 34 Loss = 1.747147798538208\n",
      "Epoch 8, Step 35 Loss = 1.6743438243865967\n",
      "Epoch 8, Step 36 Loss = 1.5681819915771484\n",
      "Epoch 8, Step 37 Loss = 1.8474864959716797\n",
      "Epoch 8, Step 38 Loss = 1.91981041431427\n",
      "Epoch 8, Step 39 Loss = 2.0726399421691895\n",
      "Epoch 8, Step 40 Loss = 1.9438170194625854\n",
      "Epoch 8, Step 41 Loss = 2.3426222801208496\n",
      "Training loss: 1.6236\n",
      "Epoch 9/50\n",
      "Epoch 9, Step 1 Loss = 1.275696039199829\n",
      "Epoch 9, Step 2 Loss = 1.0992761850357056\n",
      "Epoch 9, Step 3 Loss = 1.5431667566299438\n",
      "Epoch 9, Step 4 Loss = 0.9183670282363892\n",
      "Epoch 9, Step 5 Loss = 0.8184390068054199\n",
      "Epoch 9, Step 6 Loss = 1.3304667472839355\n",
      "Epoch 9, Step 7 Loss = 0.897975504398346\n",
      "Epoch 9, Step 8 Loss = 1.1459355354309082\n",
      "Epoch 9, Step 9 Loss = 1.438381314277649\n",
      "Epoch 9, Step 10 Loss = 1.0645737648010254\n",
      "Epoch 9, Step 11 Loss = 1.3491363525390625\n",
      "Epoch 9, Step 12 Loss = 1.1364060640335083\n",
      "Epoch 9, Step 13 Loss = 1.0865511894226074\n",
      "Epoch 9, Step 14 Loss = 1.3332750797271729\n",
      "Epoch 9, Step 15 Loss = 0.9631143808364868\n",
      "Epoch 9, Step 16 Loss = 1.3366464376449585\n",
      "Epoch 9, Step 17 Loss = 1.7700207233428955\n",
      "Epoch 9, Step 18 Loss = 1.345045804977417\n",
      "Epoch 9, Step 19 Loss = 1.4446020126342773\n",
      "Epoch 9, Step 20 Loss = 0.9314931631088257\n",
      "Epoch 9, Step 21 Loss = 1.2002835273742676\n",
      "Epoch 9, Step 22 Loss = 1.4740923643112183\n",
      "Epoch 9, Step 23 Loss = 1.2935287952423096\n",
      "Epoch 9, Step 24 Loss = 1.163067102432251\n",
      "Epoch 9, Step 25 Loss = 1.0631353855133057\n",
      "Epoch 9, Step 26 Loss = 1.4188830852508545\n",
      "Epoch 9, Step 27 Loss = 1.6867668628692627\n",
      "Epoch 9, Step 28 Loss = 1.57766592502594\n",
      "Epoch 9, Step 29 Loss = 1.715458869934082\n",
      "Epoch 9, Step 30 Loss = 1.3177738189697266\n",
      "Epoch 9, Step 31 Loss = 1.1993815898895264\n",
      "Epoch 9, Step 32 Loss = 1.3744150400161743\n",
      "Epoch 9, Step 33 Loss = 1.7205018997192383\n",
      "Epoch 9, Step 34 Loss = 1.8809889554977417\n",
      "Epoch 9, Step 35 Loss = 1.289209246635437\n",
      "Epoch 9, Step 36 Loss = 1.0882166624069214\n",
      "Epoch 9, Step 37 Loss = 1.6678986549377441\n",
      "Epoch 9, Step 38 Loss = 1.8323577642440796\n",
      "Epoch 9, Step 39 Loss = 1.1222655773162842\n",
      "Epoch 9, Step 40 Loss = 1.4609487056732178\n",
      "Epoch 9, Step 41 Loss = 1.1080639362335205\n",
      "Training loss: 1.3142\n",
      "Epoch 10/50\n",
      "Epoch 10, Step 1 Loss = 1.2786494493484497\n",
      "Epoch 10, Step 2 Loss = 1.0970027446746826\n",
      "Epoch 10, Step 3 Loss = 0.8124555945396423\n",
      "Epoch 10, Step 4 Loss = 0.8445950746536255\n",
      "Epoch 10, Step 5 Loss = 0.9385128021240234\n",
      "Epoch 10, Step 6 Loss = 0.9971009492874146\n",
      "Epoch 10, Step 7 Loss = 0.8469245433807373\n",
      "Epoch 10, Step 8 Loss = 0.9579296708106995\n",
      "Epoch 10, Step 9 Loss = 0.8635643720626831\n",
      "Epoch 10, Step 10 Loss = 1.049793004989624\n",
      "Epoch 10, Step 11 Loss = 0.8209973573684692\n",
      "Epoch 10, Step 12 Loss = 1.18941330909729\n",
      "Epoch 10, Step 13 Loss = 1.3186014890670776\n",
      "Epoch 10, Step 14 Loss = 1.187070608139038\n",
      "Epoch 10, Step 15 Loss = 1.1127506494522095\n",
      "Epoch 10, Step 16 Loss = 0.8859661817550659\n",
      "Epoch 10, Step 17 Loss = 1.2814855575561523\n",
      "Epoch 10, Step 18 Loss = 1.132091760635376\n",
      "Epoch 10, Step 19 Loss = 0.8025225400924683\n",
      "Epoch 10, Step 20 Loss = 0.954023003578186\n",
      "Epoch 10, Step 21 Loss = 0.7678282856941223\n",
      "Epoch 10, Step 22 Loss = 0.7965063452720642\n",
      "Epoch 10, Step 23 Loss = 1.0004370212554932\n",
      "Epoch 10, Step 24 Loss = 1.4336848258972168\n",
      "Epoch 10, Step 25 Loss = 1.0825684070587158\n",
      "Epoch 10, Step 26 Loss = 1.039428472518921\n",
      "Epoch 10, Step 27 Loss = 0.8122011423110962\n",
      "Epoch 10, Step 28 Loss = 1.4011372327804565\n",
      "Epoch 10, Step 29 Loss = 1.7064372301101685\n",
      "Epoch 10, Step 30 Loss = 1.7054377794265747\n",
      "Epoch 10, Step 31 Loss = 1.3327546119689941\n",
      "Epoch 10, Step 32 Loss = 0.9772325754165649\n",
      "Epoch 10, Step 33 Loss = 1.3577723503112793\n",
      "Epoch 10, Step 34 Loss = 1.2499263286590576\n",
      "Epoch 10, Step 35 Loss = 1.3879950046539307\n",
      "Epoch 10, Step 36 Loss = 1.3350341320037842\n",
      "Epoch 10, Step 37 Loss = 1.5494048595428467\n",
      "Epoch 10, Step 38 Loss = 1.3711583614349365\n",
      "Epoch 10, Step 39 Loss = 1.3924602270126343\n",
      "Epoch 10, Step 40 Loss = 1.182005524635315\n",
      "Epoch 10, Step 41 Loss = 1.6054506301879883\n",
      "Training loss: 1.1429\n",
      "Epoch 11/50\n",
      "Epoch 11, Step 1 Loss = 0.8833168745040894\n",
      "Epoch 11, Step 2 Loss = 0.7040802240371704\n",
      "Epoch 11, Step 3 Loss = 0.8683988451957703\n",
      "Epoch 11, Step 4 Loss = 1.1424994468688965\n",
      "Epoch 11, Step 5 Loss = 0.7768316864967346\n",
      "Epoch 11, Step 6 Loss = 0.8412127494812012\n",
      "Epoch 11, Step 7 Loss = 0.6859012842178345\n",
      "Epoch 11, Step 8 Loss = 0.7745251655578613\n",
      "Epoch 11, Step 9 Loss = 0.7042658925056458\n",
      "Epoch 11, Step 10 Loss = 1.174585223197937\n",
      "Epoch 11, Step 11 Loss = 0.660727858543396\n",
      "Epoch 11, Step 12 Loss = 0.7709975242614746\n",
      "Epoch 11, Step 13 Loss = 0.6935734152793884\n",
      "Epoch 11, Step 14 Loss = 0.6723505854606628\n",
      "Epoch 11, Step 15 Loss = 1.1052260398864746\n",
      "Epoch 11, Step 16 Loss = 0.8329979181289673\n",
      "Epoch 11, Step 17 Loss = 0.802127480506897\n",
      "Epoch 11, Step 18 Loss = 1.1975504159927368\n",
      "Epoch 11, Step 19 Loss = 0.4679452180862427\n",
      "Epoch 11, Step 20 Loss = 0.7303410172462463\n",
      "Epoch 11, Step 21 Loss = 1.0736725330352783\n",
      "Epoch 11, Step 22 Loss = 1.3681522607803345\n",
      "Epoch 11, Step 23 Loss = 0.6214661002159119\n",
      "Epoch 11, Step 24 Loss = 1.2277910709381104\n",
      "Epoch 11, Step 25 Loss = 0.8565479516983032\n",
      "Epoch 11, Step 26 Loss = 0.9164059162139893\n",
      "Epoch 11, Step 27 Loss = 1.3888835906982422\n",
      "Epoch 11, Step 28 Loss = 1.3268988132476807\n",
      "Epoch 11, Step 29 Loss = 1.3779525756835938\n",
      "Epoch 11, Step 30 Loss = 1.0845305919647217\n",
      "Epoch 11, Step 31 Loss = 0.9048800468444824\n",
      "Epoch 11, Step 32 Loss = 1.5227241516113281\n",
      "Epoch 11, Step 33 Loss = 1.9441078901290894\n",
      "Epoch 11, Step 34 Loss = 0.95216304063797\n",
      "Epoch 11, Step 35 Loss = 1.060166597366333\n",
      "Epoch 11, Step 36 Loss = 1.0124878883361816\n",
      "Epoch 11, Step 37 Loss = 1.2492694854736328\n",
      "Epoch 11, Step 38 Loss = 0.9656162261962891\n",
      "Epoch 11, Step 39 Loss = 0.8836604356765747\n",
      "Epoch 11, Step 40 Loss = 1.3288871049880981\n",
      "Epoch 11, Step 41 Loss = 1.7919001579284668\n",
      "Training loss: 1.0085\n",
      "Epoch 12/50\n",
      "Epoch 12, Step 1 Loss = 0.6905553340911865\n",
      "Epoch 12, Step 2 Loss = 0.8264514803886414\n",
      "Epoch 12, Step 3 Loss = 0.9980895519256592\n",
      "Epoch 12, Step 4 Loss = 0.6349456310272217\n",
      "Epoch 12, Step 5 Loss = 0.6126190423965454\n",
      "Epoch 12, Step 6 Loss = 0.7739459276199341\n",
      "Epoch 12, Step 7 Loss = 0.6055166721343994\n",
      "Epoch 12, Step 8 Loss = 0.7434406280517578\n",
      "Epoch 12, Step 9 Loss = 1.0319390296936035\n",
      "Epoch 12, Step 10 Loss = 0.6544758677482605\n",
      "Epoch 12, Step 11 Loss = 0.9770196676254272\n",
      "Epoch 12, Step 12 Loss = 1.2442126274108887\n",
      "Epoch 12, Step 13 Loss = 0.6621362566947937\n",
      "Epoch 12, Step 14 Loss = 0.8896538615226746\n",
      "Epoch 12, Step 15 Loss = 1.1823110580444336\n",
      "Epoch 12, Step 16 Loss = 0.5711265206336975\n",
      "Epoch 12, Step 17 Loss = 1.4042941331863403\n",
      "Epoch 12, Step 18 Loss = 0.9547690749168396\n",
      "Epoch 12, Step 19 Loss = 0.4862843453884125\n",
      "Epoch 12, Step 20 Loss = 0.7859623432159424\n",
      "Epoch 12, Step 21 Loss = 1.1619021892547607\n",
      "Epoch 12, Step 22 Loss = 1.2137408256530762\n",
      "Epoch 12, Step 23 Loss = 0.915061354637146\n",
      "Epoch 12, Step 24 Loss = 0.633043110370636\n",
      "Epoch 12, Step 25 Loss = 0.5757829546928406\n",
      "Epoch 12, Step 26 Loss = 0.6655729413032532\n",
      "Epoch 12, Step 27 Loss = 1.4028655290603638\n",
      "Epoch 12, Step 28 Loss = 0.503163754940033\n",
      "Epoch 12, Step 29 Loss = 0.9873055219650269\n",
      "Epoch 12, Step 30 Loss = 0.9280576109886169\n",
      "Epoch 12, Step 31 Loss = 1.650102138519287\n",
      "Epoch 12, Step 32 Loss = 1.379337191581726\n",
      "Epoch 12, Step 33 Loss = 1.3021464347839355\n",
      "Epoch 12, Step 34 Loss = 1.1951348781585693\n",
      "Epoch 12, Step 35 Loss = 0.9990324974060059\n",
      "Epoch 12, Step 36 Loss = 1.2983849048614502\n",
      "Epoch 12, Step 37 Loss = 1.0890222787857056\n",
      "Epoch 12, Step 38 Loss = 1.9124250411987305\n",
      "Epoch 12, Step 39 Loss = 0.9483280181884766\n",
      "Epoch 12, Step 40 Loss = 1.190354585647583\n",
      "Epoch 12, Step 41 Loss = 0.6089842915534973\n",
      "Training loss: 0.9583\n",
      "Epoch 13/50\n",
      "Epoch 13, Step 1 Loss = 0.721106231212616\n",
      "Epoch 13, Step 2 Loss = 0.7948713898658752\n",
      "Epoch 13, Step 3 Loss = 0.8091636896133423\n",
      "Epoch 13, Step 4 Loss = 0.533552885055542\n",
      "Epoch 13, Step 5 Loss = 0.6933048963546753\n",
      "Epoch 13, Step 6 Loss = 0.6318694353103638\n",
      "Epoch 13, Step 7 Loss = 0.538088858127594\n",
      "Epoch 13, Step 8 Loss = 0.5142487287521362\n",
      "Epoch 13, Step 9 Loss = 0.8160521984100342\n",
      "Epoch 13, Step 10 Loss = 0.7672345638275146\n",
      "Epoch 13, Step 11 Loss = 0.7702573537826538\n",
      "Epoch 13, Step 12 Loss = 0.39963003993034363\n",
      "Epoch 13, Step 13 Loss = 0.25191885232925415\n",
      "Epoch 13, Step 14 Loss = 0.658500611782074\n",
      "Epoch 13, Step 15 Loss = 0.8555717468261719\n",
      "Epoch 13, Step 16 Loss = 0.7768698334693909\n",
      "Epoch 13, Step 17 Loss = 1.0074489116668701\n",
      "Epoch 13, Step 18 Loss = 0.9497573375701904\n",
      "Epoch 13, Step 19 Loss = 0.7684075832366943\n",
      "Epoch 13, Step 20 Loss = 0.5135617256164551\n",
      "Epoch 13, Step 21 Loss = 0.5884933471679688\n",
      "Epoch 13, Step 22 Loss = 0.9674568176269531\n",
      "Epoch 13, Step 23 Loss = 0.7378652095794678\n",
      "Epoch 13, Step 24 Loss = 1.1157300472259521\n",
      "Epoch 13, Step 25 Loss = 0.7045073509216309\n",
      "Epoch 13, Step 26 Loss = 0.8173147439956665\n",
      "Epoch 13, Step 27 Loss = 1.0719406604766846\n",
      "Epoch 13, Step 28 Loss = 0.7037149667739868\n",
      "Epoch 13, Step 29 Loss = 1.157842993736267\n",
      "Epoch 13, Step 30 Loss = 0.984200119972229\n",
      "Epoch 13, Step 31 Loss = 1.196934461593628\n",
      "Epoch 13, Step 32 Loss = 0.8413265347480774\n",
      "Epoch 13, Step 33 Loss = 1.465179443359375\n",
      "Epoch 13, Step 34 Loss = 1.4090386629104614\n",
      "Epoch 13, Step 35 Loss = 1.4444403648376465\n",
      "Epoch 13, Step 36 Loss = 1.346003532409668\n",
      "Epoch 13, Step 37 Loss = 0.9431290626525879\n",
      "Epoch 13, Step 38 Loss = 1.731429100036621\n",
      "Epoch 13, Step 39 Loss = 2.0522375106811523\n",
      "Epoch 13, Step 40 Loss = 1.1543903350830078\n",
      "Epoch 13, Step 41 Loss = 0.9867773056030273\n",
      "Training loss: 0.9071\n",
      "Epoch 14/50\n",
      "Epoch 14, Step 1 Loss = 0.6894287467002869\n",
      "Epoch 14, Step 2 Loss = 0.5848823189735413\n",
      "Epoch 14, Step 3 Loss = 0.8858870267868042\n",
      "Epoch 14, Step 4 Loss = 1.226919174194336\n",
      "Epoch 14, Step 5 Loss = 0.5590106248855591\n",
      "Epoch 14, Step 6 Loss = 0.6228777170181274\n",
      "Epoch 14, Step 7 Loss = 0.5450167059898376\n",
      "Epoch 14, Step 8 Loss = 0.4570848345756531\n",
      "Epoch 14, Step 9 Loss = 0.6618631482124329\n",
      "Epoch 14, Step 10 Loss = 0.9651984572410583\n",
      "Epoch 14, Step 11 Loss = 0.7708029747009277\n",
      "Epoch 14, Step 12 Loss = 0.8574960231781006\n",
      "Epoch 14, Step 13 Loss = 0.6321160793304443\n",
      "Epoch 14, Step 14 Loss = 0.8798708319664001\n",
      "Epoch 14, Step 15 Loss = 0.5203539133071899\n",
      "Epoch 14, Step 16 Loss = 0.7837821245193481\n",
      "Epoch 14, Step 17 Loss = 0.7879838943481445\n",
      "Epoch 14, Step 18 Loss = 0.9957171678543091\n",
      "Epoch 14, Step 19 Loss = 1.0829473733901978\n",
      "Epoch 14, Step 20 Loss = 0.8418115377426147\n",
      "Epoch 14, Step 21 Loss = 0.89549720287323\n",
      "Epoch 14, Step 22 Loss = 1.206782579421997\n",
      "Epoch 14, Step 23 Loss = 1.051324725151062\n",
      "Epoch 14, Step 24 Loss = 0.9158727526664734\n",
      "Epoch 14, Step 25 Loss = 0.8799557685852051\n",
      "Epoch 14, Step 26 Loss = 1.1047916412353516\n",
      "Epoch 14, Step 27 Loss = 0.8856791257858276\n",
      "Epoch 14, Step 28 Loss = 0.6360663771629333\n",
      "Epoch 14, Step 29 Loss = 1.0678822994232178\n",
      "Epoch 14, Step 30 Loss = 0.7927465438842773\n",
      "Epoch 14, Step 31 Loss = 1.3454543352127075\n",
      "Epoch 14, Step 32 Loss = 0.6245653629302979\n",
      "Epoch 14, Step 33 Loss = 1.0978143215179443\n",
      "Epoch 14, Step 34 Loss = 1.0885138511657715\n",
      "Epoch 14, Step 35 Loss = 1.1158795356750488\n",
      "Epoch 14, Step 36 Loss = 0.7959380149841309\n",
      "Epoch 14, Step 37 Loss = 0.6020033955574036\n",
      "Epoch 14, Step 38 Loss = 1.1262515783309937\n",
      "Epoch 14, Step 39 Loss = 1.236322283744812\n",
      "Epoch 14, Step 40 Loss = 0.8842058181762695\n",
      "Epoch 14, Step 41 Loss = 1.2073701620101929\n",
      "Training loss: 0.8759\n",
      "Epoch 15/50\n",
      "Epoch 15, Step 1 Loss = 0.5491656064987183\n",
      "Epoch 15, Step 2 Loss = 0.651126503944397\n",
      "Epoch 15, Step 3 Loss = 0.5561791658401489\n",
      "Epoch 15, Step 4 Loss = 0.9627397656440735\n",
      "Epoch 15, Step 5 Loss = 0.9224632978439331\n",
      "Epoch 15, Step 6 Loss = 0.6519650220870972\n",
      "Epoch 15, Step 7 Loss = 1.1719651222229004\n",
      "Epoch 15, Step 8 Loss = 0.6848304867744446\n",
      "Epoch 15, Step 9 Loss = 1.0693714618682861\n",
      "Epoch 15, Step 10 Loss = 0.9286853075027466\n",
      "Epoch 15, Step 11 Loss = 0.6228381395339966\n",
      "Epoch 15, Step 12 Loss = 0.8315264582633972\n",
      "Epoch 15, Step 13 Loss = 0.7366898655891418\n",
      "Epoch 15, Step 14 Loss = 0.9946259260177612\n",
      "Epoch 15, Step 15 Loss = 0.5350999236106873\n",
      "Epoch 15, Step 16 Loss = 0.9436584711074829\n",
      "Epoch 15, Step 17 Loss = 1.0492477416992188\n",
      "Epoch 15, Step 18 Loss = 1.2106374502182007\n",
      "Epoch 15, Step 19 Loss = 0.7675784826278687\n",
      "Epoch 15, Step 20 Loss = 0.8370496034622192\n",
      "Epoch 15, Step 21 Loss = 1.2055392265319824\n",
      "Epoch 15, Step 22 Loss = 0.5349675416946411\n",
      "Epoch 15, Step 23 Loss = 0.7162810564041138\n",
      "Epoch 15, Step 24 Loss = 0.7971168756484985\n",
      "Epoch 15, Step 25 Loss = 0.7144044637680054\n",
      "Epoch 15, Step 26 Loss = 0.7211403250694275\n",
      "Epoch 15, Step 27 Loss = 0.9083045721054077\n",
      "Epoch 15, Step 28 Loss = 1.3001514673233032\n",
      "Epoch 15, Step 29 Loss = 1.2850806713104248\n",
      "Epoch 15, Step 30 Loss = 1.396942138671875\n",
      "Epoch 15, Step 31 Loss = 0.7378858327865601\n",
      "Epoch 15, Step 32 Loss = 0.7899701595306396\n",
      "Epoch 15, Step 33 Loss = 0.5877174139022827\n",
      "Epoch 15, Step 34 Loss = 0.8148166537284851\n",
      "Epoch 15, Step 35 Loss = 0.7193212509155273\n",
      "Epoch 15, Step 36 Loss = 0.9100437760353088\n",
      "Epoch 15, Step 37 Loss = 0.524925172328949\n",
      "Epoch 15, Step 38 Loss = 1.3693435192108154\n",
      "Epoch 15, Step 39 Loss = 0.7643789649009705\n",
      "Epoch 15, Step 40 Loss = 1.3621081113815308\n",
      "Epoch 15, Step 41 Loss = 1.1372547149658203\n",
      "Training loss: 0.8774\n",
      "Epoch 16/50\n",
      "Epoch 16, Step 1 Loss = 0.7382377982139587\n",
      "Epoch 16, Step 2 Loss = 0.7516993880271912\n",
      "Epoch 16, Step 3 Loss = 0.5106860399246216\n",
      "Epoch 16, Step 4 Loss = 0.9124302864074707\n",
      "Epoch 16, Step 5 Loss = 1.172189712524414\n",
      "Epoch 16, Step 6 Loss = 0.6767155528068542\n",
      "Epoch 16, Step 7 Loss = 0.7888936996459961\n",
      "Epoch 16, Step 8 Loss = 0.5821336507797241\n",
      "Epoch 16, Step 9 Loss = 0.8912888169288635\n",
      "Epoch 16, Step 10 Loss = 0.7459061741828918\n",
      "Epoch 16, Step 11 Loss = 0.8408567905426025\n",
      "Epoch 16, Step 12 Loss = 0.5500297546386719\n",
      "Epoch 16, Step 13 Loss = 0.5411040186882019\n",
      "Epoch 16, Step 14 Loss = 1.0838754177093506\n",
      "Epoch 16, Step 15 Loss = 0.5779829621315002\n",
      "Epoch 16, Step 16 Loss = 0.8810641765594482\n",
      "Epoch 16, Step 17 Loss = 1.1915123462677002\n",
      "Epoch 16, Step 18 Loss = 1.2892792224884033\n",
      "Epoch 16, Step 19 Loss = 0.6828494668006897\n",
      "Epoch 16, Step 20 Loss = 0.8750303983688354\n",
      "Epoch 16, Step 21 Loss = 0.8258119821548462\n",
      "Epoch 16, Step 22 Loss = 0.5957841873168945\n",
      "Epoch 16, Step 23 Loss = 0.86259925365448\n",
      "Epoch 16, Step 24 Loss = 0.6322288513183594\n",
      "Epoch 16, Step 25 Loss = 0.777890682220459\n",
      "Epoch 16, Step 26 Loss = 0.3770381510257721\n",
      "Epoch 16, Step 27 Loss = 1.0404305458068848\n",
      "Epoch 16, Step 28 Loss = 0.9163964986801147\n",
      "Epoch 16, Step 29 Loss = 1.1934592723846436\n",
      "Epoch 16, Step 30 Loss = 0.9722370505332947\n",
      "Epoch 16, Step 31 Loss = 1.2464349269866943\n",
      "Epoch 16, Step 32 Loss = 0.8483526706695557\n",
      "Epoch 16, Step 33 Loss = 0.8106119632720947\n",
      "Epoch 16, Step 34 Loss = 0.769938588142395\n",
      "Epoch 16, Step 35 Loss = 0.8925319314002991\n",
      "Epoch 16, Step 36 Loss = 0.7097540497779846\n",
      "Epoch 16, Step 37 Loss = 0.8980832695960999\n",
      "Epoch 16, Step 38 Loss = 0.6863082647323608\n",
      "Epoch 16, Step 39 Loss = 0.8804643750190735\n",
      "Epoch 16, Step 40 Loss = 1.142110824584961\n",
      "Epoch 16, Step 41 Loss = 1.0552529096603394\n",
      "Training loss: 0.8395\n",
      "Epoch 17/50\n",
      "Epoch 17, Step 1 Loss = 0.46708670258522034\n",
      "Epoch 17, Step 2 Loss = 0.7174344658851624\n",
      "Epoch 17, Step 3 Loss = 0.9089505672454834\n",
      "Epoch 17, Step 4 Loss = 0.36678993701934814\n",
      "Epoch 17, Step 5 Loss = 0.6170835494995117\n",
      "Epoch 17, Step 6 Loss = 0.37134259939193726\n",
      "Epoch 17, Step 7 Loss = 1.0588269233703613\n",
      "Epoch 17, Step 8 Loss = 0.42757126688957214\n",
      "Epoch 17, Step 9 Loss = 0.7901266813278198\n",
      "Epoch 17, Step 10 Loss = 0.7582770586013794\n",
      "Epoch 17, Step 11 Loss = 0.8411630392074585\n",
      "Epoch 17, Step 12 Loss = 0.560565710067749\n",
      "Epoch 17, Step 13 Loss = 0.9722360968589783\n",
      "Epoch 17, Step 14 Loss = 0.36366984248161316\n",
      "Epoch 17, Step 15 Loss = 1.0801804065704346\n",
      "Epoch 17, Step 16 Loss = 0.41620683670043945\n",
      "Epoch 17, Step 17 Loss = 0.6640208959579468\n",
      "Epoch 17, Step 18 Loss = 0.9083994626998901\n",
      "Epoch 17, Step 19 Loss = 0.5248661637306213\n",
      "Epoch 17, Step 20 Loss = 0.49244701862335205\n",
      "Epoch 17, Step 21 Loss = 0.6212786436080933\n",
      "Epoch 17, Step 22 Loss = 0.9235913157463074\n",
      "Epoch 17, Step 23 Loss = 0.9987368583679199\n",
      "Epoch 17, Step 24 Loss = 0.9619505405426025\n",
      "Epoch 17, Step 25 Loss = 0.7922455072402954\n",
      "Epoch 17, Step 26 Loss = 1.1933661699295044\n",
      "Epoch 17, Step 27 Loss = 0.9646594524383545\n",
      "Epoch 17, Step 28 Loss = 0.5331231355667114\n",
      "Epoch 17, Step 29 Loss = 0.8959470391273499\n",
      "Epoch 17, Step 30 Loss = 0.6483607888221741\n",
      "Epoch 17, Step 31 Loss = 1.2349125146865845\n",
      "Epoch 17, Step 32 Loss = 0.8964647650718689\n",
      "Epoch 17, Step 33 Loss = 1.0362125635147095\n",
      "Epoch 17, Step 34 Loss = 0.8303563594818115\n",
      "Epoch 17, Step 35 Loss = 0.989284336566925\n",
      "Epoch 17, Step 36 Loss = 0.9076873660087585\n",
      "Epoch 17, Step 37 Loss = 1.0090641975402832\n",
      "Epoch 17, Step 38 Loss = 0.6079294085502625\n",
      "Epoch 17, Step 39 Loss = 1.076423168182373\n",
      "Epoch 17, Step 40 Loss = 0.7188038229942322\n",
      "Epoch 17, Step 41 Loss = 1.0025098323822021\n",
      "Training loss: 0.7842\n",
      "Epoch 18/50\n",
      "Epoch 18, Step 1 Loss = 0.5788148641586304\n",
      "Epoch 18, Step 2 Loss = 0.5773481726646423\n",
      "Epoch 18, Step 3 Loss = 0.5673637986183167\n",
      "Epoch 18, Step 4 Loss = 0.9918352365493774\n",
      "Epoch 18, Step 5 Loss = 0.6605153679847717\n",
      "Epoch 18, Step 6 Loss = 0.6004811525344849\n",
      "Epoch 18, Step 7 Loss = 0.8422973155975342\n",
      "Epoch 18, Step 8 Loss = 0.6671258211135864\n",
      "Epoch 18, Step 9 Loss = 0.6576662659645081\n",
      "Epoch 18, Step 10 Loss = 0.7080525159835815\n",
      "Epoch 18, Step 11 Loss = 0.8333425521850586\n",
      "Epoch 18, Step 12 Loss = 0.5697138905525208\n",
      "Epoch 18, Step 13 Loss = 0.36111295223236084\n",
      "Epoch 18, Step 14 Loss = 0.619262158870697\n",
      "Epoch 18, Step 15 Loss = 0.7816246747970581\n",
      "Epoch 18, Step 16 Loss = 1.1328067779541016\n",
      "Epoch 18, Step 17 Loss = 0.78355473279953\n",
      "Epoch 18, Step 18 Loss = 0.8063523769378662\n",
      "Epoch 18, Step 19 Loss = 0.6976454257965088\n",
      "Epoch 18, Step 20 Loss = 0.8755161762237549\n",
      "Epoch 18, Step 21 Loss = 1.222953200340271\n",
      "Epoch 18, Step 22 Loss = 0.4979054629802704\n",
      "Epoch 18, Step 23 Loss = 0.715173602104187\n",
      "Epoch 18, Step 24 Loss = 1.2051494121551514\n",
      "Epoch 18, Step 25 Loss = 1.024070143699646\n",
      "Epoch 18, Step 26 Loss = 0.5813636779785156\n",
      "Epoch 18, Step 27 Loss = 0.6520939469337463\n",
      "Epoch 18, Step 28 Loss = 0.7632173895835876\n",
      "Epoch 18, Step 29 Loss = 0.8041924238204956\n",
      "Epoch 18, Step 30 Loss = 0.7334244251251221\n",
      "Epoch 18, Step 31 Loss = 1.0067932605743408\n",
      "Epoch 18, Step 32 Loss = 1.2559617757797241\n",
      "Epoch 18, Step 33 Loss = 0.8265875577926636\n",
      "Epoch 18, Step 34 Loss = 1.2647266387939453\n",
      "Epoch 18, Step 35 Loss = 0.4514334201812744\n",
      "Epoch 18, Step 36 Loss = 0.7834365963935852\n",
      "Epoch 18, Step 37 Loss = 1.163230299949646\n",
      "Epoch 18, Step 38 Loss = 1.0358730554580688\n",
      "Epoch 18, Step 39 Loss = 0.9517830610275269\n",
      "Epoch 18, Step 40 Loss = 0.7649928331375122\n",
      "Epoch 18, Step 41 Loss = 1.144351601600647\n",
      "Training loss: 0.8088\n",
      "Epoch 19/50\n",
      "Epoch 19, Step 1 Loss = 0.4864656329154968\n",
      "Epoch 19, Step 2 Loss = 1.1119117736816406\n",
      "Epoch 19, Step 3 Loss = 0.5116944313049316\n",
      "Epoch 19, Step 4 Loss = 0.4732462763786316\n",
      "Epoch 19, Step 5 Loss = 0.5103588104248047\n",
      "Epoch 19, Step 6 Loss = 0.5676424503326416\n",
      "Epoch 19, Step 7 Loss = 0.35649460554122925\n",
      "Epoch 19, Step 8 Loss = 0.40093570947647095\n",
      "Epoch 19, Step 9 Loss = 0.5045288801193237\n",
      "Epoch 19, Step 10 Loss = 0.7594746947288513\n",
      "Epoch 19, Step 11 Loss = 0.7959871292114258\n",
      "Epoch 19, Step 12 Loss = 0.6711700558662415\n",
      "Epoch 19, Step 13 Loss = 0.621383786201477\n",
      "Epoch 19, Step 14 Loss = 0.6864609718322754\n",
      "Epoch 19, Step 15 Loss = 0.7985429167747498\n",
      "Epoch 19, Step 16 Loss = 0.6627746224403381\n",
      "Epoch 19, Step 17 Loss = 0.6885360479354858\n",
      "Epoch 19, Step 18 Loss = 0.5574655532836914\n",
      "Epoch 19, Step 19 Loss = 0.7046521902084351\n",
      "Epoch 19, Step 20 Loss = 0.5171090364456177\n",
      "Epoch 19, Step 21 Loss = 0.9810172319412231\n",
      "Epoch 19, Step 22 Loss = 0.8545206189155579\n",
      "Epoch 19, Step 23 Loss = 1.3927358388900757\n",
      "Epoch 19, Step 24 Loss = 0.48950016498565674\n",
      "Epoch 19, Step 25 Loss = 0.3913341164588928\n",
      "Epoch 19, Step 26 Loss = 0.8367630243301392\n",
      "Epoch 19, Step 27 Loss = 0.7122974395751953\n",
      "Epoch 19, Step 28 Loss = 0.8181997537612915\n",
      "Epoch 19, Step 29 Loss = 1.5903511047363281\n",
      "Epoch 19, Step 30 Loss = 0.5238569378852844\n",
      "Epoch 19, Step 31 Loss = 0.7156630754470825\n",
      "Epoch 19, Step 32 Loss = 0.9381549954414368\n",
      "Epoch 19, Step 33 Loss = 0.5365365147590637\n",
      "Epoch 19, Step 34 Loss = 0.9746559858322144\n",
      "Epoch 19, Step 35 Loss = 0.8923450708389282\n",
      "Epoch 19, Step 36 Loss = 0.9542863965034485\n",
      "Epoch 19, Step 37 Loss = 0.9931188821792603\n",
      "Epoch 19, Step 38 Loss = 0.9684268236160278\n",
      "Epoch 19, Step 39 Loss = 1.3833900690078735\n",
      "Epoch 19, Step 40 Loss = 1.6940257549285889\n",
      "Epoch 19, Step 41 Loss = 1.04009211063385\n",
      "Training loss: 0.7821\n",
      "Epoch 20/50\n",
      "Epoch 20, Step 1 Loss = 0.780423641204834\n",
      "Epoch 20, Step 2 Loss = 0.4699288606643677\n",
      "Epoch 20, Step 3 Loss = 0.7795870304107666\n",
      "Epoch 20, Step 4 Loss = 0.7558362483978271\n",
      "Epoch 20, Step 5 Loss = 0.8669312596321106\n",
      "Epoch 20, Step 6 Loss = 0.6172364950180054\n",
      "Epoch 20, Step 7 Loss = 0.5917636752128601\n",
      "Epoch 20, Step 8 Loss = 0.6894497275352478\n",
      "Epoch 20, Step 9 Loss = 0.8180829882621765\n",
      "Epoch 20, Step 10 Loss = 0.9306538105010986\n",
      "Epoch 20, Step 11 Loss = 0.7964876890182495\n",
      "Epoch 20, Step 12 Loss = 0.8687946200370789\n",
      "Epoch 20, Step 13 Loss = 0.7496474981307983\n",
      "Epoch 20, Step 14 Loss = 0.5355471968650818\n",
      "Epoch 20, Step 15 Loss = 0.5306060910224915\n",
      "Epoch 20, Step 16 Loss = 0.17252874374389648\n",
      "Epoch 20, Step 17 Loss = 0.4505783021450043\n",
      "Epoch 20, Step 18 Loss = 0.9927520751953125\n",
      "Epoch 20, Step 19 Loss = 0.6321755647659302\n",
      "Epoch 20, Step 20 Loss = 0.5897274017333984\n",
      "Epoch 20, Step 21 Loss = 0.6984103322029114\n",
      "Epoch 20, Step 22 Loss = 1.3727059364318848\n",
      "Epoch 20, Step 23 Loss = 0.47649577260017395\n",
      "Epoch 20, Step 24 Loss = 0.5233489274978638\n",
      "Epoch 20, Step 25 Loss = 0.8461859822273254\n",
      "Epoch 20, Step 26 Loss = 1.1328346729278564\n",
      "Epoch 20, Step 27 Loss = 0.8342816829681396\n",
      "Epoch 20, Step 28 Loss = 0.8231085538864136\n",
      "Epoch 20, Step 29 Loss = 0.6823415756225586\n",
      "Epoch 20, Step 30 Loss = 0.7795970439910889\n",
      "Epoch 20, Step 31 Loss = 1.4237899780273438\n",
      "Epoch 20, Step 32 Loss = 0.3763549327850342\n",
      "Epoch 20, Step 33 Loss = 0.8536288142204285\n",
      "Epoch 20, Step 34 Loss = 0.6755695343017578\n",
      "Epoch 20, Step 35 Loss = 1.2847745418548584\n",
      "Epoch 20, Step 36 Loss = 1.3023958206176758\n",
      "Epoch 20, Step 37 Loss = 1.1590304374694824\n",
      "Epoch 20, Step 38 Loss = 0.9598859548568726\n",
      "Epoch 20, Step 39 Loss = 1.0599702596664429\n",
      "Epoch 20, Step 40 Loss = 0.48324620723724365\n",
      "Epoch 20, Step 41 Loss = 1.205122709274292\n",
      "Training loss: 0.7944\n",
      "Epoch 21/50\n",
      "Epoch 21, Step 1 Loss = 0.5709732174873352\n",
      "Epoch 21, Step 2 Loss = 0.6900883913040161\n",
      "Epoch 21, Step 3 Loss = 0.36904382705688477\n",
      "Epoch 21, Step 4 Loss = 0.6185592412948608\n",
      "Epoch 21, Step 5 Loss = 0.6291738748550415\n",
      "Epoch 21, Step 6 Loss = 0.35519325733184814\n",
      "Epoch 21, Step 7 Loss = 0.4833361506462097\n",
      "Epoch 21, Step 8 Loss = 0.8338671922683716\n",
      "Epoch 21, Step 9 Loss = 0.6777772903442383\n",
      "Epoch 21, Step 10 Loss = 0.5755529403686523\n",
      "Epoch 21, Step 11 Loss = 0.7212489247322083\n",
      "Epoch 21, Step 12 Loss = 0.9046968221664429\n",
      "Epoch 21, Step 13 Loss = 0.791182279586792\n",
      "Epoch 21, Step 14 Loss = 0.6403211355209351\n",
      "Epoch 21, Step 15 Loss = 0.6934814453125\n",
      "Epoch 21, Step 16 Loss = 0.8184195756912231\n",
      "Epoch 21, Step 17 Loss = 0.8349698781967163\n",
      "Epoch 21, Step 18 Loss = 0.3755404055118561\n",
      "Epoch 21, Step 19 Loss = 0.6812021732330322\n",
      "Epoch 21, Step 20 Loss = 1.0976225137710571\n",
      "Epoch 21, Step 21 Loss = 0.4116898477077484\n",
      "Epoch 21, Step 22 Loss = 0.6456550359725952\n",
      "Epoch 21, Step 23 Loss = 1.061761736869812\n",
      "Epoch 21, Step 24 Loss = 0.7258299589157104\n",
      "Epoch 21, Step 25 Loss = 0.9333698749542236\n",
      "Epoch 21, Step 26 Loss = 0.5797640085220337\n",
      "Epoch 21, Step 27 Loss = 0.8100771307945251\n",
      "Epoch 21, Step 28 Loss = 0.9724067449569702\n",
      "Epoch 21, Step 29 Loss = 0.7928096055984497\n",
      "Epoch 21, Step 30 Loss = 1.328818440437317\n",
      "Epoch 21, Step 31 Loss = 0.1721140444278717\n",
      "Epoch 21, Step 32 Loss = 0.9385588765144348\n",
      "Epoch 21, Step 33 Loss = 0.8056342005729675\n",
      "Epoch 21, Step 34 Loss = 0.9459800124168396\n",
      "Epoch 21, Step 35 Loss = 1.164791464805603\n",
      "Epoch 21, Step 36 Loss = 1.1994836330413818\n",
      "Epoch 21, Step 37 Loss = 0.9033673405647278\n",
      "Epoch 21, Step 38 Loss = 1.2687007188796997\n",
      "Epoch 21, Step 39 Loss = 1.200110673904419\n",
      "Epoch 21, Step 40 Loss = 1.0663164854049683\n",
      "Epoch 21, Step 41 Loss = 1.0668094158172607\n",
      "Training loss: 0.7892\n",
      "Epoch 22/50\n",
      "Epoch 22, Step 1 Loss = 0.5845053195953369\n",
      "Epoch 22, Step 2 Loss = 0.5831477642059326\n",
      "Epoch 22, Step 3 Loss = 0.725339412689209\n",
      "Epoch 22, Step 4 Loss = 0.7076818346977234\n",
      "Epoch 22, Step 5 Loss = 0.4936547875404358\n",
      "Epoch 22, Step 6 Loss = 0.7988169193267822\n",
      "Epoch 22, Step 7 Loss = 0.8422545194625854\n",
      "Epoch 22, Step 8 Loss = 0.6563161611557007\n",
      "Epoch 22, Step 9 Loss = 0.6194627285003662\n",
      "Epoch 22, Step 10 Loss = 0.33058053255081177\n",
      "Epoch 22, Step 11 Loss = 0.617002010345459\n",
      "Epoch 22, Step 12 Loss = 0.41835787892341614\n",
      "Epoch 22, Step 13 Loss = 0.32087063789367676\n",
      "Epoch 22, Step 14 Loss = 1.0714199542999268\n",
      "Epoch 22, Step 15 Loss = 0.7597237229347229\n",
      "Epoch 22, Step 16 Loss = 0.4681873619556427\n",
      "Epoch 22, Step 17 Loss = 0.8022819757461548\n",
      "Epoch 22, Step 18 Loss = 0.8963053226470947\n",
      "Epoch 22, Step 19 Loss = 0.8245261907577515\n",
      "Epoch 22, Step 20 Loss = 0.724872350692749\n",
      "Epoch 22, Step 21 Loss = 0.35371100902557373\n",
      "Epoch 22, Step 22 Loss = 0.6754624843597412\n",
      "Epoch 22, Step 23 Loss = 1.3101240396499634\n",
      "Epoch 22, Step 24 Loss = 0.6928398013114929\n",
      "Epoch 22, Step 25 Loss = 0.4352606534957886\n",
      "Epoch 22, Step 26 Loss = 1.0375359058380127\n",
      "Epoch 22, Step 27 Loss = 1.3428385257720947\n",
      "Epoch 22, Step 28 Loss = 0.4004325270652771\n",
      "Epoch 22, Step 29 Loss = 1.0117024183273315\n",
      "Epoch 22, Step 30 Loss = 1.0633344650268555\n",
      "Epoch 22, Step 31 Loss = 0.6003906726837158\n",
      "Epoch 22, Step 32 Loss = 0.5320755243301392\n",
      "Epoch 22, Step 33 Loss = 1.3818587064743042\n",
      "Epoch 22, Step 34 Loss = 0.6537731289863586\n",
      "Epoch 22, Step 35 Loss = 1.4360053539276123\n",
      "Epoch 22, Step 36 Loss = 0.5308899283409119\n",
      "Epoch 22, Step 37 Loss = 0.8795220255851746\n",
      "Epoch 22, Step 38 Loss = 1.5712122917175293\n",
      "Epoch 22, Step 39 Loss = 0.817493736743927\n",
      "Epoch 22, Step 40 Loss = 1.0176643133163452\n",
      "Epoch 22, Step 41 Loss = 0.9612234830856323\n",
      "Training loss: 0.7793\n",
      "Epoch 23/50\n",
      "Epoch 23, Step 1 Loss = 0.47761455178260803\n",
      "Epoch 23, Step 2 Loss = 0.6155617237091064\n",
      "Epoch 23, Step 3 Loss = 0.7144697904586792\n",
      "Epoch 23, Step 4 Loss = 0.7707551717758179\n",
      "Epoch 23, Step 5 Loss = 0.8141547441482544\n",
      "Epoch 23, Step 6 Loss = 0.7909611463546753\n",
      "Epoch 23, Step 7 Loss = 0.4490928053855896\n",
      "Epoch 23, Step 8 Loss = 1.2131447792053223\n",
      "Epoch 23, Step 9 Loss = 0.7174906730651855\n",
      "Epoch 23, Step 10 Loss = 1.0090277194976807\n",
      "Epoch 23, Step 11 Loss = 0.632156491279602\n",
      "Epoch 23, Step 12 Loss = 0.8597615957260132\n",
      "Epoch 23, Step 13 Loss = 0.5683395266532898\n",
      "Epoch 23, Step 14 Loss = 0.7521064877510071\n",
      "Epoch 23, Step 15 Loss = 0.32328951358795166\n",
      "Epoch 23, Step 16 Loss = 0.8031842708587646\n",
      "Epoch 23, Step 17 Loss = 0.8988744020462036\n",
      "Epoch 23, Step 18 Loss = 0.5463143587112427\n",
      "Epoch 23, Step 19 Loss = 0.8978040218353271\n",
      "Epoch 23, Step 20 Loss = 0.5285267233848572\n",
      "Epoch 23, Step 21 Loss = 0.7087336778640747\n",
      "Epoch 23, Step 22 Loss = 0.5592994093894958\n",
      "Epoch 23, Step 23 Loss = 0.849442720413208\n",
      "Epoch 23, Step 24 Loss = 0.712441086769104\n",
      "Epoch 23, Step 25 Loss = 1.3683245182037354\n",
      "Epoch 23, Step 26 Loss = 0.6677772998809814\n",
      "Epoch 23, Step 27 Loss = 0.5750569701194763\n",
      "Epoch 23, Step 28 Loss = 0.7615153789520264\n",
      "Epoch 23, Step 29 Loss = 0.43152472376823425\n",
      "Epoch 23, Step 30 Loss = 1.1183204650878906\n",
      "Epoch 23, Step 31 Loss = 0.8077860474586487\n",
      "Epoch 23, Step 32 Loss = 0.7300184369087219\n",
      "Epoch 23, Step 33 Loss = 0.48890745639801025\n",
      "Epoch 23, Step 34 Loss = 1.1299870014190674\n",
      "Epoch 23, Step 35 Loss = 0.6759878993034363\n",
      "Epoch 23, Step 36 Loss = 0.7430802583694458\n",
      "Epoch 23, Step 37 Loss = 1.2531931400299072\n",
      "Epoch 23, Step 38 Loss = 0.8498209118843079\n",
      "Epoch 23, Step 39 Loss = 1.1135518550872803\n",
      "Epoch 23, Step 40 Loss = 0.8492364287376404\n",
      "Epoch 23, Step 41 Loss = 0.9572258591651917\n",
      "Training loss: 0.7740\n",
      "Epoch 24/50\n",
      "Epoch 24, Step 1 Loss = 0.5517848134040833\n",
      "Epoch 24, Step 2 Loss = 0.6851425766944885\n",
      "Epoch 24, Step 3 Loss = 0.7384101152420044\n",
      "Epoch 24, Step 4 Loss = 0.3077937364578247\n",
      "Epoch 24, Step 5 Loss = 1.0298296213150024\n",
      "Epoch 24, Step 6 Loss = 0.7237170338630676\n",
      "Epoch 24, Step 7 Loss = 0.5347539186477661\n",
      "Epoch 24, Step 8 Loss = 0.7549485564231873\n",
      "Epoch 24, Step 9 Loss = 0.6390308737754822\n",
      "Epoch 24, Step 10 Loss = 0.6716359853744507\n",
      "Epoch 24, Step 11 Loss = 0.9599330425262451\n",
      "Epoch 24, Step 12 Loss = 0.6439158320426941\n",
      "Epoch 24, Step 13 Loss = 0.4778015613555908\n",
      "Epoch 24, Step 14 Loss = 0.6469922065734863\n",
      "Epoch 24, Step 15 Loss = 1.292057752609253\n",
      "Epoch 24, Step 16 Loss = 1.4384217262268066\n",
      "Epoch 24, Step 17 Loss = 0.5582048892974854\n",
      "Epoch 24, Step 18 Loss = 0.5877009630203247\n",
      "Epoch 24, Step 19 Loss = 0.7398495674133301\n",
      "Epoch 24, Step 20 Loss = 1.0443097352981567\n",
      "Epoch 24, Step 21 Loss = 0.6470316648483276\n",
      "Epoch 24, Step 22 Loss = 0.9964259266853333\n",
      "Epoch 24, Step 23 Loss = 0.5609477758407593\n",
      "Epoch 24, Step 24 Loss = 0.9814362525939941\n",
      "Epoch 24, Step 25 Loss = 0.7433655261993408\n",
      "Epoch 24, Step 26 Loss = 0.7812293767929077\n",
      "Epoch 24, Step 27 Loss = 0.7663158774375916\n",
      "Epoch 24, Step 28 Loss = 0.4235685467720032\n",
      "Epoch 24, Step 29 Loss = 0.29348841309547424\n",
      "Epoch 24, Step 30 Loss = 0.8103470206260681\n",
      "Epoch 24, Step 31 Loss = 0.6294936537742615\n",
      "Epoch 24, Step 32 Loss = 0.840262234210968\n",
      "Epoch 24, Step 33 Loss = 0.9711620807647705\n",
      "Epoch 24, Step 34 Loss = 0.6268429756164551\n",
      "Epoch 24, Step 35 Loss = 0.7856006622314453\n",
      "Epoch 24, Step 36 Loss = 1.0850670337677002\n",
      "Epoch 24, Step 37 Loss = 0.7207388281822205\n",
      "Epoch 24, Step 38 Loss = 1.1137914657592773\n",
      "Epoch 24, Step 39 Loss = 0.6742216944694519\n",
      "Epoch 24, Step 40 Loss = 1.0554004907608032\n",
      "Epoch 24, Step 41 Loss = 0.8584730625152588\n",
      "Training loss: 0.7656\n",
      "Epoch 25/50\n",
      "Epoch 25, Step 1 Loss = 0.24959853291511536\n",
      "Epoch 25, Step 2 Loss = 0.6494513750076294\n",
      "Epoch 25, Step 3 Loss = 0.5771979689598083\n",
      "Epoch 25, Step 4 Loss = 0.5477597713470459\n",
      "Epoch 25, Step 5 Loss = 0.34980136156082153\n",
      "Epoch 25, Step 6 Loss = 0.4426867663860321\n",
      "Epoch 25, Step 7 Loss = 0.4552735984325409\n",
      "Epoch 25, Step 8 Loss = 0.3295777142047882\n",
      "Epoch 25, Step 9 Loss = 0.41865596175193787\n",
      "Epoch 25, Step 10 Loss = 0.23134316504001617\n",
      "Epoch 25, Step 11 Loss = 0.3725888133049011\n",
      "Epoch 25, Step 12 Loss = 0.7077562808990479\n",
      "Epoch 25, Step 13 Loss = 0.6478903293609619\n",
      "Epoch 25, Step 14 Loss = 0.5658504366874695\n",
      "Epoch 25, Step 15 Loss = 0.6537082195281982\n",
      "Epoch 25, Step 16 Loss = 0.42493587732315063\n",
      "Epoch 25, Step 17 Loss = 0.3741335868835449\n",
      "Epoch 25, Step 18 Loss = 0.5371362566947937\n",
      "Epoch 25, Step 19 Loss = 0.46349990367889404\n",
      "Epoch 25, Step 20 Loss = 1.1083238124847412\n",
      "Epoch 25, Step 21 Loss = 0.8007142543792725\n",
      "Epoch 25, Step 22 Loss = 1.0465272665023804\n",
      "Epoch 25, Step 23 Loss = 1.0586352348327637\n",
      "Epoch 25, Step 24 Loss = 0.9523066282272339\n",
      "Epoch 25, Step 25 Loss = 0.7497074604034424\n",
      "Epoch 25, Step 26 Loss = 1.2768652439117432\n",
      "Epoch 25, Step 27 Loss = 0.7805019617080688\n",
      "Epoch 25, Step 28 Loss = 0.8833795785903931\n",
      "Epoch 25, Step 29 Loss = 0.9728353023529053\n",
      "Epoch 25, Step 30 Loss = 1.2161979675292969\n",
      "Epoch 25, Step 31 Loss = 0.7365953922271729\n",
      "Epoch 25, Step 32 Loss = 0.2530035972595215\n",
      "Epoch 25, Step 33 Loss = 1.008117914199829\n",
      "Epoch 25, Step 34 Loss = 1.062683343887329\n",
      "Epoch 25, Step 35 Loss = 1.2873365879058838\n",
      "Epoch 25, Step 36 Loss = 0.5704135894775391\n",
      "Epoch 25, Step 37 Loss = 0.4654158055782318\n",
      "Epoch 25, Step 38 Loss = 1.2073521614074707\n",
      "Epoch 25, Step 39 Loss = 0.8474376201629639\n",
      "Epoch 25, Step 40 Loss = 1.313984751701355\n",
      "Epoch 25, Step 41 Loss = 0.8410751819610596\n",
      "Training loss: 0.7180\n",
      "Epoch 26/50\n",
      "Epoch 26, Step 1 Loss = 0.4419490098953247\n",
      "Epoch 26, Step 2 Loss = 0.3261522650718689\n",
      "Epoch 26, Step 3 Loss = 0.6126474142074585\n",
      "Epoch 26, Step 4 Loss = 0.44015079736709595\n",
      "Epoch 26, Step 5 Loss = 0.44178980588912964\n",
      "Epoch 26, Step 6 Loss = 0.7741236090660095\n",
      "Epoch 26, Step 7 Loss = 0.36587443947792053\n",
      "Epoch 26, Step 8 Loss = 0.49951133131980896\n",
      "Epoch 26, Step 9 Loss = 0.5096279978752136\n",
      "Epoch 26, Step 10 Loss = 0.442015141248703\n",
      "Epoch 26, Step 11 Loss = 0.8716884851455688\n",
      "Epoch 26, Step 12 Loss = 0.9043219089508057\n",
      "Epoch 26, Step 13 Loss = 0.7393273115158081\n",
      "Epoch 26, Step 14 Loss = 0.7306168675422668\n",
      "Epoch 26, Step 15 Loss = 1.0451455116271973\n",
      "Epoch 26, Step 16 Loss = 0.7863460779190063\n",
      "Epoch 26, Step 17 Loss = 0.45092734694480896\n",
      "Epoch 26, Step 18 Loss = 1.0393091440200806\n",
      "Epoch 26, Step 19 Loss = 0.9054766297340393\n",
      "Epoch 26, Step 20 Loss = 0.8416045904159546\n",
      "Epoch 26, Step 21 Loss = 0.8123604655265808\n",
      "Epoch 26, Step 22 Loss = 0.5793843269348145\n",
      "Epoch 26, Step 23 Loss = 0.8780726194381714\n",
      "Epoch 26, Step 24 Loss = 0.49997836351394653\n",
      "Epoch 26, Step 25 Loss = 1.0614420175552368\n",
      "Epoch 26, Step 26 Loss = 0.5746513605117798\n",
      "Epoch 26, Step 27 Loss = 0.8872972726821899\n",
      "Epoch 26, Step 28 Loss = 0.6967195868492126\n",
      "Epoch 26, Step 29 Loss = 0.6881335377693176\n",
      "Epoch 26, Step 30 Loss = 0.4602104425430298\n",
      "Epoch 26, Step 31 Loss = 1.2429155111312866\n",
      "Epoch 26, Step 32 Loss = 0.6520790457725525\n",
      "Epoch 26, Step 33 Loss = 1.05392324924469\n",
      "Epoch 26, Step 34 Loss = 0.5868393182754517\n",
      "Epoch 26, Step 35 Loss = 0.7076355218887329\n",
      "Epoch 26, Step 36 Loss = 0.8542157411575317\n",
      "Epoch 26, Step 37 Loss = 1.218893051147461\n",
      "Epoch 26, Step 38 Loss = 0.6537500619888306\n",
      "Epoch 26, Step 39 Loss = 0.8095451593399048\n",
      "Epoch 26, Step 40 Loss = 1.0556349754333496\n",
      "Epoch 26, Step 41 Loss = 0.5883644819259644\n",
      "Training loss: 0.7251\n",
      "Epoch 27/50\n",
      "Epoch 27, Step 1 Loss = 0.4049608111381531\n",
      "Epoch 27, Step 2 Loss = 0.4346100687980652\n",
      "Epoch 27, Step 3 Loss = 0.8373207449913025\n",
      "Epoch 27, Step 4 Loss = 0.5761862993240356\n",
      "Epoch 27, Step 5 Loss = 0.34524649381637573\n",
      "Epoch 27, Step 6 Loss = 0.6552242636680603\n",
      "Epoch 27, Step 7 Loss = 1.033734917640686\n",
      "Epoch 27, Step 8 Loss = 0.9508367776870728\n",
      "Epoch 27, Step 9 Loss = 0.7634824514389038\n",
      "Epoch 27, Step 10 Loss = 0.4559842348098755\n",
      "Epoch 27, Step 11 Loss = 0.6121451258659363\n",
      "Epoch 27, Step 12 Loss = 0.43008673191070557\n",
      "Epoch 27, Step 13 Loss = 0.5762113332748413\n",
      "Epoch 27, Step 14 Loss = 0.4616187810897827\n",
      "Epoch 27, Step 15 Loss = 0.4883437156677246\n",
      "Epoch 27, Step 16 Loss = 0.712446391582489\n",
      "Epoch 27, Step 17 Loss = 0.9519147276878357\n",
      "Epoch 27, Step 18 Loss = 0.81884765625\n",
      "Epoch 27, Step 19 Loss = 0.9422619342803955\n",
      "Epoch 27, Step 20 Loss = 0.3243010640144348\n",
      "Epoch 27, Step 21 Loss = 0.5917032957077026\n",
      "Epoch 27, Step 22 Loss = 0.7306028604507446\n",
      "Epoch 27, Step 23 Loss = 0.7686399221420288\n",
      "Epoch 27, Step 24 Loss = 0.7937538623809814\n",
      "Epoch 27, Step 25 Loss = 0.7054140567779541\n",
      "Epoch 27, Step 26 Loss = 0.7864954471588135\n",
      "Epoch 27, Step 27 Loss = 1.2057113647460938\n",
      "Epoch 27, Step 28 Loss = 1.1356770992279053\n",
      "Epoch 27, Step 29 Loss = 0.7649576663970947\n",
      "Epoch 27, Step 30 Loss = 1.3014062643051147\n",
      "Epoch 27, Step 31 Loss = 0.9464113712310791\n",
      "Epoch 27, Step 32 Loss = 0.9286661744117737\n",
      "Epoch 27, Step 33 Loss = 1.0258891582489014\n",
      "Epoch 27, Step 34 Loss = 0.851201057434082\n",
      "Epoch 27, Step 35 Loss = 0.6630445718765259\n",
      "Epoch 27, Step 36 Loss = 0.4865299463272095\n",
      "Epoch 27, Step 37 Loss = 0.9211798906326294\n",
      "Epoch 27, Step 38 Loss = 0.5444949865341187\n",
      "Epoch 27, Step 39 Loss = 0.7187789678573608\n",
      "Epoch 27, Step 40 Loss = 0.6803209185600281\n",
      "Epoch 27, Step 41 Loss = 1.1984187364578247\n",
      "Training loss: 0.7445\n",
      "Epoch 28/50\n",
      "Epoch 28, Step 1 Loss = 0.6274235248565674\n",
      "Epoch 28, Step 2 Loss = 0.7580114603042603\n",
      "Epoch 28, Step 3 Loss = 1.089759349822998\n",
      "Epoch 28, Step 4 Loss = 0.5900725722312927\n",
      "Epoch 28, Step 5 Loss = 0.5814332962036133\n",
      "Epoch 28, Step 6 Loss = 0.5593808889389038\n",
      "Epoch 28, Step 7 Loss = 0.658993124961853\n",
      "Epoch 28, Step 8 Loss = 0.5482511520385742\n",
      "Epoch 28, Step 9 Loss = 0.5974034667015076\n",
      "Epoch 28, Step 10 Loss = 0.5655083656311035\n",
      "Epoch 28, Step 11 Loss = 0.935886025428772\n",
      "Epoch 28, Step 12 Loss = 0.5533271431922913\n",
      "Epoch 28, Step 13 Loss = 0.5952534675598145\n",
      "Epoch 28, Step 14 Loss = 0.5216598510742188\n",
      "Epoch 28, Step 15 Loss = 0.9164899587631226\n",
      "Epoch 28, Step 16 Loss = 0.6686288118362427\n",
      "Epoch 28, Step 17 Loss = 0.5876032114028931\n",
      "Epoch 28, Step 18 Loss = 1.1255310773849487\n",
      "Epoch 28, Step 19 Loss = 0.7379752397537231\n",
      "Epoch 28, Step 20 Loss = 0.561758816242218\n",
      "Epoch 28, Step 21 Loss = 0.421511709690094\n",
      "Epoch 28, Step 22 Loss = 0.7825270891189575\n",
      "Epoch 28, Step 23 Loss = 0.9695289134979248\n",
      "Epoch 28, Step 24 Loss = 0.521430492401123\n",
      "Epoch 28, Step 25 Loss = 0.5314459204673767\n",
      "Epoch 28, Step 26 Loss = 0.6144281625747681\n",
      "Epoch 28, Step 27 Loss = 0.7497327327728271\n",
      "Epoch 28, Step 28 Loss = 1.0138370990753174\n",
      "Epoch 28, Step 29 Loss = 0.6880950927734375\n",
      "Epoch 28, Step 30 Loss = 0.8714418411254883\n",
      "Epoch 28, Step 31 Loss = 1.2068172693252563\n",
      "Epoch 28, Step 32 Loss = 0.8218632340431213\n",
      "Epoch 28, Step 33 Loss = 0.7630095481872559\n",
      "Epoch 28, Step 34 Loss = 0.9011203050613403\n",
      "Epoch 28, Step 35 Loss = 0.6828917264938354\n",
      "Epoch 28, Step 36 Loss = 0.9800856709480286\n",
      "Epoch 28, Step 37 Loss = 0.9956458806991577\n",
      "Epoch 28, Step 38 Loss = 0.779025673866272\n",
      "Epoch 28, Step 39 Loss = 0.32388314604759216\n",
      "Epoch 28, Step 40 Loss = 1.4369946718215942\n",
      "Epoch 28, Step 41 Loss = 0.5761222839355469\n",
      "Training loss: 0.7418\n",
      "Epoch 29/50\n",
      "Epoch 29, Step 1 Loss = 0.7177194356918335\n",
      "Epoch 29, Step 2 Loss = 0.7403901815414429\n",
      "Epoch 29, Step 3 Loss = 0.17938928306102753\n",
      "Epoch 29, Step 4 Loss = 0.7691805958747864\n",
      "Epoch 29, Step 5 Loss = 0.5637506246566772\n",
      "Epoch 29, Step 6 Loss = 0.654419481754303\n",
      "Epoch 29, Step 7 Loss = 0.6671611070632935\n",
      "Epoch 29, Step 8 Loss = 0.5773540139198303\n",
      "Epoch 29, Step 9 Loss = 0.4834708571434021\n",
      "Epoch 29, Step 10 Loss = 0.7046959400177002\n",
      "Epoch 29, Step 11 Loss = 0.7578261494636536\n",
      "Epoch 29, Step 12 Loss = 0.8392131328582764\n",
      "Epoch 29, Step 13 Loss = 0.41309645771980286\n",
      "Epoch 29, Step 14 Loss = 0.5121179819107056\n",
      "Epoch 29, Step 15 Loss = 0.2563806474208832\n",
      "Epoch 29, Step 16 Loss = 0.818911612033844\n",
      "Epoch 29, Step 17 Loss = 0.8406175374984741\n",
      "Epoch 29, Step 18 Loss = 0.5647333860397339\n",
      "Epoch 29, Step 19 Loss = 0.6368611454963684\n",
      "Epoch 29, Step 20 Loss = 0.7341422438621521\n",
      "Epoch 29, Step 21 Loss = 0.6388757824897766\n",
      "Epoch 29, Step 22 Loss = 0.4356706142425537\n",
      "Epoch 29, Step 23 Loss = 0.876392662525177\n",
      "Epoch 29, Step 24 Loss = 0.6657302379608154\n",
      "Epoch 29, Step 25 Loss = 1.024100661277771\n",
      "Epoch 29, Step 26 Loss = 0.8056056499481201\n",
      "Epoch 29, Step 27 Loss = 0.9829323291778564\n",
      "Epoch 29, Step 28 Loss = 1.0965981483459473\n",
      "Epoch 29, Step 29 Loss = 1.0197302103042603\n",
      "Epoch 29, Step 30 Loss = 1.0559954643249512\n",
      "Epoch 29, Step 31 Loss = 0.5651553869247437\n",
      "Epoch 29, Step 32 Loss = 1.2211802005767822\n",
      "Epoch 29, Step 33 Loss = 0.8129949569702148\n",
      "Epoch 29, Step 34 Loss = 0.3528425097465515\n",
      "Epoch 29, Step 35 Loss = 0.9582513570785522\n",
      "Epoch 29, Step 36 Loss = 0.856726348400116\n",
      "Epoch 29, Step 37 Loss = 1.0832473039627075\n",
      "Epoch 29, Step 38 Loss = 0.9684062004089355\n",
      "Epoch 29, Step 39 Loss = 0.9419820308685303\n",
      "Epoch 29, Step 40 Loss = 1.4670383930206299\n",
      "Epoch 29, Step 41 Loss = 0.8516164422035217\n",
      "Training loss: 0.7588\n",
      "Epoch 30/50\n",
      "Epoch 30, Step 1 Loss = 0.7257521152496338\n",
      "Epoch 30, Step 2 Loss = 0.4074857831001282\n",
      "Epoch 30, Step 3 Loss = 0.33743754029273987\n",
      "Epoch 30, Step 4 Loss = 0.43697965145111084\n",
      "Epoch 30, Step 5 Loss = 0.5188120603561401\n",
      "Epoch 30, Step 6 Loss = 0.4191173315048218\n",
      "Epoch 30, Step 7 Loss = 0.5765597224235535\n",
      "Epoch 30, Step 8 Loss = 0.3818362355232239\n",
      "Epoch 30, Step 9 Loss = 0.4263385534286499\n",
      "Epoch 30, Step 10 Loss = 0.5683127641677856\n",
      "Epoch 30, Step 11 Loss = 0.6332129240036011\n",
      "Epoch 30, Step 12 Loss = 0.345032662153244\n",
      "Epoch 30, Step 13 Loss = 0.7870596051216125\n",
      "Epoch 30, Step 14 Loss = 0.6886993646621704\n",
      "Epoch 30, Step 15 Loss = 0.8467063903808594\n",
      "Epoch 30, Step 16 Loss = 0.7388713359832764\n",
      "Epoch 30, Step 17 Loss = 0.673891007900238\n",
      "Epoch 30, Step 18 Loss = 0.7012462615966797\n",
      "Epoch 30, Step 19 Loss = 0.2856177091598511\n",
      "Epoch 30, Step 20 Loss = 0.7425378561019897\n",
      "Epoch 30, Step 21 Loss = 0.9214401245117188\n",
      "Epoch 30, Step 22 Loss = 0.5779579281806946\n",
      "Epoch 30, Step 23 Loss = 1.0976935625076294\n",
      "Epoch 30, Step 24 Loss = 0.3393995761871338\n",
      "Epoch 30, Step 25 Loss = 0.6648727655410767\n",
      "Epoch 30, Step 26 Loss = 0.9957576990127563\n",
      "Epoch 30, Step 27 Loss = 0.6225588321685791\n",
      "Epoch 30, Step 28 Loss = 1.0401787757873535\n",
      "Epoch 30, Step 29 Loss = 0.7799339294433594\n",
      "Epoch 30, Step 30 Loss = 0.8066495060920715\n",
      "Epoch 30, Step 31 Loss = 1.2730491161346436\n",
      "Epoch 30, Step 32 Loss = 0.5369752049446106\n",
      "Epoch 30, Step 33 Loss = 0.9337944984436035\n",
      "Epoch 30, Step 34 Loss = 0.7438715100288391\n",
      "Epoch 30, Step 35 Loss = 1.0069483518600464\n",
      "Epoch 30, Step 36 Loss = 1.3687913417816162\n",
      "Epoch 30, Step 37 Loss = 0.984743058681488\n",
      "Epoch 30, Step 38 Loss = 0.6956467032432556\n",
      "Epoch 30, Step 39 Loss = 0.7461920976638794\n",
      "Epoch 30, Step 40 Loss = 1.3269777297973633\n",
      "Epoch 30, Step 41 Loss = 0.7630513906478882\n",
      "Training loss: 0.7187\n",
      "Epoch 31/50\n",
      "Epoch 31, Step 1 Loss = 0.7129622101783752\n",
      "Epoch 31, Step 2 Loss = 0.7993255257606506\n",
      "Epoch 31, Step 3 Loss = 0.9273801445960999\n",
      "Epoch 31, Step 4 Loss = 0.5349665880203247\n",
      "Epoch 31, Step 5 Loss = 0.2121606171131134\n",
      "Epoch 31, Step 6 Loss = 0.8943257927894592\n",
      "Epoch 31, Step 7 Loss = 0.7012497186660767\n",
      "Epoch 31, Step 8 Loss = 0.4461262822151184\n",
      "Epoch 31, Step 9 Loss = 0.5828976035118103\n",
      "Epoch 31, Step 10 Loss = 0.7859761714935303\n",
      "Epoch 31, Step 11 Loss = 0.518771767616272\n",
      "Epoch 31, Step 12 Loss = 0.6075031161308289\n",
      "Epoch 31, Step 13 Loss = 0.48935821652412415\n",
      "Epoch 31, Step 14 Loss = 0.39284753799438477\n",
      "Epoch 31, Step 15 Loss = 0.5727389454841614\n",
      "Epoch 31, Step 16 Loss = 0.7943766117095947\n",
      "Epoch 31, Step 17 Loss = 0.3880558907985687\n",
      "Epoch 31, Step 18 Loss = 0.6387224197387695\n",
      "Epoch 31, Step 19 Loss = 0.8136411905288696\n",
      "Epoch 31, Step 20 Loss = 0.41019386053085327\n",
      "Epoch 31, Step 21 Loss = 0.5324201583862305\n",
      "Epoch 31, Step 22 Loss = 0.7418825626373291\n",
      "Epoch 31, Step 23 Loss = 0.8894339799880981\n",
      "Epoch 31, Step 24 Loss = 0.6678178310394287\n",
      "Epoch 31, Step 25 Loss = 0.9278767108917236\n",
      "Epoch 31, Step 26 Loss = 0.6431596279144287\n",
      "Epoch 31, Step 27 Loss = 0.8844243288040161\n",
      "Epoch 31, Step 28 Loss = 0.9082926511764526\n",
      "Epoch 31, Step 29 Loss = 0.6616939306259155\n",
      "Epoch 31, Step 30 Loss = 0.8318099975585938\n",
      "Epoch 31, Step 31 Loss = 0.7157481908798218\n",
      "Epoch 31, Step 32 Loss = 0.654748797416687\n",
      "Epoch 31, Step 33 Loss = 1.1506695747375488\n",
      "Epoch 31, Step 34 Loss = 1.287000298500061\n",
      "Epoch 31, Step 35 Loss = 0.48817113041877747\n",
      "Epoch 31, Step 36 Loss = 0.3427158296108246\n",
      "Epoch 31, Step 37 Loss = 1.4009113311767578\n",
      "Epoch 31, Step 38 Loss = 1.5924218893051147\n",
      "Epoch 31, Step 39 Loss = 0.9940725564956665\n",
      "Epoch 31, Step 40 Loss = 1.1439385414123535\n",
      "Epoch 31, Step 41 Loss = 1.0927140712738037\n",
      "Training loss: 0.7506\n",
      "Epoch 32/50\n",
      "Epoch 32, Step 1 Loss = 0.34604090452194214\n",
      "Epoch 32, Step 2 Loss = 0.7336006164550781\n",
      "Epoch 32, Step 3 Loss = 0.7604711651802063\n",
      "Epoch 32, Step 4 Loss = 0.3488624393939972\n",
      "Epoch 32, Step 5 Loss = 0.5370609760284424\n",
      "Epoch 32, Step 6 Loss = 0.6293958425521851\n",
      "Epoch 32, Step 7 Loss = 0.8148139715194702\n",
      "Epoch 32, Step 8 Loss = 0.38330647349357605\n",
      "Epoch 32, Step 9 Loss = 0.31370481848716736\n",
      "Epoch 32, Step 10 Loss = 0.6650631427764893\n",
      "Epoch 32, Step 11 Loss = 0.6858667135238647\n",
      "Epoch 32, Step 12 Loss = 0.27860453724861145\n",
      "Epoch 32, Step 13 Loss = 0.7591713666915894\n",
      "Epoch 32, Step 14 Loss = 0.8220736980438232\n",
      "Epoch 32, Step 15 Loss = 0.6522769927978516\n",
      "Epoch 32, Step 16 Loss = 0.7215441465377808\n",
      "Epoch 32, Step 17 Loss = 0.7271765470504761\n",
      "Epoch 32, Step 18 Loss = 0.49412667751312256\n",
      "Epoch 32, Step 19 Loss = 1.0922397375106812\n",
      "Epoch 32, Step 20 Loss = 0.7881543040275574\n",
      "Epoch 32, Step 21 Loss = 0.931618332862854\n",
      "Epoch 32, Step 22 Loss = 0.7450526356697083\n",
      "Epoch 32, Step 23 Loss = 0.6657693386077881\n",
      "Epoch 32, Step 24 Loss = 0.5639671087265015\n",
      "Epoch 32, Step 25 Loss = 0.6417680382728577\n",
      "Epoch 32, Step 26 Loss = 1.0479440689086914\n",
      "Epoch 32, Step 27 Loss = 0.6952601671218872\n",
      "Epoch 32, Step 28 Loss = 0.4648483991622925\n",
      "Epoch 32, Step 29 Loss = 0.6774377226829529\n",
      "Epoch 32, Step 30 Loss = 0.6892000436782837\n",
      "Epoch 32, Step 31 Loss = 0.6575508117675781\n",
      "Epoch 32, Step 32 Loss = 0.8751816153526306\n",
      "Epoch 32, Step 33 Loss = 0.8424464464187622\n",
      "Epoch 32, Step 34 Loss = 0.5158406496047974\n",
      "Epoch 32, Step 35 Loss = 0.6753734946250916\n",
      "Epoch 32, Step 36 Loss = 0.9120492339134216\n",
      "Epoch 32, Step 37 Loss = 0.8961268663406372\n",
      "Epoch 32, Step 38 Loss = 0.6496360301971436\n",
      "Epoch 32, Step 39 Loss = 0.6624730825424194\n",
      "Epoch 32, Step 40 Loss = 0.7565060257911682\n",
      "Epoch 32, Step 41 Loss = 1.1453336477279663\n",
      "Training loss: 0.6894\n",
      "Epoch 33/50\n",
      "Epoch 33, Step 1 Loss = 0.7039443254470825\n",
      "Epoch 33, Step 2 Loss = 0.7141255736351013\n",
      "Epoch 33, Step 3 Loss = 0.68325275182724\n",
      "Epoch 33, Step 4 Loss = 0.6681165099143982\n",
      "Epoch 33, Step 5 Loss = 0.8452361822128296\n",
      "Epoch 33, Step 6 Loss = 0.6336157917976379\n",
      "Epoch 33, Step 7 Loss = 0.7120170593261719\n",
      "Epoch 33, Step 8 Loss = 0.5421360731124878\n",
      "Epoch 33, Step 9 Loss = 0.6971293091773987\n",
      "Epoch 33, Step 10 Loss = 1.0870164632797241\n",
      "Epoch 33, Step 11 Loss = 0.7759764194488525\n",
      "Epoch 33, Step 12 Loss = 0.9616830348968506\n",
      "Epoch 33, Step 13 Loss = 0.8229840993881226\n",
      "Epoch 33, Step 14 Loss = 0.5539583563804626\n",
      "Epoch 33, Step 15 Loss = 0.7449449300765991\n",
      "Epoch 33, Step 16 Loss = 0.5511244535446167\n",
      "Epoch 33, Step 17 Loss = 0.21076521277427673\n",
      "Epoch 33, Step 18 Loss = 0.4409329295158386\n",
      "Epoch 33, Step 19 Loss = 0.6933004260063171\n",
      "Epoch 33, Step 20 Loss = 0.7284040451049805\n",
      "Epoch 33, Step 21 Loss = 0.6668967008590698\n",
      "Epoch 33, Step 22 Loss = 0.7002242803573608\n",
      "Epoch 33, Step 23 Loss = 0.3448331356048584\n",
      "Epoch 33, Step 24 Loss = 0.9516314268112183\n",
      "Epoch 33, Step 25 Loss = 0.5317582488059998\n",
      "Epoch 33, Step 26 Loss = 0.4472758173942566\n",
      "Epoch 33, Step 27 Loss = 0.8607431650161743\n",
      "Epoch 33, Step 28 Loss = 0.9300462603569031\n",
      "Epoch 33, Step 29 Loss = 0.8436911106109619\n",
      "Epoch 33, Step 30 Loss = 0.6407572627067566\n",
      "Epoch 33, Step 31 Loss = 0.9485528469085693\n",
      "Epoch 33, Step 32 Loss = 1.07816743850708\n",
      "Epoch 33, Step 33 Loss = 0.5077735185623169\n",
      "Epoch 33, Step 34 Loss = 0.55084228515625\n",
      "Epoch 33, Step 35 Loss = 0.5909416675567627\n",
      "Epoch 33, Step 36 Loss = 0.5735852122306824\n",
      "Epoch 33, Step 37 Loss = 0.9069307446479797\n",
      "Epoch 33, Step 38 Loss = 0.8832322955131531\n",
      "Epoch 33, Step 39 Loss = 0.7181926965713501\n",
      "Epoch 33, Step 40 Loss = 0.9125514030456543\n",
      "Epoch 33, Step 41 Loss = 0.8754227161407471\n",
      "Training loss: 0.7130\n",
      "Epoch 34/50\n",
      "Epoch 34, Step 1 Loss = 0.7040985822677612\n",
      "Epoch 34, Step 2 Loss = 0.41336560249328613\n",
      "Epoch 34, Step 3 Loss = 0.5693118572235107\n",
      "Epoch 34, Step 4 Loss = 0.5364190340042114\n",
      "Epoch 34, Step 5 Loss = 0.34591466188430786\n",
      "Epoch 34, Step 6 Loss = 0.7196243405342102\n",
      "Epoch 34, Step 7 Loss = 0.5659747123718262\n",
      "Epoch 34, Step 8 Loss = 0.5512961149215698\n",
      "Epoch 34, Step 9 Loss = 0.6306192874908447\n",
      "Epoch 34, Step 10 Loss = 0.6324307322502136\n",
      "Epoch 34, Step 11 Loss = 0.4857143759727478\n",
      "Epoch 34, Step 12 Loss = 0.6908189058303833\n",
      "Epoch 34, Step 13 Loss = 0.7493272423744202\n",
      "Epoch 34, Step 14 Loss = 0.30929118394851685\n",
      "Epoch 34, Step 15 Loss = 0.47230133414268494\n",
      "Epoch 34, Step 16 Loss = 0.9841249585151672\n",
      "Epoch 34, Step 17 Loss = 0.904220700263977\n",
      "Epoch 34, Step 18 Loss = 0.6631613969802856\n",
      "Epoch 34, Step 19 Loss = 0.7238321304321289\n",
      "Epoch 34, Step 20 Loss = 0.4269334673881531\n",
      "Epoch 34, Step 21 Loss = 0.9002648591995239\n",
      "Epoch 34, Step 22 Loss = 0.6025575995445251\n",
      "Epoch 34, Step 23 Loss = 0.8162715435028076\n",
      "Epoch 34, Step 24 Loss = 0.763804018497467\n",
      "Epoch 34, Step 25 Loss = 0.5910195708274841\n",
      "Epoch 34, Step 26 Loss = 0.5914409756660461\n",
      "Epoch 34, Step 27 Loss = 1.2806954383850098\n",
      "Epoch 34, Step 28 Loss = 0.893223762512207\n",
      "Epoch 34, Step 29 Loss = 1.189164638519287\n",
      "Epoch 34, Step 30 Loss = 1.016411542892456\n",
      "Epoch 34, Step 31 Loss = 0.8640241026878357\n",
      "Epoch 34, Step 32 Loss = 0.6128184795379639\n",
      "Epoch 34, Step 33 Loss = 1.224993348121643\n",
      "Epoch 34, Step 34 Loss = 0.3926452398300171\n",
      "Epoch 34, Step 35 Loss = 0.5146263241767883\n",
      "Epoch 34, Step 36 Loss = 0.5752359628677368\n",
      "Epoch 34, Step 37 Loss = 0.8435490727424622\n",
      "Epoch 34, Step 38 Loss = 0.46983906626701355\n",
      "Epoch 34, Step 39 Loss = 0.6537963151931763\n",
      "Epoch 34, Step 40 Loss = 1.1084039211273193\n",
      "Epoch 34, Step 41 Loss = 0.697861909866333\n",
      "Training loss: 0.6995\n",
      "Epoch 35/50\n",
      "Epoch 35, Step 1 Loss = 0.47760510444641113\n",
      "Epoch 35, Step 2 Loss = 0.6830283403396606\n",
      "Epoch 35, Step 3 Loss = 0.4386857748031616\n",
      "Epoch 35, Step 4 Loss = 0.6747066378593445\n",
      "Epoch 35, Step 5 Loss = 0.48676592111587524\n",
      "Epoch 35, Step 6 Loss = 0.8096015453338623\n",
      "Epoch 35, Step 7 Loss = 0.3365391492843628\n",
      "Epoch 35, Step 8 Loss = 0.9610314965248108\n",
      "Epoch 35, Step 9 Loss = 0.2942391037940979\n",
      "Epoch 35, Step 10 Loss = 0.7108007073402405\n",
      "Epoch 35, Step 11 Loss = 0.6200035810470581\n",
      "Epoch 35, Step 12 Loss = 0.7695800065994263\n",
      "Epoch 35, Step 13 Loss = 0.6330828666687012\n",
      "Epoch 35, Step 14 Loss = 0.3551021218299866\n",
      "Epoch 35, Step 15 Loss = 0.8829396367073059\n",
      "Epoch 35, Step 16 Loss = 0.6474813222885132\n",
      "Epoch 35, Step 17 Loss = 0.5891302227973938\n",
      "Epoch 35, Step 18 Loss = 0.5686817765235901\n",
      "Epoch 35, Step 19 Loss = 0.5074267387390137\n",
      "Epoch 35, Step 20 Loss = 1.1783638000488281\n",
      "Epoch 35, Step 21 Loss = 0.2985978126525879\n",
      "Epoch 35, Step 22 Loss = 0.7431966662406921\n",
      "Epoch 35, Step 23 Loss = 0.8729086518287659\n",
      "Epoch 35, Step 24 Loss = 0.7434945702552795\n",
      "Epoch 35, Step 25 Loss = 1.1165968179702759\n",
      "Epoch 35, Step 26 Loss = 0.9020033478736877\n",
      "Epoch 35, Step 27 Loss = 0.6396799087524414\n",
      "Epoch 35, Step 28 Loss = 0.8267083168029785\n",
      "Epoch 35, Step 29 Loss = 0.43832167983055115\n",
      "Epoch 35, Step 30 Loss = 0.3240883946418762\n",
      "Epoch 35, Step 31 Loss = 1.5320441722869873\n",
      "Epoch 35, Step 32 Loss = 1.0113292932510376\n",
      "Epoch 35, Step 33 Loss = 0.6245856285095215\n",
      "Epoch 35, Step 34 Loss = 1.0365885496139526\n",
      "Epoch 35, Step 35 Loss = 0.6659572124481201\n",
      "Epoch 35, Step 36 Loss = 1.2937061786651611\n",
      "Epoch 35, Step 37 Loss = 0.7661644220352173\n",
      "Epoch 35, Step 38 Loss = 1.1380622386932373\n",
      "Epoch 35, Step 39 Loss = 0.5019537210464478\n",
      "Epoch 35, Step 40 Loss = 1.1602015495300293\n",
      "Epoch 35, Step 41 Loss = 0.8252807259559631\n",
      "Training loss: 0.7338\n",
      "Epoch 36/50\n",
      "Epoch 36, Step 1 Loss = 0.9287229776382446\n",
      "Epoch 36, Step 2 Loss = 0.4286159873008728\n",
      "Epoch 36, Step 3 Loss = 0.6544703245162964\n",
      "Epoch 36, Step 4 Loss = 0.6025395393371582\n",
      "Epoch 36, Step 5 Loss = 0.13857059180736542\n",
      "Epoch 36, Step 6 Loss = 0.6395251750946045\n",
      "Epoch 36, Step 7 Loss = 0.47481486201286316\n",
      "Epoch 36, Step 8 Loss = 0.8816623091697693\n",
      "Epoch 36, Step 9 Loss = 0.8305234909057617\n",
      "Epoch 36, Step 10 Loss = 0.4580632448196411\n",
      "Epoch 36, Step 11 Loss = 0.34827885031700134\n",
      "Epoch 36, Step 12 Loss = 0.33666422963142395\n",
      "Epoch 36, Step 13 Loss = 0.46002197265625\n",
      "Epoch 36, Step 14 Loss = 0.5689723491668701\n",
      "Epoch 36, Step 15 Loss = 0.4959728717803955\n",
      "Epoch 36, Step 16 Loss = 0.8123180866241455\n",
      "Epoch 36, Step 17 Loss = 0.14237657189369202\n",
      "Epoch 36, Step 18 Loss = 0.7664170861244202\n",
      "Epoch 36, Step 19 Loss = 0.26420432329177856\n",
      "Epoch 36, Step 20 Loss = 0.460326611995697\n",
      "Epoch 36, Step 21 Loss = 0.6792809963226318\n",
      "Epoch 36, Step 22 Loss = 0.6190203428268433\n",
      "Epoch 36, Step 23 Loss = 0.6173267364501953\n",
      "Epoch 36, Step 24 Loss = 0.6785248517990112\n",
      "Epoch 36, Step 25 Loss = 0.948076069355011\n",
      "Epoch 36, Step 26 Loss = 0.9588513374328613\n",
      "Epoch 36, Step 27 Loss = 0.6406652331352234\n",
      "Epoch 36, Step 28 Loss = 0.7807220220565796\n",
      "Epoch 36, Step 29 Loss = 0.7100073099136353\n",
      "Epoch 36, Step 30 Loss = 0.5270320773124695\n",
      "Epoch 36, Step 31 Loss = 1.0393867492675781\n",
      "Epoch 36, Step 32 Loss = 0.721128523349762\n",
      "Epoch 36, Step 33 Loss = 0.6419361233711243\n",
      "Epoch 36, Step 34 Loss = 0.7769132256507874\n",
      "Epoch 36, Step 35 Loss = 0.8668177127838135\n",
      "Epoch 36, Step 36 Loss = 0.8499858379364014\n",
      "Epoch 36, Step 37 Loss = 1.0874086618423462\n",
      "Epoch 36, Step 38 Loss = 0.7715031504631042\n",
      "Epoch 36, Step 39 Loss = 0.6853827238082886\n",
      "Epoch 36, Step 40 Loss = 0.611075758934021\n",
      "Epoch 36, Step 41 Loss = 0.8764336109161377\n",
      "Training loss: 0.6532\n",
      "Epoch 37/50\n",
      "Epoch 37, Step 1 Loss = 0.45080867409706116\n",
      "Epoch 37, Step 2 Loss = 0.3141046464443207\n",
      "Epoch 37, Step 3 Loss = 0.9296969771385193\n",
      "Epoch 37, Step 4 Loss = 0.3210161328315735\n",
      "Epoch 37, Step 5 Loss = 0.4735663831233978\n",
      "Epoch 37, Step 6 Loss = 0.676971435546875\n",
      "Epoch 37, Step 7 Loss = 0.6614030599594116\n",
      "Epoch 37, Step 8 Loss = 0.3318169116973877\n",
      "Epoch 37, Step 9 Loss = 0.7737083435058594\n",
      "Epoch 37, Step 10 Loss = 0.8063405156135559\n",
      "Epoch 37, Step 11 Loss = 0.6999821662902832\n",
      "Epoch 37, Step 12 Loss = 0.675891637802124\n",
      "Epoch 37, Step 13 Loss = 0.8204079866409302\n",
      "Epoch 37, Step 14 Loss = 0.6811233162879944\n",
      "Epoch 37, Step 15 Loss = 1.0780690908432007\n",
      "Epoch 37, Step 16 Loss = 0.4484531283378601\n",
      "Epoch 37, Step 17 Loss = 0.6738325953483582\n",
      "Epoch 37, Step 18 Loss = 0.4734708368778229\n",
      "Epoch 37, Step 19 Loss = 0.670148491859436\n",
      "Epoch 37, Step 20 Loss = 0.9957923293113708\n",
      "Epoch 37, Step 21 Loss = 0.6175721287727356\n",
      "Epoch 37, Step 22 Loss = 0.7620834112167358\n",
      "Epoch 37, Step 23 Loss = 0.5314059853553772\n",
      "Epoch 37, Step 24 Loss = 0.26471763849258423\n",
      "Epoch 37, Step 25 Loss = 0.4531151354312897\n",
      "Epoch 37, Step 26 Loss = 0.827213704586029\n",
      "Epoch 37, Step 27 Loss = 0.7661671042442322\n",
      "Epoch 37, Step 28 Loss = 0.7746183276176453\n",
      "Epoch 37, Step 29 Loss = 0.46849325299263\n",
      "Epoch 37, Step 30 Loss = 0.9602156281471252\n",
      "Epoch 37, Step 31 Loss = 0.9435863494873047\n",
      "Epoch 37, Step 32 Loss = 0.6411545276641846\n",
      "Epoch 37, Step 33 Loss = 0.8907886743545532\n",
      "Epoch 37, Step 34 Loss = 0.9695249795913696\n",
      "Epoch 37, Step 35 Loss = 1.18254554271698\n",
      "Epoch 37, Step 36 Loss = 0.8746691346168518\n",
      "Epoch 37, Step 37 Loss = 0.539535641670227\n",
      "Epoch 37, Step 38 Loss = 1.3893132209777832\n",
      "Epoch 37, Step 39 Loss = 1.1061161756515503\n",
      "Epoch 37, Step 40 Loss = 0.6929827928543091\n",
      "Epoch 37, Step 41 Loss = 1.0673432350158691\n",
      "Training loss: 0.7239\n",
      "Epoch 38/50\n",
      "Epoch 38, Step 1 Loss = 0.5339126586914062\n",
      "Epoch 38, Step 2 Loss = 0.8514453172683716\n",
      "Epoch 38, Step 3 Loss = 0.3589193820953369\n",
      "Epoch 38, Step 4 Loss = 0.556409478187561\n",
      "Epoch 38, Step 5 Loss = 0.6576540470123291\n",
      "Epoch 38, Step 6 Loss = 0.6151766777038574\n",
      "Epoch 38, Step 7 Loss = 0.6719062328338623\n",
      "Epoch 38, Step 8 Loss = 0.597325325012207\n",
      "Epoch 38, Step 9 Loss = 0.7199094891548157\n",
      "Epoch 38, Step 10 Loss = 0.5713533759117126\n",
      "Epoch 38, Step 11 Loss = 0.32235825061798096\n",
      "Epoch 38, Step 12 Loss = 0.5022187232971191\n",
      "Epoch 38, Step 13 Loss = 0.3951234221458435\n",
      "Epoch 38, Step 14 Loss = 0.5158470869064331\n",
      "Epoch 38, Step 15 Loss = 0.5317295789718628\n",
      "Epoch 38, Step 16 Loss = 0.7801372408866882\n",
      "Epoch 38, Step 17 Loss = 0.9838098883628845\n",
      "Epoch 38, Step 18 Loss = 0.6000288128852844\n",
      "Epoch 38, Step 19 Loss = 0.505068302154541\n",
      "Epoch 38, Step 20 Loss = 0.5688052177429199\n",
      "Epoch 38, Step 21 Loss = 0.9291436672210693\n",
      "Epoch 38, Step 22 Loss = 0.8879345059394836\n",
      "Epoch 38, Step 23 Loss = 0.8831629753112793\n",
      "Epoch 38, Step 24 Loss = 0.6554341316223145\n",
      "Epoch 38, Step 25 Loss = 0.5024108290672302\n",
      "Epoch 38, Step 26 Loss = 0.9091567993164062\n",
      "Epoch 38, Step 27 Loss = 0.8355690836906433\n",
      "Epoch 38, Step 28 Loss = 0.9044769406318665\n",
      "Epoch 38, Step 29 Loss = 0.5914760828018188\n",
      "Epoch 38, Step 30 Loss = 0.5100759267807007\n",
      "Epoch 38, Step 31 Loss = 1.1450538635253906\n",
      "Epoch 38, Step 32 Loss = 0.6362961530685425\n",
      "Epoch 38, Step 33 Loss = 1.2004587650299072\n",
      "Epoch 38, Step 34 Loss = 0.4936596155166626\n",
      "Epoch 38, Step 35 Loss = 0.9161277413368225\n",
      "Epoch 38, Step 36 Loss = 0.6802051067352295\n",
      "Epoch 38, Step 37 Loss = 0.8339720964431763\n",
      "Epoch 38, Step 38 Loss = 0.8758600950241089\n",
      "Epoch 38, Step 39 Loss = 0.9040158987045288\n",
      "Epoch 38, Step 40 Loss = 0.7272517681121826\n",
      "Epoch 38, Step 41 Loss = 0.580627977848053\n",
      "Training loss: 0.6937\n",
      "Epoch 39/50\n",
      "Epoch 39, Step 1 Loss = 0.5449870824813843\n",
      "Epoch 39, Step 2 Loss = 0.7545762062072754\n",
      "Epoch 39, Step 3 Loss = 0.5709904432296753\n",
      "Epoch 39, Step 4 Loss = 0.5782196521759033\n",
      "Epoch 39, Step 5 Loss = 0.5566233992576599\n",
      "Epoch 39, Step 6 Loss = 0.7621914148330688\n",
      "Epoch 39, Step 7 Loss = 0.2952711880207062\n",
      "Epoch 39, Step 8 Loss = 0.14772728085517883\n",
      "Epoch 39, Step 9 Loss = 0.7314237952232361\n",
      "Epoch 39, Step 10 Loss = 0.933098554611206\n",
      "Epoch 39, Step 11 Loss = 0.7012547254562378\n",
      "Epoch 39, Step 12 Loss = 0.4484742283821106\n",
      "Epoch 39, Step 13 Loss = 0.4924323558807373\n",
      "Epoch 39, Step 14 Loss = 0.5779774188995361\n",
      "Epoch 39, Step 15 Loss = 0.6465986967086792\n",
      "Epoch 39, Step 16 Loss = 0.6417077779769897\n",
      "Epoch 39, Step 17 Loss = 0.3855559825897217\n",
      "Epoch 39, Step 18 Loss = 1.1542291641235352\n",
      "Epoch 39, Step 19 Loss = 0.5946074724197388\n",
      "Epoch 39, Step 20 Loss = 0.581673800945282\n",
      "Epoch 39, Step 21 Loss = 0.4863682687282562\n",
      "Epoch 39, Step 22 Loss = 0.7467398643493652\n",
      "Epoch 39, Step 23 Loss = 0.5791952610015869\n",
      "Epoch 39, Step 24 Loss = 0.6083045601844788\n",
      "Epoch 39, Step 25 Loss = 0.6064837574958801\n",
      "Epoch 39, Step 26 Loss = 0.9520926475524902\n",
      "Epoch 39, Step 27 Loss = 0.6527225971221924\n",
      "Epoch 39, Step 28 Loss = 0.8878482580184937\n",
      "Epoch 39, Step 29 Loss = 1.1822741031646729\n",
      "Epoch 39, Step 30 Loss = 0.4476524591445923\n",
      "Epoch 39, Step 31 Loss = 0.38090407848358154\n",
      "Epoch 39, Step 32 Loss = 0.4449988901615143\n",
      "Epoch 39, Step 33 Loss = 0.8206332921981812\n",
      "Epoch 39, Step 34 Loss = 0.49674761295318604\n",
      "Epoch 39, Step 35 Loss = 0.4743898808956146\n",
      "Epoch 39, Step 36 Loss = 1.0967142581939697\n",
      "Epoch 39, Step 37 Loss = 1.1295405626296997\n",
      "Epoch 39, Step 38 Loss = 1.0613418817520142\n",
      "Epoch 39, Step 39 Loss = 0.4858855605125427\n",
      "Epoch 39, Step 40 Loss = 0.771533727645874\n",
      "Epoch 39, Step 41 Loss = 1.329984426498413\n",
      "Training loss: 0.6766\n",
      "Epoch 40/50\n",
      "Epoch 40, Step 1 Loss = 0.47398489713668823\n",
      "Epoch 40, Step 2 Loss = 0.7635666131973267\n",
      "Epoch 40, Step 3 Loss = 0.7977292537689209\n",
      "Epoch 40, Step 4 Loss = 0.5820976495742798\n",
      "Epoch 40, Step 5 Loss = 0.6618762612342834\n",
      "Epoch 40, Step 6 Loss = 0.602142333984375\n",
      "Epoch 40, Step 7 Loss = 0.5284123420715332\n",
      "Epoch 40, Step 8 Loss = 0.9293440580368042\n",
      "Epoch 40, Step 9 Loss = 0.5368322730064392\n",
      "Epoch 40, Step 10 Loss = 0.6094717979431152\n",
      "Epoch 40, Step 11 Loss = 0.7551605105400085\n",
      "Epoch 40, Step 12 Loss = 0.5405257940292358\n",
      "Epoch 40, Step 13 Loss = 0.5585504770278931\n",
      "Epoch 40, Step 14 Loss = 0.5965860486030579\n",
      "Epoch 40, Step 15 Loss = 1.1763463020324707\n",
      "Epoch 40, Step 16 Loss = 0.512952446937561\n",
      "Epoch 40, Step 17 Loss = 0.2555907666683197\n",
      "Epoch 40, Step 18 Loss = 0.6137667894363403\n",
      "Epoch 40, Step 19 Loss = 0.7571356296539307\n",
      "Epoch 40, Step 20 Loss = 0.6543834209442139\n",
      "Epoch 40, Step 21 Loss = 0.7195264101028442\n",
      "Epoch 40, Step 22 Loss = 0.40618202090263367\n",
      "Epoch 40, Step 23 Loss = 0.9084782600402832\n",
      "Epoch 40, Step 24 Loss = 0.5728850960731506\n",
      "Epoch 40, Step 25 Loss = 1.0699059963226318\n",
      "Epoch 40, Step 26 Loss = 1.2432053089141846\n",
      "Epoch 40, Step 27 Loss = 0.7829718589782715\n",
      "Epoch 40, Step 28 Loss = 0.7331425547599792\n",
      "Epoch 40, Step 29 Loss = 0.6315820217132568\n",
      "Epoch 40, Step 30 Loss = 0.9831476211547852\n",
      "Epoch 40, Step 31 Loss = 0.6522273421287537\n",
      "Epoch 40, Step 32 Loss = 1.1218496561050415\n",
      "Epoch 40, Step 33 Loss = 0.5833686590194702\n",
      "Epoch 40, Step 34 Loss = 0.8788854479789734\n",
      "Epoch 40, Step 35 Loss = 0.6737947463989258\n",
      "Epoch 40, Step 36 Loss = 0.5206072330474854\n",
      "Epoch 40, Step 37 Loss = 0.4011281132698059\n",
      "Epoch 40, Step 38 Loss = 0.6611921787261963\n",
      "Epoch 40, Step 39 Loss = 0.7278804779052734\n",
      "Epoch 40, Step 40 Loss = 1.0253312587738037\n",
      "Epoch 40, Step 41 Loss = 0.9117074012756348\n",
      "Training loss: 0.7101\n",
      "Epoch 41/50\n",
      "Epoch 41, Step 1 Loss = 0.7426254749298096\n",
      "Epoch 41, Step 2 Loss = 0.4528173804283142\n",
      "Epoch 41, Step 3 Loss = 0.27338552474975586\n",
      "Epoch 41, Step 4 Loss = 1.1145375967025757\n",
      "Epoch 41, Step 5 Loss = 0.6472014784812927\n",
      "Epoch 41, Step 6 Loss = 0.8147615790367126\n",
      "Epoch 41, Step 7 Loss = 0.5450953841209412\n",
      "Epoch 41, Step 8 Loss = 0.4832240343093872\n",
      "Epoch 41, Step 9 Loss = 0.4490485191345215\n",
      "Epoch 41, Step 10 Loss = 0.22060441970825195\n",
      "Epoch 41, Step 11 Loss = 0.4716000556945801\n",
      "Epoch 41, Step 12 Loss = 0.3494430184364319\n",
      "Epoch 41, Step 13 Loss = 1.018640398979187\n",
      "Epoch 41, Step 14 Loss = 0.7828095555305481\n",
      "Epoch 41, Step 15 Loss = 0.4084048569202423\n",
      "Epoch 41, Step 16 Loss = 0.6992495059967041\n",
      "Epoch 41, Step 17 Loss = 0.828724205493927\n",
      "Epoch 41, Step 18 Loss = 0.694927453994751\n",
      "Epoch 41, Step 19 Loss = 0.897027313709259\n",
      "Epoch 41, Step 20 Loss = 0.48088109493255615\n",
      "Epoch 41, Step 21 Loss = 0.6555620431900024\n",
      "Epoch 41, Step 22 Loss = 0.7298879623413086\n",
      "Epoch 41, Step 23 Loss = 0.6974611282348633\n",
      "Epoch 41, Step 24 Loss = 0.4232197403907776\n",
      "Epoch 41, Step 25 Loss = 0.9055272340774536\n",
      "Epoch 41, Step 26 Loss = 0.8032904863357544\n",
      "Epoch 41, Step 27 Loss = 0.8032423257827759\n",
      "Epoch 41, Step 28 Loss = 0.659794807434082\n",
      "Epoch 41, Step 29 Loss = 0.9943528175354004\n",
      "Epoch 41, Step 30 Loss = 1.2659275531768799\n",
      "Epoch 41, Step 31 Loss = 1.083744764328003\n",
      "Epoch 41, Step 32 Loss = 0.688522219657898\n",
      "Epoch 41, Step 33 Loss = 0.6177573204040527\n",
      "Epoch 41, Step 34 Loss = 1.2135804891586304\n",
      "Epoch 41, Step 35 Loss = 0.7957034111022949\n",
      "Epoch 41, Step 36 Loss = 1.1731619834899902\n",
      "Epoch 41, Step 37 Loss = 0.7987294793128967\n",
      "Epoch 41, Step 38 Loss = 0.5481323003768921\n",
      "Epoch 41, Step 39 Loss = 0.48232755064964294\n",
      "Epoch 41, Step 40 Loss = 0.749830961227417\n",
      "Epoch 41, Step 41 Loss = 0.8133829832077026\n",
      "Training loss: 0.7141\n",
      "Epoch 42/50\n",
      "Epoch 42, Step 1 Loss = 0.18896126747131348\n",
      "Epoch 42, Step 2 Loss = 0.7280318737030029\n",
      "Epoch 42, Step 3 Loss = 0.3893035650253296\n",
      "Epoch 42, Step 4 Loss = 0.7260746955871582\n",
      "Epoch 42, Step 5 Loss = 0.37740257382392883\n",
      "Epoch 42, Step 6 Loss = 0.48084020614624023\n",
      "Epoch 42, Step 7 Loss = 0.43903788924217224\n",
      "Epoch 42, Step 8 Loss = 0.7934222221374512\n",
      "Epoch 42, Step 9 Loss = 0.411462664604187\n",
      "Epoch 42, Step 10 Loss = 0.44363465905189514\n",
      "Epoch 42, Step 11 Loss = 0.5042121410369873\n",
      "Epoch 42, Step 12 Loss = 0.5149654746055603\n",
      "Epoch 42, Step 13 Loss = 0.4318259060382843\n",
      "Epoch 42, Step 14 Loss = 0.6625046730041504\n",
      "Epoch 42, Step 15 Loss = 1.0771254301071167\n",
      "Epoch 42, Step 16 Loss = 1.11518132686615\n",
      "Epoch 42, Step 17 Loss = 0.5615501403808594\n",
      "Epoch 42, Step 18 Loss = 0.9603430032730103\n",
      "Epoch 42, Step 19 Loss = 0.4358537495136261\n",
      "Epoch 42, Step 20 Loss = 1.0978367328643799\n",
      "Epoch 42, Step 21 Loss = 0.7691879868507385\n",
      "Epoch 42, Step 22 Loss = 0.4357459545135498\n",
      "Epoch 42, Step 23 Loss = 1.1029213666915894\n",
      "Epoch 42, Step 24 Loss = 0.8121088743209839\n",
      "Epoch 42, Step 25 Loss = 0.9684486389160156\n",
      "Epoch 42, Step 26 Loss = 0.5589677691459656\n",
      "Epoch 42, Step 27 Loss = 0.7329678535461426\n",
      "Epoch 42, Step 28 Loss = 0.8168090581893921\n",
      "Epoch 42, Step 29 Loss = 0.7008389234542847\n",
      "Epoch 42, Step 30 Loss = 0.34289658069610596\n",
      "Epoch 42, Step 31 Loss = 0.6149953603744507\n",
      "Epoch 42, Step 32 Loss = 0.7333365678787231\n",
      "Epoch 42, Step 33 Loss = 0.4221797585487366\n",
      "Epoch 42, Step 34 Loss = 0.4823113977909088\n",
      "Epoch 42, Step 35 Loss = 0.6891993880271912\n",
      "Epoch 42, Step 36 Loss = 1.1992107629776\n",
      "Epoch 42, Step 37 Loss = 1.014587640762329\n",
      "Epoch 42, Step 38 Loss = 0.9744032621383667\n",
      "Epoch 42, Step 39 Loss = 0.3958364427089691\n",
      "Epoch 42, Step 40 Loss = 1.2289106845855713\n",
      "Epoch 42, Step 41 Loss = 1.0068292617797852\n",
      "Training loss: 0.6913\n",
      "Epoch 43/50\n",
      "Epoch 43, Step 1 Loss = 0.7517631649971008\n",
      "Epoch 43, Step 2 Loss = 0.6860010027885437\n",
      "Epoch 43, Step 3 Loss = 0.7169508934020996\n",
      "Epoch 43, Step 4 Loss = 0.5836647748947144\n",
      "Epoch 43, Step 5 Loss = 0.7310794591903687\n",
      "Epoch 43, Step 6 Loss = 0.5019257664680481\n",
      "Epoch 43, Step 7 Loss = 0.28725314140319824\n",
      "Epoch 43, Step 8 Loss = 0.5494832396507263\n",
      "Epoch 43, Step 9 Loss = 0.6542160511016846\n",
      "Epoch 43, Step 10 Loss = 0.791318416595459\n",
      "Epoch 43, Step 11 Loss = 0.4133208692073822\n",
      "Epoch 43, Step 12 Loss = 0.5746089220046997\n",
      "Epoch 43, Step 13 Loss = 0.6845263242721558\n",
      "Epoch 43, Step 14 Loss = 0.3486518859863281\n",
      "Epoch 43, Step 15 Loss = 0.23085618019104004\n",
      "Epoch 43, Step 16 Loss = 0.5757492780685425\n",
      "Epoch 43, Step 17 Loss = 0.5734782218933105\n",
      "Epoch 43, Step 18 Loss = 1.183337688446045\n",
      "Epoch 43, Step 19 Loss = 0.47196531295776367\n",
      "Epoch 43, Step 20 Loss = 0.5966466069221497\n",
      "Epoch 43, Step 21 Loss = 0.7618163824081421\n",
      "Epoch 43, Step 22 Loss = 0.5685698986053467\n",
      "Epoch 43, Step 23 Loss = 0.4865853488445282\n",
      "Epoch 43, Step 24 Loss = 0.8800857067108154\n",
      "Epoch 43, Step 25 Loss = 0.5490574836730957\n",
      "Epoch 43, Step 26 Loss = 0.7440507411956787\n",
      "Epoch 43, Step 27 Loss = 0.7646279335021973\n",
      "Epoch 43, Step 28 Loss = 1.0104832649230957\n",
      "Epoch 43, Step 29 Loss = 0.5848846435546875\n",
      "Epoch 43, Step 30 Loss = 0.5776108503341675\n",
      "Epoch 43, Step 31 Loss = 0.7196124792098999\n",
      "Epoch 43, Step 32 Loss = 0.37258899211883545\n",
      "Epoch 43, Step 33 Loss = 1.277209758758545\n",
      "Epoch 43, Step 34 Loss = 0.8960859775543213\n",
      "Epoch 43, Step 35 Loss = 1.130645513534546\n",
      "Epoch 43, Step 36 Loss = 0.6222028732299805\n",
      "Epoch 43, Step 37 Loss = 0.7337273955345154\n",
      "Epoch 43, Step 38 Loss = 0.8781793117523193\n",
      "Epoch 43, Step 39 Loss = 0.6040830016136169\n",
      "Epoch 43, Step 40 Loss = 1.0517722368240356\n",
      "Epoch 43, Step 41 Loss = 0.8674737811088562\n",
      "Training loss: 0.6826\n",
      "Epoch 44/50\n",
      "Epoch 44, Step 1 Loss = 1.0063841342926025\n",
      "Epoch 44, Step 2 Loss = 0.767174482345581\n",
      "Epoch 44, Step 3 Loss = 0.5734058618545532\n",
      "Epoch 44, Step 4 Loss = 0.6090506315231323\n",
      "Epoch 44, Step 5 Loss = 0.805670440196991\n",
      "Epoch 44, Step 6 Loss = 0.5839628577232361\n",
      "Epoch 44, Step 7 Loss = 0.3394129276275635\n",
      "Epoch 44, Step 8 Loss = 0.3589895963668823\n",
      "Epoch 44, Step 9 Loss = 0.5270039439201355\n",
      "Epoch 44, Step 10 Loss = 0.5465265512466431\n",
      "Epoch 44, Step 11 Loss = 0.495360404253006\n",
      "Epoch 44, Step 12 Loss = 0.832815408706665\n",
      "Epoch 44, Step 13 Loss = 0.36905670166015625\n",
      "Epoch 44, Step 14 Loss = 0.5561335682868958\n",
      "Epoch 44, Step 15 Loss = 0.6488534212112427\n",
      "Epoch 44, Step 16 Loss = 0.6951999664306641\n",
      "Epoch 44, Step 17 Loss = 0.6915713548660278\n",
      "Epoch 44, Step 18 Loss = 0.468879759311676\n",
      "Epoch 44, Step 19 Loss = 0.6000461578369141\n",
      "Epoch 44, Step 20 Loss = 0.9143304824829102\n",
      "Epoch 44, Step 21 Loss = 0.8127065896987915\n",
      "Epoch 44, Step 22 Loss = 0.533947229385376\n",
      "Epoch 44, Step 23 Loss = 0.7196999788284302\n",
      "Epoch 44, Step 24 Loss = 0.5843943357467651\n",
      "Epoch 44, Step 25 Loss = 0.7215520739555359\n",
      "Epoch 44, Step 26 Loss = 0.5163231492042542\n",
      "Epoch 44, Step 27 Loss = 0.8835340738296509\n",
      "Epoch 44, Step 28 Loss = 0.8301687836647034\n",
      "Epoch 44, Step 29 Loss = 0.8916476368904114\n",
      "Epoch 44, Step 30 Loss = 0.37449419498443604\n",
      "Epoch 44, Step 31 Loss = 0.7049722671508789\n",
      "Epoch 44, Step 32 Loss = 0.5380967855453491\n",
      "Epoch 44, Step 33 Loss = 0.6716420650482178\n",
      "Epoch 44, Step 34 Loss = 0.3923022747039795\n",
      "Epoch 44, Step 35 Loss = 0.28567996621131897\n",
      "Epoch 44, Step 36 Loss = 0.572147011756897\n",
      "Epoch 44, Step 37 Loss = 0.9472424983978271\n",
      "Epoch 44, Step 38 Loss = 0.8892595767974854\n",
      "Epoch 44, Step 39 Loss = 0.6178771257400513\n",
      "Epoch 44, Step 40 Loss = 1.076704740524292\n",
      "Epoch 44, Step 41 Loss = 0.5833075642585754\n",
      "Training loss: 0.6473\n",
      "Epoch 45/50\n",
      "Epoch 45, Step 1 Loss = 0.29413658380508423\n",
      "Epoch 45, Step 2 Loss = 0.45575252175331116\n",
      "Epoch 45, Step 3 Loss = 0.30226340889930725\n",
      "Epoch 45, Step 4 Loss = 0.3847050070762634\n",
      "Epoch 45, Step 5 Loss = 0.37255072593688965\n",
      "Epoch 45, Step 6 Loss = 0.42156362533569336\n",
      "Epoch 45, Step 7 Loss = 0.710885763168335\n",
      "Epoch 45, Step 8 Loss = 0.6332497596740723\n",
      "Epoch 45, Step 9 Loss = 0.8857806324958801\n",
      "Epoch 45, Step 10 Loss = 0.5483258366584778\n",
      "Epoch 45, Step 11 Loss = 0.5566515326499939\n",
      "Epoch 45, Step 12 Loss = 0.3202000558376312\n",
      "Epoch 45, Step 13 Loss = 0.6423018574714661\n",
      "Epoch 45, Step 14 Loss = 0.8602747917175293\n",
      "Epoch 45, Step 15 Loss = 1.0290580987930298\n",
      "Epoch 45, Step 16 Loss = 0.8728766441345215\n",
      "Epoch 45, Step 17 Loss = 1.133155107498169\n",
      "Epoch 45, Step 18 Loss = 0.7307056188583374\n",
      "Epoch 45, Step 19 Loss = 0.41031405329704285\n",
      "Epoch 45, Step 20 Loss = 0.6054811477661133\n",
      "Epoch 45, Step 21 Loss = 0.591697633266449\n",
      "Epoch 45, Step 22 Loss = 0.9341933131217957\n",
      "Epoch 45, Step 23 Loss = 0.4515122175216675\n",
      "Epoch 45, Step 24 Loss = 0.7629911303520203\n",
      "Epoch 45, Step 25 Loss = 0.3284851312637329\n",
      "Epoch 45, Step 26 Loss = 0.6779488921165466\n",
      "Epoch 45, Step 27 Loss = 1.004145622253418\n",
      "Epoch 45, Step 28 Loss = 0.987823486328125\n",
      "Epoch 45, Step 29 Loss = 0.6614652872085571\n",
      "Epoch 45, Step 30 Loss = 0.353060245513916\n",
      "Epoch 45, Step 31 Loss = 1.32371187210083\n",
      "Epoch 45, Step 32 Loss = 0.5402356386184692\n",
      "Epoch 45, Step 33 Loss = 0.9151719808578491\n",
      "Epoch 45, Step 34 Loss = 0.6617058515548706\n",
      "Epoch 45, Step 35 Loss = 0.6992228031158447\n",
      "Epoch 45, Step 36 Loss = 0.6376121044158936\n",
      "Epoch 45, Step 37 Loss = 0.4130373001098633\n",
      "Epoch 45, Step 38 Loss = 0.5648888945579529\n",
      "Epoch 45, Step 39 Loss = 0.7473626136779785\n",
      "Epoch 45, Step 40 Loss = 0.5813822746276855\n",
      "Epoch 45, Step 41 Loss = 0.716545045375824\n",
      "Training loss: 0.6518\n",
      "Epoch 46/50\n",
      "Epoch 46, Step 1 Loss = 0.6872984170913696\n",
      "Epoch 46, Step 2 Loss = 0.4630601406097412\n",
      "Epoch 46, Step 3 Loss = 0.737218976020813\n",
      "Epoch 46, Step 4 Loss = 0.5005321502685547\n",
      "Epoch 46, Step 5 Loss = 0.6852221488952637\n",
      "Epoch 46, Step 6 Loss = 0.7310914993286133\n",
      "Epoch 46, Step 7 Loss = 0.3265596330165863\n",
      "Epoch 46, Step 8 Loss = 0.791161060333252\n",
      "Epoch 46, Step 9 Loss = 0.4421069025993347\n",
      "Epoch 46, Step 10 Loss = 0.5268298983573914\n",
      "Epoch 46, Step 11 Loss = 0.9255757331848145\n",
      "Epoch 46, Step 12 Loss = 0.53810715675354\n",
      "Epoch 46, Step 13 Loss = 1.0466487407684326\n",
      "Epoch 46, Step 14 Loss = 0.5914807319641113\n",
      "Epoch 46, Step 15 Loss = 0.5227682590484619\n",
      "Epoch 46, Step 16 Loss = 0.35478806495666504\n",
      "Epoch 46, Step 17 Loss = 0.8831976652145386\n",
      "Epoch 46, Step 18 Loss = 0.5192223787307739\n",
      "Epoch 46, Step 19 Loss = 0.5483877658843994\n",
      "Epoch 46, Step 20 Loss = 0.7882548570632935\n",
      "Epoch 46, Step 21 Loss = 0.8632035851478577\n",
      "Epoch 46, Step 22 Loss = 0.5981197357177734\n",
      "Epoch 46, Step 23 Loss = 0.5837075710296631\n",
      "Epoch 46, Step 24 Loss = 0.5659332275390625\n",
      "Epoch 46, Step 25 Loss = 0.33911755681037903\n",
      "Epoch 46, Step 26 Loss = 0.6520130038261414\n",
      "Epoch 46, Step 27 Loss = 0.8802798390388489\n",
      "Epoch 46, Step 28 Loss = 0.8195502758026123\n",
      "Epoch 46, Step 29 Loss = 0.5944918990135193\n",
      "Epoch 46, Step 30 Loss = 0.9296266436576843\n",
      "Epoch 46, Step 31 Loss = 0.843462347984314\n",
      "Epoch 46, Step 32 Loss = 0.7328594923019409\n",
      "Epoch 46, Step 33 Loss = 0.5142461657524109\n",
      "Epoch 46, Step 34 Loss = 0.4718887209892273\n",
      "Epoch 46, Step 35 Loss = 0.7417828440666199\n",
      "Epoch 46, Step 36 Loss = 0.7522392272949219\n",
      "Epoch 46, Step 37 Loss = 0.41849076747894287\n",
      "Epoch 46, Step 38 Loss = 0.4536173939704895\n",
      "Epoch 46, Step 39 Loss = 1.1777715682983398\n",
      "Epoch 46, Step 40 Loss = 0.8180881142616272\n",
      "Epoch 46, Step 41 Loss = 1.1330904960632324\n",
      "Training loss: 0.6706\n",
      "Epoch 47/50\n",
      "Epoch 47, Step 1 Loss = 0.7776731252670288\n",
      "Epoch 47, Step 2 Loss = 0.7416822910308838\n",
      "Epoch 47, Step 3 Loss = 0.6066128015518188\n",
      "Epoch 47, Step 4 Loss = 0.6689579486846924\n",
      "Epoch 47, Step 5 Loss = 0.48833274841308594\n",
      "Epoch 47, Step 6 Loss = 0.22546477615833282\n",
      "Epoch 47, Step 7 Loss = 0.3283589482307434\n",
      "Epoch 47, Step 8 Loss = 0.3766535520553589\n",
      "Epoch 47, Step 9 Loss = 0.7051494121551514\n",
      "Epoch 47, Step 10 Loss = 0.6608179807662964\n",
      "Epoch 47, Step 11 Loss = 0.7159097194671631\n",
      "Epoch 47, Step 12 Loss = 0.7400717735290527\n",
      "Epoch 47, Step 13 Loss = 0.19996824860572815\n",
      "Epoch 47, Step 14 Loss = 0.43069952726364136\n",
      "Epoch 47, Step 15 Loss = 0.4905182719230652\n",
      "Epoch 47, Step 16 Loss = 1.0587090253829956\n",
      "Epoch 47, Step 17 Loss = 0.565114438533783\n",
      "Epoch 47, Step 18 Loss = 0.7538987994194031\n",
      "Epoch 47, Step 19 Loss = 0.8773974776268005\n",
      "Epoch 47, Step 20 Loss = 0.8984183073043823\n",
      "Epoch 47, Step 21 Loss = 0.7655477523803711\n",
      "Epoch 47, Step 22 Loss = 1.0090007781982422\n",
      "Epoch 47, Step 23 Loss = 0.9842958450317383\n",
      "Epoch 47, Step 24 Loss = 0.5637524724006653\n",
      "Epoch 47, Step 25 Loss = 1.0192229747772217\n",
      "Epoch 47, Step 26 Loss = 0.5044518709182739\n",
      "Epoch 47, Step 27 Loss = 0.855248212814331\n",
      "Epoch 47, Step 28 Loss = 0.7263007760047913\n",
      "Epoch 47, Step 29 Loss = 0.364881306886673\n",
      "Epoch 47, Step 30 Loss = 0.9551352262496948\n",
      "Epoch 47, Step 31 Loss = 0.21065855026245117\n",
      "Epoch 47, Step 32 Loss = 0.4006454348564148\n",
      "Epoch 47, Step 33 Loss = 0.4395049214363098\n",
      "Epoch 47, Step 34 Loss = 0.8729376792907715\n",
      "Epoch 47, Step 35 Loss = 1.1666224002838135\n",
      "Epoch 47, Step 36 Loss = 1.0316879749298096\n",
      "Epoch 47, Step 37 Loss = 1.1309947967529297\n",
      "Epoch 47, Step 38 Loss = 0.49808239936828613\n",
      "Epoch 47, Step 39 Loss = 1.3751044273376465\n",
      "Epoch 47, Step 40 Loss = 0.6162591576576233\n",
      "Epoch 47, Step 41 Loss = 0.6534573435783386\n",
      "Training loss: 0.6940\n",
      "Epoch 48/50\n",
      "Epoch 48, Step 1 Loss = 0.28077057003974915\n",
      "Epoch 48, Step 2 Loss = 0.39689189195632935\n",
      "Epoch 48, Step 3 Loss = 0.7050575613975525\n",
      "Epoch 48, Step 4 Loss = 0.4458927810192108\n",
      "Epoch 48, Step 5 Loss = 0.5811354517936707\n",
      "Epoch 48, Step 6 Loss = 0.4123399555683136\n",
      "Epoch 48, Step 7 Loss = 0.6398464441299438\n",
      "Epoch 48, Step 8 Loss = 0.6321234703063965\n",
      "Epoch 48, Step 9 Loss = 0.40455490350723267\n",
      "Epoch 48, Step 10 Loss = 0.432341992855072\n",
      "Epoch 48, Step 11 Loss = 0.8923367261886597\n",
      "Epoch 48, Step 12 Loss = 0.4977725148200989\n",
      "Epoch 48, Step 13 Loss = 1.2125043869018555\n",
      "Epoch 48, Step 14 Loss = 0.46673309803009033\n",
      "Epoch 48, Step 15 Loss = 0.5473093390464783\n",
      "Epoch 48, Step 16 Loss = 0.27056869864463806\n",
      "Epoch 48, Step 17 Loss = 0.5983221530914307\n",
      "Epoch 48, Step 18 Loss = 0.6767252683639526\n",
      "Epoch 48, Step 19 Loss = 0.584635853767395\n",
      "Epoch 48, Step 20 Loss = 0.7771595120429993\n",
      "Epoch 48, Step 21 Loss = 1.004945993423462\n",
      "Epoch 48, Step 22 Loss = 0.5914855003356934\n",
      "Epoch 48, Step 23 Loss = 0.2666775584220886\n",
      "Epoch 48, Step 24 Loss = 0.7638981342315674\n",
      "Epoch 48, Step 25 Loss = 1.319084644317627\n",
      "Epoch 48, Step 26 Loss = 1.1025989055633545\n",
      "Epoch 48, Step 27 Loss = 0.8660075068473816\n",
      "Epoch 48, Step 28 Loss = 0.9172816276550293\n",
      "Epoch 48, Step 29 Loss = 0.8783936500549316\n",
      "Epoch 48, Step 30 Loss = 0.40934303402900696\n",
      "Epoch 48, Step 31 Loss = 0.6847604513168335\n",
      "Epoch 48, Step 32 Loss = 0.8245234489440918\n",
      "Epoch 48, Step 33 Loss = 0.3995145559310913\n",
      "Epoch 48, Step 34 Loss = 0.6744600534439087\n",
      "Epoch 48, Step 35 Loss = 0.4292300343513489\n",
      "Epoch 48, Step 36 Loss = 1.0407291650772095\n",
      "Epoch 48, Step 37 Loss = 0.6472635269165039\n",
      "Epoch 48, Step 38 Loss = 0.3708844482898712\n",
      "Epoch 48, Step 39 Loss = 0.9640520811080933\n",
      "Epoch 48, Step 40 Loss = 0.9120758771896362\n",
      "Epoch 48, Step 41 Loss = 1.1804941892623901\n",
      "Training loss: 0.6757\n",
      "Epoch 49/50\n",
      "Epoch 49, Step 1 Loss = 0.5012216567993164\n",
      "Epoch 49, Step 2 Loss = 0.5301405787467957\n",
      "Epoch 49, Step 3 Loss = 0.5364866256713867\n",
      "Epoch 49, Step 4 Loss = 0.4372262954711914\n",
      "Epoch 49, Step 5 Loss = 0.2606673240661621\n",
      "Epoch 49, Step 6 Loss = 0.835087776184082\n",
      "Epoch 49, Step 7 Loss = 0.5841578245162964\n",
      "Epoch 49, Step 8 Loss = 0.41935238242149353\n",
      "Epoch 49, Step 9 Loss = 0.7818311452865601\n",
      "Epoch 49, Step 10 Loss = 0.4258148670196533\n",
      "Epoch 49, Step 11 Loss = 0.47139668464660645\n",
      "Epoch 49, Step 12 Loss = 0.7859751582145691\n",
      "Epoch 49, Step 13 Loss = 0.6328334808349609\n",
      "Epoch 49, Step 14 Loss = 0.9428267478942871\n",
      "Epoch 49, Step 15 Loss = 1.0527610778808594\n",
      "Epoch 49, Step 16 Loss = 0.6446574926376343\n",
      "Epoch 49, Step 17 Loss = 0.44085314869880676\n",
      "Epoch 49, Step 18 Loss = 0.6259797811508179\n",
      "Epoch 49, Step 19 Loss = 0.38130688667297363\n",
      "Epoch 49, Step 20 Loss = 0.9969441890716553\n",
      "Epoch 49, Step 21 Loss = 0.42151594161987305\n",
      "Epoch 49, Step 22 Loss = 0.6775136590003967\n",
      "Epoch 49, Step 23 Loss = 0.45002245903015137\n",
      "Epoch 49, Step 24 Loss = 0.7010412812232971\n",
      "Epoch 49, Step 25 Loss = 0.6125363707542419\n",
      "Epoch 49, Step 26 Loss = 0.7349673509597778\n",
      "Epoch 49, Step 27 Loss = 0.7794391512870789\n",
      "Epoch 49, Step 28 Loss = 0.9884154796600342\n",
      "Epoch 49, Step 29 Loss = 0.2958137094974518\n",
      "Epoch 49, Step 30 Loss = 0.3319358825683594\n",
      "Epoch 49, Step 31 Loss = 0.9130727052688599\n",
      "Epoch 49, Step 32 Loss = 0.7495388984680176\n",
      "Epoch 49, Step 33 Loss = 0.43250563740730286\n",
      "Epoch 49, Step 34 Loss = 0.8531441688537598\n",
      "Epoch 49, Step 35 Loss = 0.5724550485610962\n",
      "Epoch 49, Step 36 Loss = 1.0871838331222534\n",
      "Epoch 49, Step 37 Loss = 0.6651344895362854\n",
      "Epoch 49, Step 38 Loss = 0.6430655121803284\n",
      "Epoch 49, Step 39 Loss = 0.9532122611999512\n",
      "Epoch 49, Step 40 Loss = 0.6729593276977539\n",
      "Epoch 49, Step 41 Loss = 0.9995574355125427\n",
      "Training loss: 0.6542\n",
      "Epoch 50/50\n",
      "Epoch 50, Step 1 Loss = 0.7494978308677673\n",
      "Epoch 50, Step 2 Loss = 0.2410268783569336\n",
      "Epoch 50, Step 3 Loss = 0.35779908299446106\n",
      "Epoch 50, Step 4 Loss = 0.2852572798728943\n",
      "Epoch 50, Step 5 Loss = 0.6107709407806396\n",
      "Epoch 50, Step 6 Loss = 0.7240972518920898\n",
      "Epoch 50, Step 7 Loss = 0.5852916240692139\n",
      "Epoch 50, Step 8 Loss = 0.712814450263977\n",
      "Epoch 50, Step 9 Loss = 0.6315127015113831\n",
      "Epoch 50, Step 10 Loss = 0.32392093539237976\n",
      "Epoch 50, Step 11 Loss = 0.5264071226119995\n",
      "Epoch 50, Step 12 Loss = 0.6120798587799072\n",
      "Epoch 50, Step 13 Loss = 0.5843599438667297\n",
      "Epoch 50, Step 14 Loss = 0.5631954669952393\n",
      "Epoch 50, Step 15 Loss = 0.6841822266578674\n",
      "Epoch 50, Step 16 Loss = 0.6933597326278687\n",
      "Epoch 50, Step 17 Loss = 0.710185706615448\n",
      "Epoch 50, Step 18 Loss = 1.2431211471557617\n",
      "Epoch 50, Step 19 Loss = 0.4016422629356384\n",
      "Epoch 50, Step 20 Loss = 0.5753285884857178\n",
      "Epoch 50, Step 21 Loss = 0.5016957521438599\n",
      "Epoch 50, Step 22 Loss = 0.5705816745758057\n",
      "Epoch 50, Step 23 Loss = 0.43896913528442383\n",
      "Epoch 50, Step 24 Loss = 0.7215933799743652\n",
      "Epoch 50, Step 25 Loss = 0.9686746597290039\n",
      "Epoch 50, Step 26 Loss = 0.5296100974082947\n",
      "Epoch 50, Step 27 Loss = 0.8921700716018677\n",
      "Epoch 50, Step 28 Loss = 0.12833727896213531\n",
      "Epoch 50, Step 29 Loss = 0.5334754586219788\n",
      "Epoch 50, Step 30 Loss = 0.8133219480514526\n",
      "Epoch 50, Step 31 Loss = 0.5757819414138794\n",
      "Epoch 50, Step 32 Loss = 0.6987366080284119\n",
      "Epoch 50, Step 33 Loss = 0.8124873042106628\n",
      "Epoch 50, Step 34 Loss = 0.7223832011222839\n",
      "Epoch 50, Step 35 Loss = 0.5987603664398193\n",
      "Epoch 50, Step 36 Loss = 0.74564129114151\n",
      "Epoch 50, Step 37 Loss = 1.0994153022766113\n",
      "Epoch 50, Step 38 Loss = 0.8788090944290161\n",
      "Epoch 50, Step 39 Loss = 0.9308568239212036\n",
      "Epoch 50, Step 40 Loss = 1.2128667831420898\n",
      "Epoch 50, Step 41 Loss = 1.0316026210784912\n",
      "Training loss: 0.6639\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Train the model\n",
    "train_gru4rec(model=model, train_dataset=train_dataset,optimizer=optimizer, loss_fn=loss_fn, epochs=500, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 recall: 0.6875\n",
      "Batch 2 recall: 0.4375\n",
      "Batch 3 recall: 0.4375\n",
      "Batch 4 recall: 0.8125\n",
      "Batch 5 recall: 0.7500\n",
      "Batch 6 recall: 0.7500\n",
      "Batch 7 recall: 1.0000\n",
      "Batch 8 recall: 0.7500\n",
      "Batch 9 recall: 0.6875\n",
      "Batch 10 recall: 0.6875\n",
      "Batch 11 recall: 0.8125\n",
      "Batch 12 recall: 0.8125\n",
      "Batch 13 recall: 0.7500\n",
      "Batch 14 recall: 0.7500\n",
      "Batch 15 recall: 0.5000\n",
      "Batch 16 recall: 0.9375\n",
      "Batch 17 recall: 0.7500\n",
      "Batch 18 recall: 0.6250\n",
      "Batch 19 recall: 0.8750\n",
      "Batch 20 recall: 0.6875\n",
      "Batch 21 recall: 0.7500\n",
      "Batch 22 recall: 0.8125\n",
      "Batch 23 recall: 0.8750\n",
      "Batch 24 recall: 0.6250\n",
      "Batch 25 recall: 0.8750\n",
      "Batch 26 recall: 0.9375\n",
      "Batch 27 recall: 0.7500\n",
      "Batch 28 recall: 0.7500\n",
      "Batch 29 recall: 0.7500\n",
      "Batch 30 recall: 0.8125\n",
      "Batch 31 recall: 0.8125\n",
      "Batch 32 recall: 0.8750\n",
      "Batch 33 recall: 0.8750\n",
      "Batch 34 recall: 0.8125\n",
      "Batch 35 recall: 0.6875\n",
      "Batch 36 recall: 0.8125\n",
      "Batch 37 recall: 0.7500\n",
      "Batch 38 recall: 0.8750\n",
      "Batch 39 recall: 0.7500\n",
      "Batch 40 recall: 0.6875\n",
      "Batch 41 recall: 1.0000\n",
      "Overall recall: 0.7652\n"
     ]
    }
   ],
   "source": [
    "def predict(model, item_sequences, item_features, item_genres):\n",
    "    \"\"\"\n",
    "    Predict the item with the highest probability for the given input sequences using argmax of softmax.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - item_sequences: Input item sequences (batch_size, seq_length).\n",
    "    - item_features: Input item features (batch_size, feature_length).\n",
    "    - item_genres: Input item genres (batch_size, genre_length).\n",
    "\n",
    "    Returns:\n",
    "    - predicted_items: A list of predicted items with the highest probability for each input sequence.\n",
    "    \"\"\"\n",
    "    # Run the model in inference mode (not training)\n",
    "    predicted_items, logits = model((item_sequences, item_features, item_genres), training=False)\n",
    "    \n",
    "    # Apply softmax to the logits to get probabilities\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Get the item with the highest probability by finding the index of the maximum probability\n",
    "    predicted_items = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    # Convert to numpy array for easier handling\n",
    "    predicted_items = predicted_items.numpy()\n",
    "\n",
    "    return predicted_items\n",
    "\n",
    "def compute_recall(predicted_items, targets):\n",
    "    \"\"\"\n",
    "    Compute the recall for the given predictions and targets.\n",
    "\n",
    "    Args:\n",
    "    - predicted_items: The predicted items (batch_size,).\n",
    "    - targets: The actual next items (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "    - recall: The recall metric.\n",
    "    \"\"\"\n",
    "    # True positives: Predicted item matches the target\n",
    "    true_positives = np.sum(predicted_items == targets)\n",
    "    \n",
    "    # Total relevant items (in this case, it is the number of items in the batch)\n",
    "    total_items = len(targets)\n",
    "    \n",
    "    # Recall calculation\n",
    "    recall = true_positives / total_items if total_items > 0 else 0\n",
    "    return recall\n",
    "\n",
    "# Initialize variables to calculate overall recall\n",
    "total_true_positives = 0\n",
    "total_items = 0\n",
    "\n",
    "# Loop through training dataset and predict the most probable item\n",
    "for step, batch in enumerate(train_dataset):\n",
    "    item_sequences = batch['item']\n",
    "    item_genres = batch['genre']\n",
    "    item_features = batch['features']\n",
    "    targets = batch['next_item']\n",
    "    \n",
    "    # Get the predicted item with the highest probability for each sequence in the batch\n",
    "    predicted_items = predict(model, item_sequences, item_features, item_genres)\n",
    "    \n",
    "    # Compute recall for the current batch\n",
    "    batch_recall = compute_recall(predicted_items, targets)\n",
    "    print(f\"Batch {step + 1} recall: {batch_recall:.4f}\")\n",
    "    \n",
    "    # Accumulate for overall recall\n",
    "    total_true_positives += np.sum(predicted_items == targets)\n",
    "    total_items += len(targets)\n",
    "\n",
    "# Calculate overall recall\n",
    "overall_recall = total_true_positives / total_items if total_items > 0 else 0\n",
    "print(f\"Overall recall: {overall_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('gru4rec_model.h5', save_format='h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
