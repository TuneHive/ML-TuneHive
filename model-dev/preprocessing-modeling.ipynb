{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.api.layers import Dense, Embedding, GRU, LeakyReLU, Concatenate, Masking\n",
    "from keras.api import Input\n",
    "from keras.api.models import Model\n",
    "from keras.api.losses import SparseCategoricalCrossentropy\n",
    "from keras.api.metrics import SparseCategoricalAccuracy, Mean, TopKCategoricalAccuracy\n",
    "from transformers.models.bert import TFBertTokenizer, TFBertEmbeddings  # embedding and tokenizer for description/nlp related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU4REC(Model):\n",
    "    def __init__(self, k, num_users, num_items, rnn_params, embedding_dimension, ffn1_units):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.k = k\n",
    "        \n",
    "        self.user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dimension)\n",
    "        self.item_embedding = Embedding(input_dim=num_items, output_dim=embedding_dimension, mask_zero=True)\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = []\n",
    "        self.rnn.append(GRU(**rnn_params[0], return_sequences=True))\n",
    "        for i in range(1, len(rnn_params)-1):\n",
    "            self.rnn.append(GRU(**rnn_params[i], return_sequences=True)) # this layer will have two inputs (from embedding layer, or from previous GRU layer)\n",
    "        \n",
    "        self.rnn.append(GRU(**rnn_params[-1], return_sequences=False))\n",
    "        \n",
    "        self.concat = Concatenate(axis=-1)\n",
    "        # feed-forward layer\n",
    "        self.ffn1 = Dense(ffn1_units)\n",
    "        self.activation1 = LeakyReLU(alpha=0.2)\n",
    "        self.out = Dense(k, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        user_ids, item_sequences = inputs\n",
    "        \n",
    "        # Embed users and items\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        item_embedded = self.item_embedding(item_sequences)\n",
    "        \n",
    "        x = self.rnn[0](item_embedded)\n",
    "        for i in range(1, len(self.rnn)):\n",
    "            x = self.concat([item_embedded, x])\n",
    "            x = self.rnn[i](x)\n",
    "        \n",
    "        x = self.concat([user_embedded, x])\n",
    "        \n",
    "        x = self.ffn1(x)\n",
    "        x = self.activation1(x)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_gru4rec(model, dataset, optimizer, loss_fn, num_epochs, top_k=5):\n",
    "    \"\"\"Custom training loop for GRU4REC.\"\"\"\n",
    "    # Metrics to track loss and top k precision\n",
    "    train_loss = Mean(name='train_loss')\n",
    "    \n",
    "    train_top_k_precision = TopKCategoricalAccuracy(k=top_k, name='train_top_k_precision')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        train_loss.reset_state()\n",
    "        train_top_k_precision.reset_state()\n",
    "\n",
    "        # Iterate over the dataset\n",
    "        for batch, (inputs, labels) in enumerate(dataset):\n",
    "            user_ids, item_sequences = inputs\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                predictions = model((user_ids, item_sequences), training=True)\n",
    "                loss = loss_fn(labels, predictions)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss.update_state(loss)\n",
    "            train_top_k_precision.update_state(labels, predictions)\n",
    "\n",
    "            print(f\"Batch {batch}, Loss: {train_loss.result().numpy():.4f}, \"\n",
    "                f\"Accuracy: {train_top_k_precision.result().numpy():.4f}\")\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss.result().numpy():.4f}, \"\n",
    "              f\"Accuracy: {train_top_k_precision.result().numpy():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
